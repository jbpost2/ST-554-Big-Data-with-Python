[
  {
    "objectID": "02_Big_Data_Management/SQL_Joins_Notebook.html",
    "href": "02_Big_Data_Management/SQL_Joins_Notebook.html",
    "title": "SQL Joins on chinook",
    "section": "",
    "text": "Now let’s do some joins on the chinook database. We’ll reread in the packages are make our connection to the database.\n\nimport sqlite3\nimport pandas as pd\ncon = sqlite3.connect(\"chinook.db\")\n\n\nUsing pandas we can check out the tables returned as a data frame.\n\npd.read_sql(\"SELECT * FROM sqlite_schema WHERE type = 'table';\", con)\n\n\n  \n    \n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n13\ntable\njustin_music\njustin_music\n865\nCREATE TABLE justin_music (\\n album TEX...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe tracks and albums tables each share a variable/column/key of AlbumId. Let’s do an inner join on those using pd.merge(). Remember that we can pull the full tables into pandas using pd.read_sql() and then use pandas to do the join!\n\ntracks_albums = pd.merge(left = pd.read_sql(\"SELECT * FROM tracks\", con), right = pd.read_sql(\"SELECT * FROM albums\", con),\n         how = \"inner\",\n         on = \"AlbumId\")\ntracks_albums\n\n\n  \n    \n\n\n\n\n\n\nTrackId\nName\nAlbumId\nMediaTypeId\nGenreId\nComposer\nMilliseconds\nBytes\nUnitPrice\nTitle\nArtistId\n\n\n\n\n0\n1\nFor Those About To Rock (We Salute You)\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n343719\n11170334\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n1\n6\nPut The Finger On You\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n205662\n6713451\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n2\n7\nLet's Get It Up\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n233926\n7636561\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n3\n8\nInject The Venom\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n210834\n6852860\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n4\n9\nSnowballed\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n203102\n6599424\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3498\n3499\nPini Di Roma (Pinien Von Rom) \\ I Pini Della V...\n343\n2\n24\nNone\n286741\n4718950\n0.99\nRespighi:Pines of Rome\n226\n\n\n3499\n3500\nString Quartet No. 12 in C Minor, D. 703 \"Quar...\n344\n2\n24\nFranz Schubert\n139200\n2283131\n0.99\nSchubert: The Late String Quartets & String Qu...\n272\n\n\n3500\n3501\nL'orfeo, Act 3, Sinfonia (Orchestra)\n345\n2\n24\nClaudio Monteverdi\n66639\n1189062\n0.99\nMonteverdi: L'Orfeo\n273\n\n\n3501\n3502\nQuintet for Horn, Violin, 2 Violas, and Cello ...\n346\n2\n24\nWolfgang Amadeus Mozart\n221331\n3665114\n0.99\nMozart: Chamber Music\n274\n\n\n3502\n3503\nKoyaanisqatsi\n347\n2\n10\nPhilip Glass\n206005\n3305164\n0.99\nKoyaanisqatsi (Soundtrack from the Motion Pict...\n275\n\n\n\n\n\n3503 rows × 11 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow we’ll join that table we just made with another table, invoice_items and do a left join (treating invoice_items as the left table). We’ll join on the key that is shared, trackId.\n\nnext = pd.merge(left = pd.read_sql(\"SELECT * FROM invoice_items\", con),\n         right= tracks_albums,\n         how = \"left\",\n         on = \"TrackId\")\nnext\n\n\n  \n    \n\n\n\n\n\n\nInvoiceLineId\nInvoiceId\nTrackId\nUnitPrice_x\nQuantity\nName\nAlbumId\nMediaTypeId\nGenreId\nComposer\nMilliseconds\nBytes\nUnitPrice_y\nTitle\nArtistId\n\n\n\n\n0\n1\n1\n2\n0.99\n1\nBalls to the Wall\n2\n2\n1\nNone\n342562\n5510424\n0.99\nBalls to the Wall\n2\n\n\n1\n2\n1\n4\n0.99\n1\nRestless and Wild\n3\n2\n1\nF. Baltes, R.A. Smith-Diesel, S. Kaufman, U. D...\n252051\n4331779\n0.99\nRestless and Wild\n2\n\n\n2\n3\n2\n6\n0.99\n1\nPut The Finger On You\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n205662\n6713451\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n3\n4\n2\n8\n0.99\n1\nInject The Venom\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n210834\n6852860\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n4\n5\n2\n10\n0.99\n1\nEvil Walks\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n263497\n8611245\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2235\n2236\n411\n3136\n0.99\n1\nLooking For Love\n141\n1\n3\nSykes\n391941\n12769847\n0.99\nGreatest Hits\n100\n\n\n2236\n2237\n411\n3145\n0.99\n1\nSweet Lady Luck\n141\n1\n3\nVandenberg\n273737\n8919163\n0.99\nGreatest Hits\n100\n\n\n2237\n2238\n411\n3154\n0.99\n1\nFeirinha da Pavuna/Luz do Repente/Bagaço da La...\n248\n1\n7\nArlindo Cruz/Franco/Marquinhos PQD/Negro, Jove...\n107206\n3593684\n0.99\nAo Vivo [IMPORT]\n155\n\n\n2238\n2239\n411\n3163\n0.99\n1\nSamba pras moças\n248\n1\n7\nGrazielle/Roque Ferreira\n152816\n5121366\n0.99\nAo Vivo [IMPORT]\n155\n\n\n2239\n2240\n412\n3177\n1.99\n1\nHot Girl\n249\n3\n19\nNone\n1325458\n267836576\n1.99\nThe Office, Season 1\n156\n\n\n\n\n\n2240 rows × 15 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nLastly, we’ll now combine that table with the invoices table using an outer join on the key InvoiceId. Note that you can actually do all of these joins in SQL using one call (which is likely much more efficient!). Please see the additional readings for topic 2 for more information.\n\npd.merge(left = pd.read_sql(\"SELECT * FROM invoices\", con),\n         right = next,\n         how = \"outer\",\n         on = \"InvoiceId\")\n\n\n  \n    \n\n\n\n\n\n\nInvoiceId\nCustomerId\nInvoiceDate\nBillingAddress\nBillingCity\nBillingState\nBillingCountry\nBillingPostalCode\nTotal\nInvoiceLineId\n...\nName\nAlbumId\nMediaTypeId\nGenreId\nComposer\nMilliseconds\nBytes\nUnitPrice_y\nTitle\nArtistId\n\n\n\n\n0\n1\n2\n2009-01-01 00:00:00\nTheodor-Heuss-Straße 34\nStuttgart\nNone\nGermany\n70174\n1.98\n1\n...\nBalls to the Wall\n2\n2\n1\nNone\n342562\n5510424\n0.99\nBalls to the Wall\n2\n\n\n1\n1\n2\n2009-01-01 00:00:00\nTheodor-Heuss-Straße 34\nStuttgart\nNone\nGermany\n70174\n1.98\n2\n...\nRestless and Wild\n3\n2\n1\nF. Baltes, R.A. Smith-Diesel, S. Kaufman, U. D...\n252051\n4331779\n0.99\nRestless and Wild\n2\n\n\n2\n2\n4\n2009-01-02 00:00:00\nUllevålsveien 14\nOslo\nNone\nNorway\n0171\n3.96\n3\n...\nPut The Finger On You\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n205662\n6713451\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n3\n2\n4\n2009-01-02 00:00:00\nUllevålsveien 14\nOslo\nNone\nNorway\n0171\n3.96\n4\n...\nInject The Venom\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n210834\n6852860\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n4\n2\n4\n2009-01-02 00:00:00\nUllevålsveien 14\nOslo\nNone\nNorway\n0171\n3.96\n5\n...\nEvil Walks\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n263497\n8611245\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2235\n411\n44\n2013-12-14 00:00:00\nPorthaninkatu 9\nHelsinki\nNone\nFinland\n00530\n13.86\n2236\n...\nLooking For Love\n141\n1\n3\nSykes\n391941\n12769847\n0.99\nGreatest Hits\n100\n\n\n2236\n411\n44\n2013-12-14 00:00:00\nPorthaninkatu 9\nHelsinki\nNone\nFinland\n00530\n13.86\n2237\n...\nSweet Lady Luck\n141\n1\n3\nVandenberg\n273737\n8919163\n0.99\nGreatest Hits\n100\n\n\n2237\n411\n44\n2013-12-14 00:00:00\nPorthaninkatu 9\nHelsinki\nNone\nFinland\n00530\n13.86\n2238\n...\nFeirinha da Pavuna/Luz do Repente/Bagaço da La...\n248\n1\n7\nArlindo Cruz/Franco/Marquinhos PQD/Negro, Jove...\n107206\n3593684\n0.99\nAo Vivo [IMPORT]\n155\n\n\n2238\n411\n44\n2013-12-14 00:00:00\nPorthaninkatu 9\nHelsinki\nNone\nFinland\n00530\n13.86\n2239\n...\nSamba pras moças\n248\n1\n7\nGrazielle/Roque Ferreira\n152816\n5121366\n0.99\nAo Vivo [IMPORT]\n155\n\n\n2239\n412\n58\n2013-12-22 00:00:00\n12,Community Centre\nDelhi\nNone\nIndia\n110017\n1.99\n2240\n...\nHot Girl\n249\n3\n19\nNone\n1325458\n267836576\n1.99\nThe Office, Season 1\n156\n\n\n\n\n\n2240 rows × 23 columns"
  },
  {
    "objectID": "02_Big_Data_Management/11-pyspark_pandas_on_Spark_Landing.html",
    "href": "02_Big_Data_Management/11-pyspark_pandas_on_Spark_Landing.html",
    "title": "pyspark: Pandas-on-Spark",
    "section": "",
    "text": "The video below discusses the use of pyspark and the pandas-on-spark framework. This utilizes a pandas style data frame and functionality.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nupdate video"
  },
  {
    "objectID": "02_Big_Data_Management/11-pyspark_pandas_on_Spark_Landing.html#notes",
    "href": "02_Big_Data_Management/11-pyspark_pandas_on_Spark_Landing.html#notes",
    "title": "pyspark: Pandas-on-Spark",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/09-Spark_for_Big_Data_Landing.html",
    "href": "02_Big_Data_Management/09-Spark_for_Big_Data_Landing.html",
    "title": "spark: Big Data Software",
    "section": "",
    "text": "The video below introduces the spark software for handling big data. This is a commonly used software to summarize, fit models, and handle streaming big data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nUpdate video"
  },
  {
    "objectID": "02_Big_Data_Management/09-Spark_for_Big_Data_Landing.html#notes",
    "href": "02_Big_Data_Management/09-Spark_for_Big_Data_Landing.html#notes",
    "title": "spark: Big Data Software",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html",
    "href": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html",
    "title": "Legacy Big Data Software: Hadoop Distributed File System",
    "section": "",
    "text": "The video below discusses a legacy big data software called Hadoop along with the common Map-Reduce actions that Hadoop allows one to perform on big data. While not a widely used software at this point, understanding the language and ideas of what made Hadoop popular are important.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe Charles Dickens text analyzed can be found here."
  },
  {
    "objectID": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html#notes",
    "href": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html#notes",
    "title": "Legacy Big Data Software: Hadoop Distributed File System",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version"
  },
  {
    "objectID": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html#additional-readings-for-week-8",
    "href": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html#additional-readings-for-week-8",
    "title": "Legacy Big Data Software: Hadoop Distributed File System",
    "section": "Additional Readings for Week 8",
    "text": "Additional Readings for Week 8\n\nHadoop\n\nHadoop Tutorial, Hadoop Architecture, Wikipedia article on Hadoop, What is Hadoop?, Another Intro to Hadoop\nHadoop YARN: Docs, Article\nHDFS & the Cloud: Article 1, Book (not free though)\nS3 storage\nAzure Blob Storage\n\n\n\nMapReduce\n\nMapReduce Examples: Article 1, Article 2\nA good intro through an example using PySpark\nHadoop vs Spark (IBM)\n\n\n\nSpark\n\nWhat is spark?\nBasic Spark Tutorial\nSpark APIs (databricks)\nMore on DAGs\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/05-SQL_Resources.html",
    "href": "02_Big_Data_Management/05-SQL_Resources.html",
    "title": "SQL Resources",
    "section": "",
    "text": "There is a ton more to learn about SQL if you are interested. We’ll be using it a bit more to interact with Spark. Spark has a pandas-on-Spark API (way of interacting) and a Spark-SQL API. While pandas is generally easier, it isn’t as functional as the SQL interface.\nLuckily, we’ll mostly use SQL to rename variables, select data, filter observations, and maybe create a new observation or two. As such, it is useful to understand a bit more about it than we covered in lecture/readings."
  },
  {
    "objectID": "02_Big_Data_Management/05-SQL_Resources.html#variable-types",
    "href": "02_Big_Data_Management/05-SQL_Resources.html#variable-types",
    "title": "SQL Resources",
    "section": "Variable Types",
    "text": "Variable Types\nWe briefly discussed the difference between python using None and SQLite using NULL. Generally, python data types are going to get transformed to SQLite (or, later, Spark) when we move data in and out.\nSQLite and Python types:\n\nSQLite natively supports the following types: \n\nNULL\nINTEGER\nREAL\nTEXT\nBLOB\n\nThe following Python types can be sent to SQLite and will be converted as follows:\n\n\n\n\nPython type\nSQLite type\n\n\n\n\nNone\nNULL\n\n\nint\nINTEGER\n\n\nfloat\nREAL\n\n\nstr (depends on text_factory)\nTEXT\n\n\nbytes\nBLOG"
  },
  {
    "objectID": "02_Big_Data_Management/05-SQL_Resources.html#sqlite-practice-non-graded",
    "href": "02_Big_Data_Management/05-SQL_Resources.html#sqlite-practice-non-graded",
    "title": "SQL Resources",
    "section": "SQLite Practice (Non-graded)",
    "text": "SQLite Practice (Non-graded)\nYou should\n\nhead to the sqlitetutorial.net site and read through the tutorials based around the SELECT statement (there are 7 or so listed on the page linked).\nthen read through the five ‘SQLite function’ pages (For example: https://www.sqlitetutorial.net/sqlite-avg/)\nLastly, for those that want more practice, head to datalemur.com and try some of the ‘easy’ tasks! This will help prepare you for the logic of our later SQL work in Spark!\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html",
    "href": "02_Big_Data_Management/03-Databases_SQL.html",
    "title": "Data Storage and Basic SQL",
    "section": "",
    "text": "Before we get into Big Data, we need to understand the most common way to store many different, related, datasets. We’ve already looked at basic ways to store data in a single file:\nWhen we have multiple data sets (or sources) a database is often used along with a database management system (DBMS).\nConsider the diagram below. This shows many different tables (think data frames) such as playlists, playlist_track, and tracks. These tables are linked through keys such as the Playlistid or Trackid.\nIn this notebook we explore databases and the basics of the common structured query language (SQL) used to interact with them.\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#common-database-software",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#common-database-software",
    "title": "Data Storage and Basic SQL",
    "section": "Common Database Software",
    "text": "Common Database Software\nMany common types of relational databases management systems (RDBMS) exist. Some are free and some are not. A few common ones are:\n\nOracle\nMySQL\nSQL Server\nPostgreSQL\nSQLite\nAzure SQL\n\nMost RDBMS have their own Structured Query Language (SQL), however the basic functionality is similar across them!"
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#actions-on-databases",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#actions-on-databases",
    "title": "Data Storage and Basic SQL",
    "section": "Actions on Databases",
    "text": "Actions on Databases\nThere are a few common actions we often want to perform on a database. The acronym CRUD is used to describe four actions:\n\nCreate data\nRead data\nUpdate data\nDelete data\n\nWe will write SQL code to do these actions!\nYou could imagine that, with multiple users possibly accessing a database, we need to be very careful in how we do these actions. There are four properties that relational database transactions must have:\n\nAtomicity defines all the elements that make up a complete database transaction.\nConsistency defines the rules for maintaining data points in a correct state after a transaction.\nIsolation keeps the effect of a transaction invisible to others until it is committed, to avoid confusion.\nDurability ensures that data changes become permanent once the transaction is committed.\n\nThis is commonly referred to as ACID properties.\nLet’s explore using SQLite with python!"
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#accessing-an-sqlite-database-in-python",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#accessing-an-sqlite-database-in-python",
    "title": "Data Storage and Basic SQL",
    "section": "Accessing an SQLite Database in python",
    "text": "Accessing an SQLite Database in python\nThe sqlite3 module provides the ability to connect and perform actions on an SQLite database.\n\nWe need to import this module\nThen we just need a path to our database file (if we already have one, otherwise one will be created at the path - if possible)\nWe’ll look at the commonly used chinook database the diagram above described.\nAs we need to have the ability to write to the database from our notebook, we can’t share the same database. That means you’ll need to download the chinook.db file and upload it to your folder area on the left!\nIf you want the changes to remain after you are done, you’d need to download that file before closing the session!\n\n\n#bring in the module\nimport sqlite3\n#make the connection to the .db file. My file is in the main folder on the left\ncon = sqlite3.connect(\"chinook.db\")\n\n\nAccessing the Table Schema\nEvery SQLite database contains a “schema table” with information about that database. This describes the - tables - indices (special lookup tables to improve efficiency of queries) - triggers (named database object that is executed automatically when an INSERT, UPDATE or DELETE statement is issued against the associated table) - views (read-only tables, combinations of tables, etc.)\nA schema file contains one row for each table, index, view, and trigger in the schema.\nWe can get the schema by issuing an SQL command! A common SQL command for querying a database looks like this: - SELECT column1, column2 FROM table WHERE logical_of_some_kind;\nFor SQLite from python, usually we’ll follow this structure:\n\nCreate a cursor object using cursor = con.cursor()\nWrite our SQL command as a string\nExecute the SQL code using the .execute() method (cursor.execute()) on our cursor object\nUse the .fetchall() method (cursor.fetchall()) to actually return the data requested\nClose the conneciton made by the cursor (cursor.close())\n\n\nBelow we use the multiline comment (three quotation marks to start and end) in order to write more legible SQL code\n\n\n#create a 'cursor' object from our connection\ncursor = con.cursor()\n\n#SQL query to return all table names in the data base\n#The * indicates we want to select everything\nget_schema = '''\n        SELECT *\n        FROM sqlite_schema\n        WHERE type = \"table\";\n        '''\n\n#execute the SQL query on the database!\ncursor.execute(get_schema)\n\n#The information for the query is stored in memory. We use the fetchall() method to actually return the information\nresult = cursor.fetchall()\n\n#finall we close the connection the cursor made with that query\ncursor.close()\n\nWe can now look at what is stored in result. Here it is actually a list that we can cycle through and print information from.\n\nprint(type(result))\nfor i in result:\n  print(i)\n\n&lt;class 'list'&gt;\n('table', 'albums', 'albums', 2, 'CREATE TABLE \"albums\"\\r\\n(\\r\\n    [AlbumId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Title] NVARCHAR(160)  NOT NULL,\\r\\n    [ArtistId] INTEGER  NOT NULL,\\r\\n    FOREIGN KEY ([ArtistId]) REFERENCES \"artists\" ([ArtistId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'sqlite_sequence', 'sqlite_sequence', 3, 'CREATE TABLE sqlite_sequence(name,seq)')\n('table', 'artists', 'artists', 4, 'CREATE TABLE \"artists\"\\r\\n(\\r\\n    [ArtistId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(120)\\r\\n)')\n('table', 'customers', 'customers', 5, 'CREATE TABLE \"customers\"\\r\\n(\\r\\n    [CustomerId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [FirstName] NVARCHAR(40)  NOT NULL,\\r\\n    [LastName] NVARCHAR(20)  NOT NULL,\\r\\n    [Company] NVARCHAR(80),\\r\\n    [Address] NVARCHAR(70),\\r\\n    [City] NVARCHAR(40),\\r\\n    [State] NVARCHAR(40),\\r\\n    [Country] NVARCHAR(40),\\r\\n    [PostalCode] NVARCHAR(10),\\r\\n    [Phone] NVARCHAR(24),\\r\\n    [Fax] NVARCHAR(24),\\r\\n    [Email] NVARCHAR(60)  NOT NULL,\\r\\n    [SupportRepId] INTEGER,\\r\\n    FOREIGN KEY ([SupportRepId]) REFERENCES \"employees\" ([EmployeeId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'employees', 'employees', 8, 'CREATE TABLE \"employees\"\\r\\n(\\r\\n    [EmployeeId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [LastName] NVARCHAR(20)  NOT NULL,\\r\\n    [FirstName] NVARCHAR(20)  NOT NULL,\\r\\n    [Title] NVARCHAR(30),\\r\\n    [ReportsTo] INTEGER,\\r\\n    [BirthDate] DATETIME,\\r\\n    [HireDate] DATETIME,\\r\\n    [Address] NVARCHAR(70),\\r\\n    [City] NVARCHAR(40),\\r\\n    [State] NVARCHAR(40),\\r\\n    [Country] NVARCHAR(40),\\r\\n    [PostalCode] NVARCHAR(10),\\r\\n    [Phone] NVARCHAR(24),\\r\\n    [Fax] NVARCHAR(24),\\r\\n    [Email] NVARCHAR(60),\\r\\n    FOREIGN KEY ([ReportsTo]) REFERENCES \"employees\" ([EmployeeId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'genres', 'genres', 10, 'CREATE TABLE \"genres\"\\r\\n(\\r\\n    [GenreId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(120)\\r\\n)')\n('table', 'invoices', 'invoices', 11, 'CREATE TABLE \"invoices\"\\r\\n(\\r\\n    [InvoiceId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [CustomerId] INTEGER  NOT NULL,\\r\\n    [InvoiceDate] DATETIME  NOT NULL,\\r\\n    [BillingAddress] NVARCHAR(70),\\r\\n    [BillingCity] NVARCHAR(40),\\r\\n    [BillingState] NVARCHAR(40),\\r\\n    [BillingCountry] NVARCHAR(40),\\r\\n    [BillingPostalCode] NVARCHAR(10),\\r\\n    [Total] NUMERIC(10,2)  NOT NULL,\\r\\n    FOREIGN KEY ([CustomerId]) REFERENCES \"customers\" ([CustomerId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'invoice_items', 'invoice_items', 13, 'CREATE TABLE \"invoice_items\"\\r\\n(\\r\\n    [InvoiceLineId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [InvoiceId] INTEGER  NOT NULL,\\r\\n    [TrackId] INTEGER  NOT NULL,\\r\\n    [UnitPrice] NUMERIC(10,2)  NOT NULL,\\r\\n    [Quantity] INTEGER  NOT NULL,\\r\\n    FOREIGN KEY ([InvoiceId]) REFERENCES \"invoices\" ([InvoiceId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION,\\r\\n    FOREIGN KEY ([TrackId]) REFERENCES \"tracks\" ([TrackId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'media_types', 'media_types', 15, 'CREATE TABLE \"media_types\"\\r\\n(\\r\\n    [MediaTypeId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(120)\\r\\n)')\n('table', 'playlists', 'playlists', 16, 'CREATE TABLE \"playlists\"\\r\\n(\\r\\n    [PlaylistId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(120)\\r\\n)')\n('table', 'playlist_track', 'playlist_track', 17, 'CREATE TABLE \"playlist_track\"\\r\\n(\\r\\n    [PlaylistId] INTEGER  NOT NULL,\\r\\n    [TrackId] INTEGER  NOT NULL,\\r\\n    CONSTRAINT [PK_PlaylistTrack] PRIMARY KEY  ([PlaylistId], [TrackId]),\\r\\n    FOREIGN KEY ([PlaylistId]) REFERENCES \"playlists\" ([PlaylistId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION,\\r\\n    FOREIGN KEY ([TrackId]) REFERENCES \"tracks\" ([TrackId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'tracks', 'tracks', 20, 'CREATE TABLE \"tracks\"\\r\\n(\\r\\n    [TrackId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(200)  NOT NULL,\\r\\n    [AlbumId] INTEGER,\\r\\n    [MediaTypeId] INTEGER  NOT NULL,\\r\\n    [GenreId] INTEGER,\\r\\n    [Composer] NVARCHAR(220),\\r\\n    [Milliseconds] INTEGER  NOT NULL,\\r\\n    [Bytes] INTEGER,\\r\\n    [UnitPrice] NUMERIC(10,2)  NOT NULL,\\r\\n    FOREIGN KEY ([AlbumId]) REFERENCES \"albums\" ([AlbumId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION,\\r\\n    FOREIGN KEY ([GenreId]) REFERENCES \"genres\" ([GenreId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION,\\r\\n    FOREIGN KEY ([MediaTypeId]) REFERENCES \"media_types\" ([MediaTypeId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'sqlite_stat1', 'sqlite_stat1', 864, 'CREATE TABLE sqlite_stat1(tbl,idx,stat)')\n\n\nOften we want to put this information into our common pandas data frame format!\n\nimport pandas as pd\n\n\n#create a data frame\nschema_df = pd.DataFrame(result)\nschema_df\n\n\n  \n    \n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nPretty good, just missing column names. For the schema table, those are standardized.\n\n#current column names\nschema_df.columns\n\nRangeIndex(start=0, stop=5, step=1)\n\n\n\nschema_df.rename(columns={0: 'type', 1: 'name', 2: 'tbl_name', 3: 'rootpage', 4: 'sql'}, inplace = True)\nschema_df\n\n\n  \n    \n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nAlternatively, we can use the function read_sql() from pandas to automatically put the result into a data frame!\n\nHere we don’t need a cursor object, we just pass the SQL string and the connection object.\n\n\nschema_df2 = pd.read_sql(get_schema, con)\nschema_df2\n\n\n  \n    \n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nAhh, that’s better!\n\n\nQuerying a Table\nNow that we know which tables exist, we can query them! Let’s return all the albums in the albums table.\nWe go through the same process as before (as we closed our cursor object we need to create a new one):\n\nCreate our cursor using con.cursor()\nWrite our SQL command as a string\nExecute the SQL code using the .execute() method (cursor.execute()) on our cursor object\nUse the .fetchall() method (cursor.fetchall()) to actually return the data requested\nClose the conneciton made by the cursor (cursor.close())\n\nThe FROM function allows us to choose which table to query. LIMIT 20 says to limit what is returned to 20 rows.\n\n#create the cursor instance\ncursor = con.cursor()\n#create the SQL string\nget_albums = '''\n        SELECT *\n        FROM albums\n        LIMIT 20;\n        '''\n#execute the query\nalbums = cursor.execute(get_albums)\n#grab the results\nalbum_results = albums.fetchall()\n#close the cursor\ncursor.close()\n#check the results\nalbum_results\n\n[(1, 'For Those About To Rock We Salute You', 1),\n (2, 'Balls to the Wall', 2),\n (3, 'Restless and Wild', 2),\n (4, 'Let There Be Rock', 1),\n (5, 'Big Ones', 3),\n (6, 'Jagged Little Pill', 4),\n (7, 'Facelift', 5),\n (8, 'Warner 25 Anos', 6),\n (9, 'Plays Metallica By Four Cellos', 7),\n (10, 'Audioslave', 8),\n (11, 'Out Of Exile', 8),\n (12, 'BackBeat Soundtrack', 9),\n (13, 'The Best Of Billy Cobham', 10),\n (14, 'Alcohol Fueled Brewtality Live! [Disc 1]', 11),\n (15, 'Alcohol Fueled Brewtality Live! [Disc 2]', 11),\n (16, 'Black Sabbath', 12),\n (17, 'Black Sabbath Vol. 4 (Remaster)', 12),\n (18, 'Body Count', 13),\n (19, 'Chemical Wedding', 14),\n (20, 'The Best Of Buddy Guy - The Millenium Collection', 15)]\n\n\nOk, when just reading a table, let’s go with pd.read_sql() instead since it returns things in a nicer format!\n\nalbum_results = pd.read_sql(get_albums, con)\nalbum_results\n\n\n  \n    \n\n\n\n\n\n\nAlbumId\nTitle\nArtistId\n\n\n\n\n0\n1\nFor Those About To Rock We Salute You\n1\n\n\n1\n2\nBalls to the Wall\n2\n\n\n2\n3\nRestless and Wild\n2\n\n\n3\n4\nLet There Be Rock\n1\n\n\n4\n5\nBig Ones\n3\n\n\n5\n6\nJagged Little Pill\n4\n\n\n6\n7\nFacelift\n5\n\n\n7\n8\nWarner 25 Anos\n6\n\n\n8\n9\nPlays Metallica By Four Cellos\n7\n\n\n9\n10\nAudioslave\n8\n\n\n10\n11\nOut Of Exile\n8\n\n\n11\n12\nBackBeat Soundtrack\n9\n\n\n12\n13\nThe Best Of Billy Cobham\n10\n\n\n13\n14\nAlcohol Fueled Brewtality Live! [Disc 1]\n11\n\n\n14\n15\nAlcohol Fueled Brewtality Live! [Disc 2]\n11\n\n\n15\n16\nBlack Sabbath\n12\n\n\n16\n17\nBlack Sabbath Vol. 4 (Remaster)\n12\n\n\n17\n18\nBody Count\n13\n\n\n18\n19\nChemical Wedding\n14\n\n\n19\n20\nThe Best Of Buddy Guy - The Millenium Collection\n15"
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#process-for-doing-a-crud-activity",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#process-for-doing-a-crud-activity",
    "title": "Data Storage and Basic SQL",
    "section": "Process for doing a CRUD activity",
    "text": "Process for doing a CRUD activity\n(Create, Read, Update, Delete)\nIf we wanted to do something other than just read a table, we need to follow our general structure from above:\n\nCreate a connection using sqlite3.connect(path)\nCreate a cursor object associated with the connection\nWrite an SQL query as a string\nUse cursor.execute() (or cursor.executemany()) to execute the SQL\nClose the cursor object"
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#common-sql-commands",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#common-sql-commands",
    "title": "Data Storage and Basic SQL",
    "section": "Common SQL commands",
    "text": "Common SQL commands\n\nCREATE TABLE - creates a new table\nINSERT INTO - adds records to a table\nUPDATE - modify existing records\nDELETE FROM - deletes data\nDROP TABLE - removes a table\nSELECT - reads data (use fetchone(), fetchall(), or the returned value as an iterator but we’ll use pd.read_sql() for simple read commands)\n\nAlso many important joins we’ll cover shortly.\nLet’s go through a few of these actions to show how we can write our SQL code and execute it! Note that SQL code is not case sensitive.\n\nCreate a Table\nLet’s start by creating a table. We write the SQL code and specify the name of the table and and variables we want to create.\n\nWe specify the type of data the variable will hold after naming the variable\nCheck this reference for possible data types!\n\n\n#create the cursor instance\ncursor = con.cursor()\n#write our SQL to create a table\n#here we also specify two variables: album and artist along with the type of data they'll hold\nct = \"\"\"\n    CREATE TABLE IF NOT EXISTS justin_music (\n        album TEXT,\n        artist TEXT);\n\"\"\"\n#execute the SQL code\ncursor.execute(ct)\n\n&lt;sqlite3.Cursor at 0x7a23d8f825c0&gt;\n\n\nLet’s check that it worked before we close the connection!\n\npd.read_sql('''\n        SELECT *\n        FROM justin_music;\n        ''', con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\nCool, now let’s close the connection.\n\ncursor = con.cursor()\n\nYou can see we’ll do this process a lot. It may be best to create a function to help us out. (Modified from https://realpython.com/python-sql-libraries/)\n\ndef execute_query(connection, query):\n    cursor = connection.cursor()\n    try:\n        cursor.execute(query)\n        print(\"Query executed successfully\")\n    except Error as e:\n        print(f\"The error '{e}' occurred\")\n    cursor.close()\n\nThe function will execute the query for us and close the cursor.\n\nexecute_query(con, ct)\n\nQuery executed successfully\n\n\n\n\nINSERT INTO\nNow lets add some data to our table using INSERT INTO. We need to pass the table name, optionally the columns we’ll specify, and the values to fill with.\n\ncreate_rows = \"\"\"\n       INSERT INTO\n           justin_music (album, artist)\n       VALUES\n           (\"Sixteen Stone\", \"Bush\"),\n           (\"Listener Supported\", \"Dave Matthews Band\"),\n           (\"Chris Stapleton\", \"Traveler\"),\n           (\"1989\", \"Taylor Swift\");\n\"\"\"\nexecute_query(con, create_rows)\n\nQuery executed successfully\n\n\nLet’s check if it worked!\n\npd.read_sql(\"SELECT * FROM justin_music\", con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n0\nSixteen Stone\nBush\n\n\n1\nListener Supported\nDave Matthews Band\n\n\n2\nChris Stapleton\nTraveler\n\n\n3\n1989\nTaylor Swift\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow let’s add a row with only an artist.\n\ncreate_row = \"\"\"\n       INSERT INTO\n           justin_music (artist)\n       VALUES\n           (\"Taylor Swift\");\n\"\"\"\n\nexecute_query(con, create_row)\n\nQuery executed successfully\n\n\nThis inserts a None into the data frame but note that the value in the original table is actually a NULL.\n\npd.read_sql(\"SELECT * FROM justin_music\", con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n0\nSixteen Stone\nBush\n\n\n1\nListener Supported\nDave Matthews Band\n\n\n2\nChris Stapleton\nTraveler\n\n\n3\n1989\nTaylor Swift\n\n\n4\nNone\nTaylor Swift\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nUPDATE\nWe can use update to modify an existing row of data. We use\n\nUPDATE to specify the table\nSET to specify the values of the columns\nWHERE to describe which records (rows) to modify\n\nWe can use AND, OR, IN, and other logical operators in our SQL code! (See Operators and Parse-Affecting Attributes)\n\nmod_row = \"\"\"\n    UPDATE justin_music\n    SET album = \"Red (Taylor's Version)\", artist = \"Taylor Swift\"\n    WHERE (album is null) AND (artist = \"Taylor Swift\");\n\"\"\"\n\nexecute_query(con, mod_row)\n\nQuery executed successfully\n\n\n\npd.read_sql(\"SELECT * FROM justin_music\", con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n0\nSixteen Stone\nBush\n\n\n1\nListener Supported\nDave Matthews Band\n\n\n2\nChris Stapleton\nTraveler\n\n\n3\n1989\nTaylor Swift\n\n\n4\nRed (Taylor's Version)\nTaylor Swift\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nDELETE FROM\nWe can also delete certain rows. We use\n\nDELETE FROM to specify the table\na WHERE condition to determine what is deleted\n\n\ndel_row = \"\"\"\n    DELETE FROM justin_music\n    WHERE artist = \"Taylor Swift\";\n    \"\"\"\n\nexecute_query(con, del_row)\n\nQuery executed successfully\n\n\n\npd.read_sql(\"SELECT * FROM justin_music\", con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n0\nSixteen Stone\nBush\n\n\n1\nListener Supported\nDave Matthews Band\n\n\n2\nChris Stapleton\nTraveler\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nDROP TABLE\nAnd of course we can remove an entire table using DROP TABLE. Just specify the table to remove!\n\nexecute_query(con, \"DROP TABLE justin_music\")\n\nQuery executed successfully\n\n\n\npd.read_sql(get_schema, con)\n\n\n  \n    \n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nMy table is gone :(\n\n\nSELECT\nAs a statistician, you’d spend most of your time pulling data of interest and then summarizing it, running models, etc.\nSELECT is the workhorse for that task! As we are just reading in data from the database, we can just use pd.read_sql().\nWith SELECT we\n\nSELECT the column(s) we want\nFROM the table of interest\nSpecifing the records (rows) of interest with a WHERE\n\nFirst, let’s see a way to look at all columns in a table.\n\npd.read_sql('SELECT * FROM albums', con)\n\n\n  \n    \n\n\n\n\n\n\nAlbumId\nTitle\nArtistId\n\n\n\n\n0\n1\nFor Those About To Rock We Salute You\n1\n\n\n1\n2\nBalls to the Wall\n2\n\n\n2\n3\nRestless and Wild\n2\n\n\n3\n4\nLet There Be Rock\n1\n\n\n4\n5\nBig Ones\n3\n\n\n...\n...\n...\n...\n\n\n342\n343\nRespighi:Pines of Rome\n226\n\n\n343\n344\nSchubert: The Late String Quartets & String Qu...\n272\n\n\n344\n345\nMonteverdi: L'Orfeo\n273\n\n\n345\n346\nMozart: Chamber Music\n274\n\n\n346\n347\nKoyaanisqatsi (Soundtrack from the Motion Pict...\n275\n\n\n\n\n\n347 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow let’s select the Title and ArtistID columns only. LIMIT is a good way to only return (up to) a certain number of results.\n\npd.read_sql(\"SELECT Title, artistID FROM albums LIMIT 10;\", con)\n\n\n  \n    \n\n\n\n\n\n\nTitle\nArtistId\n\n\n\n\n0\nFor Those About To Rock We Salute You\n1\n\n\n1\nBalls to the Wall\n2\n\n\n2\nRestless and Wild\n2\n\n\n3\nLet There Be Rock\n1\n\n\n4\nBig Ones\n3\n\n\n5\nJagged Little Pill\n4\n\n\n6\nFacelift\n5\n\n\n7\nWarner 25 Anos\n6\n\n\n8\nPlays Metallica By Four Cellos\n7\n\n\n9\nAudioslave\n8\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWHERE allows us to specify conditions on rows.\n\npd.read_sql(\"SELECT * FROM albums WHERE artistID = 2 LIMIT 10;\", con)\n\n\n  \n    \n\n\n\n\n\n\nAlbumId\nTitle\nArtistId\n\n\n\n\n0\n2\nBalls to the Wall\n2\n\n\n1\n3\nRestless and Wild\n2"
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#clean-up",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#clean-up",
    "title": "Data Storage and Basic SQL",
    "section": "Clean Up",
    "text": "Clean Up\nIf we want to save any changes made to our database, we need to do a commit() to the connection via con.commit(). This saves the changes made and releases any locks on the data. Other connections to the database can only see changes you’ve made if you commit them.\nWhen you are done working you should also close your connection. This is done with the close() method.\n\ncon.close()\n\nWe skimmed over some of the syntax that SQL follows. You should read over the arithmetic operators, comparison operators, and the other operators linked on the left of that page."
  },
  {
    "objectID": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html",
    "href": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html",
    "title": "Big Data Basics",
    "section": "",
    "text": "The video below gives a definition of big data and discusses common issues that arise when dealing with big data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html#notes",
    "href": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html#notes",
    "title": "Big Data Basics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version"
  },
  {
    "objectID": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html#additional-readings-for-week-7",
    "href": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html#additional-readings-for-week-7",
    "title": "Big Data Basics",
    "section": "Additional Readings for Week 7",
    "text": "Additional Readings for Week 7\n\nThe Big Data Paradigm\n\nAcademic overview\nOverview 2 (Suse)\nOverview 3 (Oracle)\n\n\n\nDatabases\n\nBasics of Databases(SQL and NoSQL)\nWhat is a database?, what is a relational database? (Oracle)\nPython SQL Libraries (realpython - a nice site but some stuff goes out of date!)\nSQLite in python (tutorial from pynative.com)\nSQLite schema table (sqlite.org documentation - a lot of other useful stuff there)\nThree plus table joins (learnsql.com)\n\n\n\nData Storage\n\nThe first answer here is useful to read\nData Warehouses: Article 1 (Oracle), Article 2 (Amazon)\nDatabases and Data Warehouses: Article 1, Article 2\nData Marts (IBM)\nMDM (SAS)\nDatabases, Data Warehouses, and Data Lake: Article 1, Article 2, Article 3\nData Lakes: Article 1, Article 2\nLake House (databricks - they actually have a lot of useful training resources as well!)\n\n\n\nBig Data Storage\n\nData pipelines: Article 1, Article 2\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/27-LASSO_Landing.html",
    "href": "01_Programming_in_python/27-LASSO_Landing.html",
    "title": "LASSO Models",
    "section": "",
    "text": "The video below describes the Least Angle Subset and Selection Operator (LASSO). This is a method for fitting an MLR type model with a penalty term involved. This clever penalty term shrinks the coefficients estimates towards zero, setting some coefficient estimates exactly to zero (hence doing variable selection). This model also introduces the notion of a tuning parameter. This tuning parameter must be selected and is usually done so using CV.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/27-LASSO_Landing.html#notes",
    "href": "01_Programming_in_python/27-LASSO_Landing.html#notes",
    "title": "LASSO Models",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThis wraps up the material for week 5! Your first project (along with the exam this week) is the next big assessment.\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/25-Cross_Validation_Landing.html",
    "href": "01_Programming_in_python/25-Cross_Validation_Landing.html",
    "title": "Cross Validation",
    "section": "",
    "text": "The video below talks about the idea of cross validation (CV). This is a method of evaluating a predictive model while not necessarily needing to split the data into a training and test set (although sometimes we will still do this and just apply CV on the training set).\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/25-Cross_Validation_Landing.html#notes",
    "href": "01_Programming_in_python/25-Cross_Validation_Landing.html#notes",
    "title": "Cross Validation",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/23-Fitting_Evaluating_SLR_Models_Landing.html",
    "href": "01_Programming_in_python/23-Fitting_Evaluating_SLR_Models_Landing.html",
    "title": "Fitting & Evaluating SLR Models",
    "section": "",
    "text": "The video below introduces the language and ideas around fitting and evaluating predictive models using the very basic simple linear regression (SLR) model as the example. This simple model is ubiquitous and is a decent starting point for modeling a quantitative response.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/23-Fitting_Evaluating_SLR_Models_Landing.html#notes",
    "href": "01_Programming_in_python/23-Fitting_Evaluating_SLR_Models_Landing.html#notes",
    "title": "Fitting & Evaluating SLR Models",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html",
    "href": "01_Programming_in_python/21-Error_Handling.html",
    "title": "Error Handling",
    "section": "",
    "text": "We’ve already learned a lot about creating our own functions. One thing we haven’t touched on is how to have our functions fail in a good way. That is, when an error occurs, can we either avoid it and finish the function execution or can we at least provide a clear explanation of what error came up?\nTo do so we’ll talk about try/catch logic. Here we try some code and can plan to catch certain errors and do something about them.\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html#errors-when-programming",
    "href": "01_Programming_in_python/21-Error_Handling.html#errors-when-programming",
    "title": "Error Handling",
    "section": "Errors When Programming",
    "text": "Errors When Programming\nCommonly you’ll have syntax errors and exceptions\n\nA Syntax error is when you typed something in wrong (and it can’t be parsed by python)\n\n\nfor x in range(0,10) #missing colon\n    print(x)\n\nSyntaxError: expected ':' (&lt;ipython-input-1-e60b965da02e&gt;, line 1)\n\n\nSyntaxError: invalid syntax (&lt;string&gt;, line 1)\n\nThese aren’t things we can fix on the backend. Code needs to be typed in correctly!\nAn exception occurs when python can’t execute your code (during the execution something bad happens)\n\n\nprint(\"the number is \" + 10)\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\nTypeError: can only concatenate str (not \"int\") to str"
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html#dealing-with-exceptions",
    "href": "01_Programming_in_python/21-Error_Handling.html#dealing-with-exceptions",
    "title": "Error Handling",
    "section": "Dealing with Exceptions",
    "text": "Dealing with Exceptions\nExceptions can be dealt with to some degree! Consider this function to print out strings passed by the user.\n\ndef print_strings(*x): #should all be strings!\n    c = 0\n    for i in x:\n        print(\"The value in position \" + str(c) + \" is: \" + i)\n        c += 1\n\nprint_strings(\"cat\", \"dog\", \"bird\")\n\nThe value in position 0 is: cat\nThe value in position 1 is: dog\nThe value in position 2 is: bird\n\n\n\nIf we pass a non-string, this will throw an exception.\n\n\ndef print_strings(*x): #should all be strings!\n    c = 0\n    for i in x:\n        print(\"The value in position \" + str(c) + \" is: \" + i)\n        c += 1\n\nprint_strings(\"cat\", 1, \"bird\")\n\nThe value in position 0 is: cat\n\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nNote that it says TypeError at the beginning of that big error message. What we can do is run a try block and set up what to do when we get certain exceptions via an except block.\nThe syntax for these is simlar to if and else blocks\n\n\ndef print_strings(*x):\n    c = 0\n    for i in x:\n        try: #try the code that is indented below here\n            print(\"The value in position \" + str(c) + \" is: \" + i)\n        except TypeError: #if we get a TypeError in the previous try block, do this and then continue execution!\n            print(\"Oh no! Not a string\")\n        c += 1\nprint_strings(\"cat\", \"dog\", 1, \"bird\")\n\nThe value in position 0 is: cat\nThe value in position 1 is: dog\nOh no! Not a string\nThe value in position 3 is: bird\n\n\n\nCan have multiple except statements and an else block to account for many situations\nLet’s create a quick function to printout information about a person given as key/value pairs\n\n\ndef print_stuff(**x): #now taking key value pairs (a dictionary within the function)\n    print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\") #we must have a Name argument\n    for key in x: #now run through the other info given and print it out\n        print(\"\\t\", key, \" : \", str(x[key])) #\\t is a tab\n\nprint_stuff(Name = \"Jack London\", Age = 41, Job = \"Writer\")\n\nPay special attention to Jack London\nHis attributes are:\n     Age  :  41\n     Job  :  Writer\n\n\nIf we don’t have a Name argument passed, we’d have an error!\n\nprint_stuff(Person = \"Jack London\", Age = 41, Job = \"Writer\")\n\nKeyError: 'Name'\n\n\nIn this case we get a KeyError (this is when we look for a certain key in a dictionary (or other similar object) and can’t find it) - We can look for this kind of error - Also look for other errors that might come up\n\ndef print_stuff(**x): #now taking key value pairs\n    try:\n        print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\")\n    except TypeError: #If x.pop(\"Name\") doesn't give a string we'll get a TypeError\n        print(\"Oh no! 'Name' is not a string\")\n    except KeyError: #If the user didn't give a \"Name\" argument we'll get a KeyError\n        print(\"You didn't supply a 'Name'!\")\n    for key in x: #Now print out the rest of the info about the person\n        print(\"\\t\", key, \" : \", str(x[key]))\nprint_stuff(Name = \"Jack London\", Age = 41, Job = \"Writer\")\n\nPay special attention to Jack London\nHis attributes are:\n     Age  :  41\n     Job  :  Writer\n\n\n\nprint_stuff(Name = 11, Age = 41, Job = \"Writer\")\n\nOh no! 'Name' is not a string\n     Age  :  41\n     Job  :  Writer\n\n\n\nprint_stuff(Person = \"Jack London\", Age = 41, Job = \"Writer\")\n\nYou didn't supply a 'Name'!\n     Person  :  Jack London\n     Age  :  41\n     Job  :  Writer\n\n\nAn else block can be given that is similar to what we use with if, elif logic - This specifies what to do if things aren’t accounted for above\n\ndef print_stuff(**x): #now taking key value pairs\n    try:\n        print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\")\n    except TypeError:\n        print(\"Oh no! 'Name' is not a string\")\n    except KeyError:\n        print(\"You didn't supply a 'Name'!\")\n    else: #if no errors occurred\n        print(\"(Valid name by the way - you rule)\")\n    for key in x:\n        print(\"\\t\", key, \" : \", str(x[key]))\n\nprint_stuff(Name = \"Jack London\", Age = 41, Job = \"Writer\")\n\nPay special attention to Jack London\nHis attributes are:\n(Valid name by the way - you rule)\n     Age  :  41\n     Job  :  Writer\n\n\n\nA finally clause can be given to always execute at end regardless\nLike an else block but it always runs\n\n\ndef print_stuff(**x): #now taking key value pairs\n    print(x)\n    try:\n        print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\")\n    except TypeError:\n        print(\"Oh no! 'Name' is not a string\")\n    except KeyError:\n        print(\"You didn't supply a 'Name'!\")\n    else:\n        print(\"(Valid name by the way)\")\n    finally:\n        print(\"This string prints no matter what\")\n    for key in x:\n        print(\"\\t\", key, \" : \", str(x[key]))\n\nprint_stuff(name = \"Jack London\", Age = 41, Job = \"Writer\")\n\n{'name': 'Jack London', 'Age': 41, 'Job': 'Writer'}\nYou didn't supply a 'Name'!\nThis string prints no matter what\n     name  :  Jack London\n     Age  :  41\n     Job  :  Writer"
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html#raising-an-exception-yourself",
    "href": "01_Programming_in_python/21-Error_Handling.html#raising-an-exception-yourself",
    "title": "Error Handling",
    "section": "Raising an Exception Yourself",
    "text": "Raising an Exception Yourself\n\nYou can define your own exceptions to be more descriptive!\nIf we try to divide by 0 we get a ZeroDivisionError exception\n\n\n3/0\n\nZeroDivisionError: division by zero\n\n\n\nInstead we can raise our own exception\n\n\ndef my_divide(x, y):\n    if y == 0:\n        raise Exception(\"Can't divide by 0...\")\n    return x/y\n\n\nmy_divide(3, 9) #works\n\n0.3333333333333333\n\n\n\nmy_divide(3,0) #raises our custom exception and provides that note!\n\nException: Can't divide by 0..."
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html#quick-video",
    "href": "01_Programming_in_python/21-Error_Handling.html#quick-video",
    "title": "Error Handling",
    "section": "Quick Video",
    "text": "Quick Video\nThis video shows an example of using error control! Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=cd799e52-f4f1-4531-8174-b10301708125&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")"
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html",
    "title": "Plotting with matplotlib",
    "section": "",
    "text": "Remember that our first steps with a data set are generally to try and get to know our data through an exploratory data analysis (EDA).\nOur overall goal is to describe the distributions of our variables. We have two types of variables: - categorical - data values represent labels - numerical - data values are numeric where we can do math on the values\nHow we summarize the distribution depends on which types of variable(s) we are working with. We’ve discussed how to find numerical summaries already: - frequencies for categorical variables (via contingency tables) - measures of center and spread for numeric variables\nThese types of measures are great for telling us about certain aspects of a distribution, such as a quick measure of center or spread, and are very useful for comparing two distributions. However, we often want to understand the shape of the distribution a variable might take on. This is best investigated via graphical summaries!\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html#plotting-systems-in-python",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html#plotting-systems-in-python",
    "title": "Plotting with matplotlib",
    "section": "Plotting Systems in python",
    "text": "Plotting Systems in python\n\nmatplotlib: based on matlab plotting. Similar to base R plotting\nseaborn: an abstraction of matplotlib but still growing\nBokeh: for interactive visuals via HTML\nplotly: general plotting system that has a python module\nplotnine: a ggplot port"
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html#plotting-with-matplotlib",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html#plotting-with-matplotlib",
    "title": "Plotting with matplotlib",
    "section": "Plotting with matplotlib",
    "text": "Plotting with matplotlib\n\nTwo APIs (or ways to interact with matplotlib)\n\nExplicit axes interface (object oriented api)\nImplicit pyplot interface (what we’ll cover)\n\nWhen using the implicit API we use functions like\n\nplt.figure(), plt.plot(...), plt.scatter(), plt.bar(), or plt.hist()\n\nWe then determine axes and artist elements\n\nWe add labels, legends, and annotations\nFinally we produce the plot (and would then usually close the plot to denote that we are done working on it - not usually needed when programming in jupyter notebooks)\n\nplt.show() then plt.close()\n\n\n\nReading in Data to Plot\n\nConsider data on titanic passengers in titanic.csv\nThis is a really common dataset to play around with\nLet’s start with a focus on plotting categorical data\n\nWe start by importing matplotlib.pyplot as plt. This is a common reference. The pyplot module has the functions we’ll use to do our plotting such as pyplot.hist() or pyplot.plot().\n\nimport matplotlib.pyplot as plt\n\nNow we’ll read in the titanic dataset using pandas. This dataset is available at: https://www4.stat.ncsu.edu/~online/datasets/titanic.csv\n\nimport pandas as pd\n#readin data\ntitanic_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/titanic.csv\")\n\nSome of the variables in the data have a lot of missing values. Let’s drop those. We also saw that converting some variables to category type variables was useful for having better labeling. Let’s do that as well.\n\n#remove some columns and a bad row\nsub_titanic_data = titanic_data.drop(columns = [\"body\", \"cabin\", \"boat\"], axis = 1) \\\n                               .iloc[:(titanic_data.shape[0]-1)]\n\n#create category versions of the variables\n#embarked variable\nsub_titanic_data[\"embarkedC\"] = sub_titanic_data.embarked.astype(\"category\")\nsub_titanic_data.embarkedC = sub_titanic_data.embarkedC.cat.rename_categories(\n                                    [\"Cherbourg\", \"Queenstown\", \"Southampton\"])\n#sex variable\nsub_titanic_data[\"sexC\"] = sub_titanic_data.sex.astype(\"category\")\nsub_titanic_data.sexC = sub_titanic_data.sexC.cat.rename_categories([\"Female\", \"Male\"])\n#survived variable\nsub_titanic_data[\"survivedC\"] = sub_titanic_data.survived.astype(\"category\")\nsub_titanic_data.survivedC = sub_titanic_data.survivedC.cat.rename_categories([\"Died\", \"Survived\"])\n\n\n\n\nBarplots\nCategorical variable - entries are a label or attribute\nOur goal is to describe the distribution of these variables. We do this by creating summary counts or frequncy counts\n\nBarplots give a visual of those counts!\n\nUse plt.bar()\n\nx represents the categories\nheight the corresponding heights\n\n\n\nWe have three categorical variables we’ll investigate. Let’s start with the embarkedC variable.\nWe know the x values (the category labels). We just need the heights to plot. We can find the heights by creating a one-way contingency table!\n\ntable = sub_titanic_data.embarkedC.value_counts()\ntable\n\n\n\n\n\n\n\n\ncount\n\n\nembarkedC\n\n\n\n\n\nSouthampton\n914\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\n\n\ndtype: int64\n\n\nNow we’ll use plt.bar() and map the categories (via calling the .categories attribute from our column of data) to x and the contingency table counts to height.\nThe x and height values just need to be paired up.\n\n#get the categories\nprint(sub_titanic_data.embarkedC.cat.categories)\n#note that the ordering does not line up with the counts\nprint(table)\n\nIndex(['Cherbourg', 'Queenstown', 'Southampton'], dtype='object')\nembarkedC\nSouthampton    914\nCherbourg      270\nQueenstown     123\nName: count, dtype: int64\n\n\nAs the ordering isn’t the same, we’ll have to be careful to make sure things are paired up appropriately!\n\nindex = [1, 2, 0]\ntable[index]\n\nFutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  table[index]\n\n\n\n\n\n\n\n\n\ncount\n\n\nembarkedC\n\n\n\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\nSouthampton\n914\n\n\n\n\ndtype: int64\n\n\n\nplt.bar(x = sub_titanic_data.embarkedC.cat.categories,  height = table[index])\n#plt.show() would be needed if we weren't in a notebook\n\n&lt;BarContainer object of 3 artists&gt;\n\n\n\n\n\nNice! A good looking barplot. This is our first step. Next we want to make it look a bit nicer by adding labels, legends, and annotations.\nIn this case, we can add a better xlabel, ylabel, and title.\n\nplt.bar(x = sub_titanic_data.embarkedC.cat.categories,  height =  table[index])\n#as these are all being run in the same cell, these get added to the plot created\nplt.xlabel(\"Port Embarked\")\nplt.ylabel(\"Number of People\")\nplt.title(\"Most Embarked in the Southampton Port\")\n\nText(0.5, 1.0, 'Most Embarked in the Southampton Port')\n\n\n\n\n\n\nA common way to resize the plot is to first call plt.subplots() and specify the figsize argument. We give this a tuple of the width and height we want.\n\n\nplt.subplots(figsize = (12, 5))\nplt.bar(x = sub_titanic_data.embarkedC.cat.categories,  height =  table[index])\nplt.xlabel(\"Port Embarked\")\nplt.ylabel(\"Number of People\")\nplt.title(\"Most Embarked in the Southampton Port\")\n\nText(0.5, 1.0, 'Most Embarked in the Southampton Port')\n\n\n\n\n\n\n\nStacked Barplot with matplotlib\nIf we want to include a second categorical variable in our plot we can do so in a few ways. The first is to color the bars by the values of the other variable. In this way we can see how that variable distributes across the categories of our current variable!\n\nThe first step is to create the table of counts for our two variables\nWe’ll do this via the pd.crosstab() function\n\n\nstack_table = pd.crosstab(sub_titanic_data.embarkedC, sub_titanic_data.survivedC)\nstack_table\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\n\n\nembarkedC\n\n\n\n\n\n\nCherbourg\n120\n150\n\n\nQueenstown\n79\n44\n\n\nSouthampton\n610\n304\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nTo manually build this bar plot with plt.bar() we can do the following: - Put our embarked variable labels on the x-axis - Create bars with heights corresponding to the counts for Died. - Create a second set of bars that sit directly on top of those bars with heights corresponding to the Survived counts - These bars should be different colors to denote the Died vs Survived counts!\nRemember that as we work on a plot within a cell, we continue to add to it unless we use plt.show(). This means we can use two calls to plt.bar() within the same cell and it will keep drawing on the same plot.\n\n#we want to get the heights for the Died bars\nstack_table.loc[:, \"Died\"]\n\n\n\n\n\n\n\n\nDied\n\n\nembarkedC\n\n\n\n\n\nCherbourg\n120\n\n\nQueenstown\n79\n\n\nSouthampton\n610\n\n\n\n\ndtype: int64\n\n\n\n#notice that these now line up with our category order so we don't need to change that\nsub_titanic_data.embarkedC.cat.categories\n\nIndex(['Cherbourg', 'Queenstown', 'Southampton'], dtype='object')\n\n\nOur first step is to plot these on a bar plot. We’ll add a label argument to plt.bar() which will make it easy to add a legend at the end.\n\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Died\"],\n  label = \"Died\")\n\n&lt;BarContainer object of 3 artists&gt;\n\n\n\n\n\nNow we want to find the Survived counts and put those on top of these bars.\n\nstack_table.loc[:, \"Survived\"]\n\n\n\n\n\n\n\n\nSurvived\n\n\nembarkedC\n\n\n\n\n\nCherbourg\n150\n\n\nQueenstown\n44\n\n\nSouthampton\n304\n\n\n\n\ndtype: int64\n\n\nWe can add a bottom = argument to our second plt.bar() call to specify where the bottom of the bars should start (the counts of the Died!)\n\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Died\"],\n  label = \"Died\")\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Survived\"],\n  bottom = stack_table.loc[:, \"Died\"],\n  label = \"Survived\"\n)\n\n&lt;BarContainer object of 3 artists&gt;\n\n\n\n\n\nNice! Colors are automatically created for us. Now we just need to add some nice labels to help us understand the plot.\nWe’ll use plt.legend() to produce a legend based off the labels used.\n\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Died\"],\n  label = \"Died\")\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Survived\"],\n  bottom = stack_table.loc[:, \"Died\"],\n  label = \"Survived\"\n  )\nplt.xlabel(\"Port Embarked\")\nplt.ylabel(\"Number of People\")\nplt.title(\"Most Embarked in the Southampton Port \\n A higher proportion survived from Cherbourg\")\nplt.legend(loc = 0)\n\n&lt;matplotlib.legend.Legend at 0x796e528860e0&gt;\n\n\n\n\n\n\n\n\nSide-by-Side Barplot with matplotlib\nAlternative to the stacked bar plot is the side-by-side bar plot. This is the same idea but we put the bars for the categories next to each other instead of on top of eachother.\nThis is similar to our first bar plot but we need to have different x locations for each bar!\nLet’s take this bar plot of just the Died with port of embarkment.\nWe can change the x values from the categories to numbers.\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  label = \"Died\")\n\n&lt;BarContainer object of 3 artists&gt;\n\n\n\n\n\nNow we can specify the widths of the bars via the width argument. By default they are almost 1 here. Let’s make them smaller.\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  width = 0.4,\n  label = \"Died\")\n\n&lt;BarContainer object of 3 artists&gt;\n\n\n\n\n\nOk, now let’s just fix the x-axis labels! This can be done by using plt.xticks(). Here we specify the x values where we want our axis values to go along with corresponding labels.\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  width = 0.4,\n  label = \"Died\")\nplt.xticks([1, 2, 3], sub_titanic_data.embarkedC.cat.categories)\n\n([&lt;matplotlib.axis.XTick at 0x796e52385c30&gt;,\n  &lt;matplotlib.axis.XTick at 0x796e52385c00&gt;,\n  &lt;matplotlib.axis.XTick at 0x796e52385450&gt;],\n [Text(1, 0, 'Cherbourg'),\n  Text(2, 0, 'Queenstown'),\n  Text(3, 0, 'Southampton')])\n\n\n\n\n\nSweet! Now we just add the bars for the Survived group next to these!\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  width = 0.4,\n  label = \"Died\")\nplt.bar(\n  x = [1.4, 2.4, 3.4],\n  height = stack_table.loc[:, \"Survived\"],\n  width = 0.4,\n  label = \"Survived\")\nplt.xticks([1.2, 2.2, 3.2], sub_titanic_data.embarkedC.cat.categories)\n\n([&lt;matplotlib.axis.XTick at 0x796e523f05e0&gt;,\n  &lt;matplotlib.axis.XTick at 0x796e523f05b0&gt;,\n  &lt;matplotlib.axis.XTick at 0x796e523a5ff0&gt;],\n [Text(1.2, 0, 'Cherbourg'),\n  Text(2.2, 0, 'Queenstown'),\n  Text(3.2, 0, 'Southampton')])\n\n\n\n\n\nNow we’ll fancy it up with some labels and titles.\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  width = 0.4,\n  label = \"Died\")\nplt.bar(\n  x = [1.4, 2.4, 3.4],\n  height = stack_table.loc[:, \"Survived\"],\n  width = 0.4,\n  label = \"Survived\")\nplt.xticks([1.2, 2.2, 3.2], sub_titanic_data.embarkedC.cat.categories)\nplt.xlabel(\"Port Embarked\")\nplt.ylabel(\"Number of People\")\nplt.legend(loc = 0)\nplt.title(\"Most Embarked in the Southampton Port \\n A higher proportion survived from Cherbourg\")\n\nText(0.5, 1.0, 'Most Embarked in the Southampton Port \\n A higher proportion survived from Cherbourg')\n\n\n\n\n\n\n\n\n\nPlotting Numeric Variables\nWhoa - that was way too much work to create side-by-side bar plots… We could go through similar processes to create histograms, scatterplots, etc…\nFunctions like plt.scatter() aren’t bad to work with:\n\nplt.scatter(sub_titanic_data.age, sub_titanic_data.fare)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Fare\")\n\nText(0, 0.5, 'Fare')\n\n\n\n\n\nBut customizing the plots is a good bit of work. pandas has functionality to do plotting on data frames that will save us time!\nHowever, it is really useful to know the basics of matplotlib as many of the plotting systems are built on it!"
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html#quick-video",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html#quick-video",
    "title": "Plotting with matplotlib",
    "section": "Quick Video",
    "text": "Quick Video\nThis video shows an example of using matplotlib plotting! Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=380014c4-f479-4ab1-b0d6-b1030168e8d1&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")"
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html#recap",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html#recap",
    "title": "Plotting with matplotlib",
    "section": "Recap",
    "text": "Recap\n\nMust understand the type of data you have to visualize it\nGoal: Describe the distribution\nmatplotlib can create custom plots\n\nLots of work to specify everything yourself\n\nMany other plotting paradigms to consider!\n\npandas and seaborn next\n\n\nIf you are on the course website, use the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!\nIf you are on Google Colab, head back to our course website for our next lesson!"
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html",
    "title": "Numerical Summaries",
    "section": "",
    "text": "Note: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#understand-how-data-is-stored",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#understand-how-data-is-stored",
    "title": "Numerical Summaries",
    "section": "Understand How Data is Stored",
    "text": "Understand How Data is Stored\nFirst, let’s read in some data. Recall, for .csv files (comma separated value files) we can read them in using pandas read_csv() function.\nWe’ll read in the classic titanic data set.\n\nimport pandas as pd\ntitanic_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/titanic.csv\")\n\n\nThe .info() method allows us to see how our variables are stored (among other things)\nColumn data types should make sense for what you expect!\n\n\ntitanic_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1310 entries, 0 to 1309\nData columns (total 14 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   pclass     1309 non-null   float64\n 1   survived   1309 non-null   float64\n 2   name       1309 non-null   object \n 3   sex        1309 non-null   object \n 4   age        1046 non-null   float64\n 5   sibsp      1309 non-null   float64\n 6   parch      1309 non-null   float64\n 7   ticket     1309 non-null   object \n 8   fare       1308 non-null   float64\n 9   cabin      295 non-null    object \n 10  embarked   1307 non-null   object \n 11  boat       486 non-null    object \n 12  body       121 non-null    float64\n 13  home.dest  745 non-null    object \ndtypes: float64(7), object(7)\nmemory usage: 143.4+ KB\n\n\n\n.head() and .tail() help to see what we have as well\n\n\ntitanic_data.head() #clearly some missing values with NaNs\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n0\n1.0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nB5\nS\n2\nNaN\nSt Louis, MO\n\n\n1\n1.0\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\n11\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\n0.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\n0.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\n135.0\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\n0.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ntitanic_data.tail() #note the last row of NaN (not a number)\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n1305\n3.0\n0.0\nZabour, Miss. Thamine\nfemale\nNaN\n1.0\n0.0\n2665\n14.4542\nNaN\nC\nNaN\nNaN\nNaN\n\n\n1306\n3.0\n0.0\nZakarian, Mr. Mapriededer\nmale\n26.5\n0.0\n0.0\n2656\n7.2250\nNaN\nC\nNaN\n304.0\nNaN\n\n\n1307\n3.0\n0.0\nZakarian, Mr. Ortin\nmale\n27.0\n0.0\n0.0\n2670\n7.2250\nNaN\nC\nNaN\nNaN\nNaN\n\n\n1308\n3.0\n0.0\nZimmerman, Mr. Leo\nmale\n29.0\n0.0\n0.0\n315082\n7.8750\nNaN\nS\nNaN\nNaN\nNaN\n\n\n1309\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#do-basic-data-validation",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#do-basic-data-validation",
    "title": "Numerical Summaries",
    "section": "Do Basic Data Validation",
    "text": "Do Basic Data Validation\n\nUse the describe() method on a data frame\nCheck that the min’s, max’s, etc. all make sense!\n\n\ntitanic_data.describe()\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nage\nsibsp\nparch\nfare\nbody\n\n\n\n\ncount\n1309.000000\n1309.000000\n1046.000000\n1309.000000\n1309.000000\n1308.000000\n121.000000\n\n\nmean\n2.294882\n0.381971\n29.881135\n0.498854\n0.385027\n33.295479\n160.809917\n\n\nstd\n0.837836\n0.486055\n14.413500\n1.041658\n0.865560\n51.758668\n97.696922\n\n\nmin\n1.000000\n0.000000\n0.166700\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n25%\n2.000000\n0.000000\n21.000000\n0.000000\n0.000000\n7.895800\n72.000000\n\n\n50%\n3.000000\n0.000000\n28.000000\n0.000000\n0.000000\n14.454200\n155.000000\n\n\n75%\n3.000000\n1.000000\n39.000000\n1.000000\n0.000000\n31.275000\n256.000000\n\n\nmax\n3.000000\n1.000000\n80.000000\n8.000000\n9.000000\n512.329200\n328.000000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nRecall we can subset our columns with []\n\n\ntitanic_data.columns\n\nIndex(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n      dtype='object')\n\n\n\nWe can determine which percentiles of selected columns to return by combining the column subsetting via selection brackets [] and the .describe() method\n\n\ntitanic_data[[\"age\", \"sibsp\", \"parch\", \"fare\"]].describe(percentiles = [0.05, 0.25, 0.99])\n\n\n  \n    \n\n\n\n\n\n\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n1046.000000\n1309.000000\n1309.000000\n1308.000000\n\n\nmean\n29.881135\n0.498854\n0.385027\n33.295479\n\n\nstd\n14.413500\n1.041658\n0.865560\n51.758668\n\n\nmin\n0.166700\n0.000000\n0.000000\n0.000000\n\n\n5%\n5.000000\n0.000000\n0.000000\n7.225000\n\n\n25%\n21.000000\n0.000000\n0.000000\n7.895800\n\n\n50%\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n99%\n65.000000\n5.000000\n4.000000\n262.375000\n\n\nmax\n80.000000\n8.000000\n9.000000\n512.329200"
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#determine-rate-of-missing-values",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#determine-rate-of-missing-values",
    "title": "Numerical Summaries",
    "section": "Determine Rate of Missing Values",
    "text": "Determine Rate of Missing Values\n\nUse is.null() method to determine the missing values\n\n\ntitanic_data.isnull()\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1305\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n1306\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n1307\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n1308\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n1309\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n1310 rows × 14 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nYikes! Can’t make heads or tails of that.\nThis is a DataFrame of booleans!\nUse the .sum() method to see how many null values we have for each column\n\n\ntitanic_data.isnull().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\npclass\n1\n\n\nsurvived\n1\n\n\nname\n1\n\n\nsex\n1\n\n\nage\n264\n\n\nsibsp\n1\n\n\nparch\n1\n\n\nticket\n1\n\n\nfare\n2\n\n\ncabin\n1015\n\n\nembarked\n3\n\n\nboat\n824\n\n\nbody\n1189\n\n\nhome.dest\n565\n\n\n\n\ndtype: int64\n\n\n\nThis type of multiple method use gives us a good chance to use our \\ operator to create more readable code by making it multi-line\n\n\ntitanic_data.isnull() \\\n  .sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\npclass\n1\n\n\nsurvived\n1\n\n\nname\n1\n\n\nsex\n1\n\n\nage\n264\n\n\nsibsp\n1\n\n\nparch\n1\n\n\nticket\n1\n\n\nfare\n2\n\n\ncabin\n1015\n\n\nembarked\n3\n\n\nboat\n824\n\n\nbody\n1189\n\n\nhome.dest\n565\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#clean-up-data-as-needed",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#clean-up-data-as-needed",
    "title": "Numerical Summaries",
    "section": "Clean Up Data As Needed",
    "text": "Clean Up Data As Needed\n\nWe can remove rows with missing using .dropna() method\nFirst, remove the cabin, boat, and body variables since they have so many missing values\n\nIf we want to just remove some columns, can use the .drop() method\n\n\n\nsub_titanic_data = titanic_data.drop(columns = [\"body\", \"cabin\", \"boat\"])\nsub_titanic_data.shape\n\n(1310, 11)\n\n\n\nCheck on the missingness now\n\n\nsub_titanic_data.isnull().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\npclass\n1\n\n\nsurvived\n1\n\n\nname\n1\n\n\nsex\n1\n\n\nage\n264\n\n\nsibsp\n1\n\n\nparch\n1\n\n\nticket\n1\n\n\nfare\n2\n\n\nembarked\n3\n\n\nhome.dest\n565\n\n\n\n\ndtype: int64\n\n\n\nNow we are ready to use the .dropna() method to remove any rows with missing data\n\n\ntemp = sub_titanic_data.dropna()\ntemp.shape #notice the reduction in rows\n\n(684, 11)\n\n\n\ntemp.isnull().sum() #no more missing values\n\n\n\n\n\n\n\n\n0\n\n\n\n\npclass\n0\n\n\nsurvived\n0\n\n\nname\n0\n\n\nsex\n0\n\n\nage\n0\n\n\nsibsp\n0\n\n\nparch\n0\n\n\nticket\n0\n\n\nfare\n0\n\n\nembarked\n0\n\n\nhome.dest\n0\n\n\n\n\ndtype: int64\n\n\n\nUsually, you don’t want to drop all the rows with any missing data as you are throwing out useful info.\nOne option is to impute the missing values… this can be dangerous but can be done with .fillna() method\n\n\nsub_titanic_data.fillna(value = 0) #note, for instance, some values of age are 0 now and the last row is all 0 values\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\nembarked\nhome.dest\n\n\n\n\n0\n1.0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nS\nSt Louis, MO\n\n\n1\n1.0\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\n0.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\n0.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\n0.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1305\n3.0\n0.0\nZabour, Miss. Thamine\nfemale\n0.0000\n1.0\n0.0\n2665\n14.4542\nC\n0\n\n\n1306\n3.0\n0.0\nZakarian, Mr. Mapriededer\nmale\n26.5000\n0.0\n0.0\n2656\n7.2250\nC\n0\n\n\n1307\n3.0\n0.0\nZakarian, Mr. Ortin\nmale\n27.0000\n0.0\n0.0\n2670\n7.2250\nC\n0\n\n\n1308\n3.0\n0.0\nZimmerman, Mr. Leo\nmale\n29.0000\n0.0\n0.0\n315082\n7.8750\nS\n0\n\n\n1309\n0.0\n0.0\n0\n0\n0.0000\n0.0\n0.0\n0\n0.0000\n0\n0\n\n\n\n\n\n1310 rows × 11 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nCan set the value you want to impute by passing a dictionary of key/value pairs\n\n\nsub_titanic_data.fillna(value = {\"home.dest\": \"Unknown\", \"age\": 200})\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\nembarked\nhome.dest\n\n\n\n\n0\n1.0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nS\nSt Louis, MO\n\n\n1\n1.0\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\n0.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\n0.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\n0.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1305\n3.0\n0.0\nZabour, Miss. Thamine\nfemale\n200.0000\n1.0\n0.0\n2665\n14.4542\nC\nUnknown\n\n\n1306\n3.0\n0.0\nZakarian, Mr. Mapriededer\nmale\n26.5000\n0.0\n0.0\n2656\n7.2250\nC\nUnknown\n\n\n1307\n3.0\n0.0\nZakarian, Mr. Ortin\nmale\n27.0000\n0.0\n0.0\n2670\n7.2250\nC\nUnknown\n\n\n1308\n3.0\n0.0\nZimmerman, Mr. Leo\nmale\n29.0000\n0.0\n0.0\n315082\n7.8750\nS\nUnknown\n\n\n1309\nNaN\nNaN\nNaN\nNaN\n200.0000\nNaN\nNaN\nNaN\nNaN\nNaN\nUnknown\n\n\n\n\n\n1310 rows × 11 columns"
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#investigate-distributions",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#investigate-distributions",
    "title": "Numerical Summaries",
    "section": "Investigate distributions",
    "text": "Investigate distributions\n\nHow to summarize data depends on the type of data\n\nCategorical (Qualitative) variable - entries are a label or attribute\n\nNumeric (Quantitative) variable - entries are a numerical value where math can be performed\n\nNumerical summaries (across subgroups)\n\nContingency Tables (for categorical data)\nMean/Median\n\nStandard Deviation/Variance/IQR\nQuantiles/Percentiles\n\nGraphical summaries (across subgroups)\n\nBar plots (for categorical data)\nHistograms\n\nBox plots\n\nScatter plots\n\n\n\n\nCategorical Data\nGoal: Describe the distribution of the variable\n\nDistribution = pattern and frequency with which you observe a variable\n\nCategorical variable - entries are a label or attribute\n\nDescribe the relative frequency (or count) for each category\nUsing pandas .value_counts() method and crosstab() function\n\n\nVariables of interest for this section: + embarked (where journey started)\n\nsub_titanic_data.embarked[0:2]\n\n\n\n\n\n\n\n\nembarked\n\n\n\n\n0\nS\n\n\n1\nS\n\n\n\n\ndtype: object\n\n\n\ntype(sub_titanic_data.embarked[0])\n\nstr\n\n\nThe str type isn’t ideal for summarizaitons. A different data type is better!\n\nCategory Type Variables\nA category type variable is really useful for categorical variables.\n\nAkin to a factor variable in R (if you know those)\nCan have more descriptive labels, ordering of categories, etc.\n\nLet’s give the embarked variable more descriptive values and by converting it to a category type and manipulating it that way.\n\nsub_titanic_data[\"embarkedC\"] = sub_titanic_data.embarked.astype(\"category\")\nsub_titanic_data.embarkedC[0:2]\n\n\n\n\n\n\n\n\nembarkedC\n\n\n\n\n0\nS\n\n\n1\nS\n\n\n\n\ndtype: category\n\n\n\nNow we can use the .cat.rename_categories() method on this category variable\n\n\nsub_titanic_data.embarkedC = sub_titanic_data.embarkedC.cat.rename_categories([\"Cherbourg\", \"Queenstown\", \"Southampton\"])\nsub_titanic_data.embarkedC[0:2]\n\n\n\n\n\n\n\n\nembarkedC\n\n\n\n\n0\nSouthampton\n\n\n1\nSouthampton\n\n\n\n\ndtype: category\n\n\nWay better! Now let’s grab two more categorical variables and do similar things:\n\nsex (Male or Female)\n\nsurvived (survived or died)\n\n\n#convert sec variable\nsub_titanic_data[\"sexC\"] = sub_titanic_data.sex.astype(\"category\")\nsub_titanic_data.sexC = sub_titanic_data.sexC.cat.rename_categories([\"Female\", \"Male\"])\n#convert survived variable\nsub_titanic_data[\"survivedC\"] = sub_titanic_data.survived.astype(\"category\")\nsub_titanic_data.survivedC = sub_titanic_data.survivedC.cat.rename_categories([\"Died\", \"Survived\"])\n\n\n\n\nContingency tables\n\nTables of counts are the main numerical summary for categorical data\nCreate one-way contingency tables (.value_counts() method) (one-way because we are looking at one variable at a time)\n\n\nsub_titanic_data.embarkedC.value_counts(dropna = False)\n\n\n\n\n\n\n\n\ncount\n\n\nembarkedC\n\n\n\n\n\nSouthampton\n914\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\nNaN\n3\n\n\n\n\ndtype: int64\n\n\n\nsub_titanic_data.survivedC.value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nsurvivedC\n\n\n\n\n\nDied\n809\n\n\nSurvived\n500\n\n\n\n\ndtype: int64\n\n\n\nsub_titanic_data.sexC.value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nsexC\n\n\n\n\n\nMale\n843\n\n\nFemale\n466\n\n\n\n\ndtype: int64\n\n\n\nAlternatively, we can find a one-way table using the pd.cross_tab() function\n\nThis function is meant to take two columns (or more) and return tabulations between those two variables\nWe can define a dummy variable to cross with\nindex argument is the row variable and columns argument is the column variable\n\n\n\nsub_titanic_data[\"dummy\"] = 0\npd.crosstab(index = sub_titanic_data.embarkedC, columns = sub_titanic_data.dummy)\n\n\n  \n    \n\n\n\n\n\ndummy\n0\n\n\nembarkedC\n\n\n\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\nSouthampton\n914\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\npd.crosstab(index = sub_titanic_data.sexC, columns = sub_titanic_data.dummy)\n\n\n  \n    \n\n\n\n\n\ndummy\n0\n\n\nsexC\n\n\n\n\n\nFemale\n466\n\n\nMale\n843\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nTo summarize two categorical variables together, we use a two-way contingency table\nNow the cross_tab() function can be used more naturally\n\n\npd.crosstab(\n  sub_titanic_data.embarkedC, #index variable\n  sub_titanic_data.survivedC) #column variable\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\n\n\nembarkedC\n\n\n\n\n\n\nCherbourg\n120\n150\n\n\nQueenstown\n79\n44\n\n\nSouthampton\n610\n304\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\npd.crosstab(\n  sub_titanic_data.sexC,\n  sub_titanic_data.survivedC)\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\n\n\nsexC\n\n\n\n\n\n\nFemale\n127\n339\n\n\nMale\n682\n161\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAdd marginal totals with margins = True argument\n\n\npd.crosstab(\n  sub_titanic_data.embarkedC,\n  sub_titanic_data.survivedC,\n  margins = True)\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\nAll\n\n\nembarkedC\n\n\n\n\n\n\n\nCherbourg\n120\n150\n270\n\n\nQueenstown\n79\n44\n123\n\n\nSouthampton\n610\n304\n914\n\n\nAll\n809\n498\n1307\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAdd row and columns names for clarity\n\nUse rownames and colnames arguments\n\n\n\npd.crosstab(\n  sub_titanic_data.embarkedC,\n  sub_titanic_data.survivedC,\n  margins = True,\n  rownames = [\"Embarked Port\"],\n  colnames = [\"Survival Status\"]\n  )\n\n\n  \n    \n\n\n\n\n\nSurvival Status\nDied\nSurvived\nAll\n\n\nEmbarked Port\n\n\n\n\n\n\n\nCherbourg\n120\n150\n270\n\n\nQueenstown\n79\n44\n123\n\n\nSouthampton\n610\n304\n914\n\n\nAll\n809\n498\n1307\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThat looks great!\nFor more than two variables we can create tables but they get harder to read. For instance, we can look at a three-way contingency table:\n\npd.crosstab(\n  [sub_titanic_data.embarkedC, sub_titanic_data.survivedC], #pass a list of columns for the rows\n  sub_titanic_data.sexC,\n  margins = True)\n\n\n  \n    \n\n\n\n\n\n\nsexC\nFemale\nMale\nAll\n\n\nembarkedC\nsurvivedC\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nSurvived\n102\n48\n150\n\n\nQueenstown\nDied\n23\n56\n79\n\n\nSurvived\n37\n7\n44\n\n\nSouthampton\nDied\n93\n517\n610\n\n\nSurvived\n198\n106\n304\n\n\nAll\n\n464\n843\n1307\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nWe can add in names for more clarity\n\n\nmy_tab = pd.crosstab(\n  [sub_titanic_data.embarkedC, sub_titanic_data.survivedC],\n  sub_titanic_data.sexC,\n  margins = True,\n  rownames = ['Embarked Port', 'Survival Status'], #a list similar to how the rows were passed\n  colnames = ['Sex'])\nmy_tab\n\n\n  \n    \n\n\n\n\n\n\nSex\nFemale\nMale\nAll\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nSurvived\n102\n48\n150\n\n\nQueenstown\nDied\n23\n56\n79\n\n\nSurvived\n37\n7\n44\n\n\nSouthampton\nDied\n93\n517\n610\n\n\nSurvived\n198\n106\n304\n\n\nAll\n\n464\n843\n1307\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nWe might want to subset the returned table to get certain values…\n\nNote that the crosstab() function returns a data frame!\n\n\ntype(my_tab)\n\n\n\n    pandas.core.frame.DataFramedef __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/frame.pyTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n&gt;&gt;&gt; d = {'col1': [1, 2], 'col2': [3, 4]}\n&gt;&gt;&gt; df = pd.DataFrame(data=d)\n&gt;&gt;&gt; df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n&gt;&gt;&gt; df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n&gt;&gt;&gt; df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n&gt;&gt;&gt; d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n&gt;&gt;&gt; df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n&gt;&gt;&gt; df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n&gt;&gt;&gt; from dataclasses import make_dataclass\n&gt;&gt;&gt; Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df\n   0\na  1\nc  3\n\n&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df2\n   x\na  1\nc  3\n      \n      \n\n\n\nmy_tab.columns # columns of the data frame\n\nIndex(['Female', 'Male', 'All'], dtype='object', name='Sex')\n\n\n\nmy_tab.index #rows of the data frame, these are tuples!\n\nMultiIndex([(  'Cherbourg',     'Died'),\n            (  'Cherbourg', 'Survived'),\n            ( 'Queenstown',     'Died'),\n            ( 'Queenstown', 'Survived'),\n            ('Southampton',     'Died'),\n            ('Southampton', 'Survived'),\n            (        'All',         '')],\n           names=['Embarked Port', 'Survival Status'])\n\n\n\nCan obtain conditional bivariate info via subsetting!\nThe MultiIndex can be tough but let’s look at some examples\nBelow returns the embarked vs survived table for females\n\n\nmy_tab[\"Female\"]\n\n\n\n\n\n\n\n\n\nFemale\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\nCherbourg\nDied\n11\n\n\nSurvived\n102\n\n\nQueenstown\nDied\n23\n\n\nSurvived\n37\n\n\nSouthampton\nDied\n93\n\n\nSurvived\n198\n\n\nAll\n\n464\n\n\n\n\ndtype: int64\n\n\n\nmy_tab.loc[:, \"Female\"] #.loc way of doing this, : gives all of that index\n\n\n\n\n\n\n\n\n\nFemale\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\nCherbourg\nDied\n11\n\n\nSurvived\n102\n\n\nQueenstown\nDied\n23\n\n\nSurvived\n37\n\n\nSouthampton\nDied\n93\n\n\nSurvived\n198\n\n\nAll\n\n464\n\n\n\n\ndtype: int64\n\n\n\nBelow returns the sex vs embarked table for those that died\n\n\nmy_tab.iloc[0:5:2, :] #0:5:2 gives a shorthand for a sequence with steps of 2s\n\n\n  \n    \n\n\n\n\n\n\nSex\nFemale\nMale\nAll\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nQueenstown\nDied\n23\n56\n79\n\n\nSouthampton\nDied\n93\n517\n610\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nUsing .loc[] is better\nMust understand our MultiIndex\n\n\nmy_tab.index\n\nMultiIndex([(  'Cherbourg',     'Died'),\n            (  'Cherbourg', 'Survived'),\n            ( 'Queenstown',     'Died'),\n            ( 'Queenstown', 'Survived'),\n            ('Southampton',     'Died'),\n            ('Southampton', 'Survived'),\n            (        'All',         '')],\n           names=['Embarked Port', 'Survival Status'])\n\n\n\nBelow uses this index to return the sex vs embarked table for those that died\n\n\nmy_tab.loc[((\"Cherbourg\", \"Queenstown\", \"Southampton\"), \"Died\"), :]\n\n\n  \n    \n\n\n\n\n\n\nSex\nFemale\nMale\nAll\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nQueenstown\nDied\n23\n56\n79\n\n\nSouthampton\nDied\n93\n517\n610\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nBelow returns the sex vs survived table for embarked of Cherbourg\n\n\nmy_tab.loc[('Cherbourg', (\"Died\", \"Survived\")), :]\n\n\n  \n    \n\n\n\n\n\n\nSex\nFemale\nMale\nAll\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nSurvived\n102\n48\n150\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nReturn the sex table for those that died and embarked at Cherbourg\n\nFirst with .iloc[] then with .loc[]\n\n\n\nmy_tab.iloc[0, :]\n\n\n\n\n\n\n\n\nCherbourg\n\n\n\nDied\n\n\nSex\n\n\n\n\n\nFemale\n11\n\n\nMale\n109\n\n\nAll\n120\n\n\n\n\ndtype: int64\n\n\n\nmy_tab.loc[('Cherbourg', 'Died')]\n\n\n\n\n\n\n\n\nCherbourg\n\n\n\nDied\n\n\nSex\n\n\n\n\n\nFemale\n11\n\n\nMale\n109\n\n\nAll\n120\n\n\n\n\ndtype: int64\n\n\n\n\n\n\nNumeric Data\nGoal: Describe the distribution of the variable\n\nDistribution = pattern and frequency with which you observe a variable\n\nNumeric variable - entries are a numerical value where math can be performed\n\nFor a single numeric variable, describe the distribution via\n\nShape: Histogram, Density plot, … (covered later)\nMeasures of center: Mean, Median, …\nMeasures of spread: Variance, Standard Deviation, Quartiles, IQR, …\n\nFor two numeric variables, describe the distribution via\n\nShape: Scatter plot, …\nMeasures of linear relationship: Covariance, Correlation, …\n\n\n\nMeasures of Center\n\nFind mean and median with methods on a Series\n\n\ntype(sub_titanic_data['fare'])\n\n\n    pandas.core.series.Seriesdef __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/series.pyOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n&gt;&gt;&gt; ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n&gt;&gt;&gt; ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n&gt;&gt;&gt; r = [1, 2]\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\n[1, 2]\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n&gt;&gt;&gt; r = np.array([1, 2])\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\narray([999,   2])\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.\n      \n      \n\n\n\nCorresponding methods exist for the common numerical summaries\n\n\nsub_titanic_data['fare'].mean()\n\n33.29547928134557\n\n\n\nsub_titanic_data['fare'].median()\n\n14.4542\n\n\n\nsub_titanic_data.age.mean() #same thing with a different way to get a column\n\n29.8811345124283\n\n\n\nsub_titanic_data.age.median()\n\n28.0\n\n\n\n\n\nMeasures of Spread\n\nStandard Deviation, Quartiles, & IQR found with Series methods as well\n\n\nsub_titanic_data.age.std()\n\n14.413499699923594\n\n\n\nsub_titanic_data.age.quantile(q = [0.2, 0.25, 0.5, 0.95])\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0.20\n19.0\n\n\n0.25\n21.0\n\n\n0.50\n28.0\n\n\n0.95\n57.0\n\n\n\n\ndtype: float64\n\n\n\nq1 = sub_titanic_data.age.quantile(q = [0.25])\nq1\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0.25\n21.0\n\n\n\n\ndtype: float64\n\n\n\nq3 = sub_titanic_data.age.quantile(q = [0.75])\nq3\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0.75\n39.0\n\n\n\n\ndtype: float64\n\n\n\ntype(q1)\n\n\n    pandas.core.series.Seriesdef __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/series.pyOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n&gt;&gt;&gt; ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n&gt;&gt;&gt; ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n&gt;&gt;&gt; r = [1, 2]\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\n[1, 2]\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n&gt;&gt;&gt; r = np.array([1, 2])\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\narray([999,   2])\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.\n      \n      \n\n\n\nAs both q1 and q3 are Series, they have indices\nThis makes them a little more difficult than you might like to subtract (to find the IRQ)\n\n\nq3-q1 #doesn't work due to the differing index names\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0.25\nNaN\n\n\n0.75\nNaN\n\n\n\n\ndtype: float64\n\n\n\nq3[0.75] - q1[0.25] #grab the values by index names and subtract those\n\n18.0\n\n\n\nAlternatively, remember that returning the .values attribute returns a numpy array. We can subtract these.\n\n\nq3.values - q1.values\n\narray([18.])\n\n\n\n\n\nMeasures of Linear Relationship\n\nCorrelation via the .corr() method on a data frame\nThis gives the correlation with any numerically (stored) variables that are passed\n\nJust because it is stored numerically doesn’t mean we should treat it numerically!\n\n\n\nsub_titanic_data[[\"age\", \"fare\", \"sibsp\", \"parch\"]].corr()\n\n\n  \n    \n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\n\n\nage\n1.000000\n0.178739\n-0.243699\n-0.150917\n\n\nfare\n0.178739\n1.000000\n0.160238\n0.221539\n\n\nsibsp\n-0.243699\n0.160238\n1.000000\n0.373587\n\n\nparch\n-0.150917\n0.221539\n0.373587\n1.000000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n\n\nSummaries Across Groups\nUsually want summaries for different subgroups of data\nTwo approaches we’ll cover: - Use .groupby() method and then use a summarization method - Use pd.crosstab() function with aggfunc argument\n\n.groupby() Examples\nExample: Get similar fare summaries for each survival status\n\nsub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean()\n\n\n\n  \n    \n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\n\n\n\n\n\n\n\n\nDied\n30.545369\n23.353831\n0.521632\n0.328801\n\n\nSurvived\n28.918228\n49.361184\n0.462000\n0.476000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nsub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].std()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].std()\n\n\n\n  \n    \n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\n\n\n\n\n\n\n\n\nDied\n13.922539\n34.145096\n1.210449\n0.912332\n\n\nSurvived\n15.061481\n68.648795\n0.685197\n0.776292\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n.unstack() method on the result can sometimes make the output clearer\n\n\nsub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean().unstack()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean().unstack()\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nsurvivedC\n\n\n\n\n\nage\nDied\n30.545369\n\n\nSurvived\n28.918228\n\n\nfare\nDied\n23.353831\n\n\nSurvived\n49.361184\n\n\nsibsp\nDied\n0.521632\n\n\nSurvived\n0.462000\n\n\nparch\nDied\n0.328801\n\n\nSurvived\n0.476000\n\n\n\n\ndtype: float64\n\n\n\nMultiple grouping variables can be given as a list\n\nExample: Get summary for numeric type variables for each survival status and embarked port\n\n\n\nsub_titanic_data.groupby([\"survivedC\", \"embarkedC\"])[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby([\"survivedC\", \"embarkedC\"])[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean()\n\n\n\n  \n    \n\n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\nembarkedC\n\n\n\n\n\n\n\n\nDied\nCherbourg\n34.468750\n40.255592\n0.316667\n0.225000\n\n\nQueenstown\n30.202703\n11.615349\n0.379747\n0.177215\n\n\nSouthampton\n29.945385\n21.546160\n0.580328\n0.368852\n\n\nSurvived\nCherbourg\n31.037248\n80.000807\n0.466667\n0.486667\n\n\nQueenstown\n24.153846\n13.833998\n0.272727\n0.000000\n\n\nSouthampton\n27.989881\n39.183470\n0.490132\n0.542763\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nsub_titanic_data.groupby([\"survivedC\", \"embarkedC\"])[[\"age\", \"fare\", \"sibsp\", \"parch\"]].std()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby([\"survivedC\", \"embarkedC\"])[[\"age\", \"fare\", \"sibsp\", \"parch\"]].std()\n\n\n\n  \n    \n\n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\nembarkedC\n\n\n\n\n\n\n\n\nDied\nCherbourg\n14.655181\n56.553704\n0.518293\n0.557040\n\n\nQueenstown\n16.785187\n10.922240\n1.016578\n0.655538\n\n\nSouthampton\n13.496871\n28.786020\n1.320897\n0.990934\n\n\nSurvived\nCherbourg\n15.523752\n97.642219\n0.575410\n0.730327\n\n\nQueenstown\n7.057457\n17.503850\n0.585230\n0.000000\n\n\nSouthampton\n14.926867\n47.656409\n0.744552\n0.831405\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAs our code gets longer, this is a good place to use \\ to extend our code down a line\n\n\nsub_titanic_data \\\n  .groupby([\"survivedC\", \"embarkedC\"]) \\\n   [[\"age\", \"fare\", \"sibsp\", \"parch\"]] \\\n   .mean()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  .groupby([\"survivedC\", \"embarkedC\"]) \\\n\n\n\n  \n    \n\n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\nembarkedC\n\n\n\n\n\n\n\n\nDied\nCherbourg\n34.468750\n40.255592\n0.316667\n0.225000\n\n\nQueenstown\n30.202703\n11.615349\n0.379747\n0.177215\n\n\nSouthampton\n29.945385\n21.546160\n0.580328\n0.368852\n\n\nSurvived\nCherbourg\n31.037248\n80.000807\n0.466667\n0.486667\n\n\nQueenstown\n24.153846\n13.833998\n0.272727\n0.000000\n\n\nSouthampton\n27.989881\n39.183470\n0.490132\n0.542763\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\npd.crosstab() Examples\n\nAlternatively we can use the pd.crosstab() function with an aggfunc to define our summarization to produce\n\nExample: Get summary for numeric type variables for each survival status\n\nA bit awkward in this case as we don’t really have a ‘column’ variable\nMake a dummy variable for that\n\n\npd.crosstab(\n  sub_titanic_data.survivedC,\n  columns = [\"mean\" for _ in range(sub_titanic_data.shape[0])], #create variable with only the value 'mean'\n  values = sub_titanic_data.fare,\n  aggfunc = 'mean')\n\n\n  \n    \n\n\n\n\n\ncol_0\nmean\n\n\nsurvivedC\n\n\n\n\n\nDied\n23.353831\n\n\nSurvived\n49.361184\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nCan return multiple summaries at once by passing them as a list\n\n\npd.crosstab(\n  sub_titanic_data.survivedC,\n  columns = [\"stat\" for _ in range(sub_titanic_data.shape[0])],\n  values = sub_titanic_data.fare,\n  aggfunc = ['mean', 'median', 'std', 'count'])\n\n\n  \n    \n\n\n\n\n\n\nmean\nmedian\nstd\ncount\n\n\ncol_0\nstat\nstat\nstat\nstat\n\n\nsurvivedC\n\n\n\n\n\n\n\n\nDied\n23.353831\n10.5\n34.145096\n808\n\n\nSurvived\n49.361184\n26.0\n68.648795\n500\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nMore natural with two grouping variables\n\nExample: Get summary for numeric type variables for each survival status and embarked port\n\n\n\npd.crosstab(\n  sub_titanic_data.embarkedC,\n  sub_titanic_data.survivedC,\n  values = sub_titanic_data.fare,\n  aggfunc = ['mean', 'count'])\n\n\n  \n    \n\n\n\n\n\n\nmean\ncount\n\n\nsurvivedC\nDied\nSurvived\nDied\nSurvived\n\n\nembarkedC\n\n\n\n\n\n\n\n\nCherbourg\n40.255592\n80.000807\n120\n150\n\n\nQueenstown\n11.615349\n13.833998\n79\n44\n\n\nSouthampton\n21.546160\n39.183470\n609\n304"
  },
  {
    "objectID": "01_Programming_in_python/15-Pandas_Data_Frames.html",
    "href": "01_Programming_in_python/15-Pandas_Data_Frames.html",
    "title": "Pandas Data Frames",
    "section": "",
    "text": "Note: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/15-Pandas_Data_Frames.html#creating-a-dataframe",
    "href": "01_Programming_in_python/15-Pandas_Data_Frames.html#creating-a-dataframe",
    "title": "Pandas Data Frames",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\nMost of the time we’ll read data from a raw file directly into a DataFrame\nHowever, you can create one with the pd.DataFrame() function\n\n\nimport pandas as pd\nimport numpy as np\n\n\nCreating a Data Frame from Lists\n\nzip() lists of the same length together\nspecify columns via columns = list of appropriate length\nspecify row names via index = list of appropriate length (if you want!)\n\n\n#populate some lists, each of equal length\nname = ['Alice', 'Bob','Charlie','Dave','Eve','Francesca','Greg']\nage = [20, 21, 22, 23, 22, 21, 22]\nmajor = ['Statistics', 'History', 'Chemistry', 'English', 'Math', 'Civil Engineering','Statistics']\n\n#create the data frame using zip()\nmy_df = pd.DataFrame(zip(name, age, major), columns = [\"name\", \"age\", \"major\"])\nmy_df\n\n\n  \n    \n\n\n\n\n\n\nname\nage\nmajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n1\nBob\n21\nHistory\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n5\nFrancesca\n21\nCivil Engineering\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\nCreating a Data Frame from a Dictionary\n\nThe pd.DataFrame() function can create DataFrames from many objects\nFor a dictionary (dict object), the keys become the column names (values must be of the same length)\n\n\npeople = {'Name': ['Alice', 'Bob','Charlie','Dave','Eve','Francesca','Greg'],\n          'Age': [20, 21, 22, 23, 22, 21, 22],\n          'Major': ['Statistics', 'History', 'Chemistry', 'English', 'Math', 'Civil Engineering','Statistics'],\n         }\npeople\n\n{'Name': ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve', 'Francesca', 'Greg'],\n 'Age': [20, 21, 22, 23, 22, 21, 22],\n 'Major': ['Statistics',\n  'History',\n  'Chemistry',\n  'English',\n  'Math',\n  'Civil Engineering',\n  'Statistics']}\n\n\n\nmy_df = pd.DataFrame(people)\nmy_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n1\nBob\n21\nHistory\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n5\nFrancesca\n21\nCivil Engineering\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\nCreating a Data Frame from a NumPy Array\n\nIf you have a 2D numpy array, the conversion to a DataFrame object is natural\nYou can specify the column names with columns = and the indices with index =\n\n\nmy_array = np.random.random((5,3))\nprint(my_array.shape)\nmy_array\n\n(5, 3)\n\n\narray([[0.44601505, 0.15726038, 0.63256689],\n       [0.74871631, 0.35006141, 0.58570382],\n       [0.14346733, 0.22706604, 0.02253265],\n       [0.57983249, 0.60743321, 0.88242121],\n       [0.20877059, 0.75132726, 0.04447515]])\n\n\n\nmy_df2 = pd.DataFrame(my_array, columns=[\"1st\", \"2nd\", \"3rd\"], index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nmy_df2\n\n\n  \n    \n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n\n\na\n0.446015\n0.157260\n0.632567\n\n\nb\n0.748716\n0.350061\n0.585704\n\n\nc\n0.143467\n0.227066\n0.022533\n\n\nd\n0.579832\n0.607433\n0.882421\n\n\ne\n0.208771\n0.751327\n0.044475"
  },
  {
    "objectID": "01_Programming_in_python/15-Pandas_Data_Frames.html#indexing-a-data-frame",
    "href": "01_Programming_in_python/15-Pandas_Data_Frames.html#indexing-a-data-frame",
    "title": "Pandas Data Frames",
    "section": "Indexing a Data Frame",
    "text": "Indexing a Data Frame\n\nIndexing Columns with []\n\nDataFrames have a .columns attribute\n\n\nmy_df2.columns\n\nIndex(['1st', '2nd', '3rd'], dtype='object')\n\n\n\nWe can access the columns using a string of the column names and ‘selection brackets’\n\n\nmy_df2[\"1st\"]\n\n\n\n\n\n\n\n\n1st\n\n\n\n\na\n0.446015\n\n\nb\n0.748716\n\n\nc\n0.143467\n\n\nd\n0.579832\n\n\ne\n0.208771\n\n\n\n\ndtype: float64\n\n\n\nNote that what gets returned is just a Series!\n\n\ntype(my_df2[\"1st\"])\n\n\n    pandas.core.series.Seriesdef __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/series.pyOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n&gt;&gt;&gt; ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n&gt;&gt;&gt; ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n&gt;&gt;&gt; r = [1, 2]\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\n[1, 2]\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n&gt;&gt;&gt; r = np.array([1, 2])\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\narray([999,   2])\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.\n      \n      \n\n\n\nWe can also return a column using the attribute syntax with the column name (a period at the end of the object followed by the column name)\n\n\nmy_df.Major\n\n\n\n\n\n\n\n\nMajor\n\n\n\n\n0\nStatistics\n\n\n1\nHistory\n\n\n2\nChemistry\n\n\n3\nEnglish\n\n\n4\nMath\n\n\n5\nCivil Engineering\n\n\n6\nStatistics\n\n\n\n\ndtype: object\n\n\n\ntype(my_df.Major) #again a Series!\n\n\n    pandas.core.series.Seriesdef __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/series.pyOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n&gt;&gt;&gt; ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n&gt;&gt;&gt; ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n&gt;&gt;&gt; r = [1, 2]\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\n[1, 2]\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n&gt;&gt;&gt; r = np.array([1, 2])\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\narray([999,   2])\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.\n      \n      \n\n\n\nReturning more than one column is easy\nYou can give a list of the column names you want to the selection brackets\n\n\nmy_df[['Name', 'Age']]\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\n\n\n\n\n0\nAlice\n20\n\n\n1\nBob\n21\n\n\n2\nCharlie\n22\n\n\n3\nDave\n23\n\n\n4\nEve\n22\n\n\n5\nFrancesca\n21\n\n\n6\nGreg\n22\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nNote you can’t use slicing for columns using just [] (we’ll need to us .iloc[] or .loc[], which we cover in a moment)\nIf you try to index with slicing you get back appropriate rows (see below)\n\n\n\nIndexing Rows by Slicing with []\n\nSimilarly, you can index the rows using [] if you use a slice or a boolean array of appropriate length\n\n\nmy_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n1\nBob\n21\nHistory\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n5\nFrancesca\n21\nCivil Engineering\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmy_df[3:5] #get the 3rd and 4th rows\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nmy_df2\n\n\n  \n    \n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n\n\na\n0.446015\n0.157260\n0.632567\n\n\nb\n0.748716\n0.350061\n0.585704\n\n\nc\n0.143467\n0.227066\n0.022533\n\n\nd\n0.579832\n0.607433\n0.882421\n\n\ne\n0.208771\n0.751327\n0.044475\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmy_df2[1:5] #get the 2nd through 5th rows (counting starts at 0!)\n\n\n  \n    \n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n\n\nb\n0.748716\n0.350061\n0.585704\n\n\nc\n0.143467\n0.227066\n0.022533\n\n\nd\n0.579832\n0.607433\n0.882421\n\n\ne\n0.208771\n0.751327\n0.044475\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nOddly, you can’t return a single row with just a number\nYou can return it using slicing (recall : usually doesn’t return the last value)\n\n\nmy_df2[1] #throws an error\n\nKeyError: 1\n\n\n\nmy_df2[1:2] #return just one row\n\n\n  \n    \n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n\n\nb\n0.748716\n0.350061\n0.585704\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\nIndexing Rows Using a Boolean Array with []\n\nOften we use a Boolean object to subset the rows (rows with a True get returned, False do not)\nThis comes up when we use a condition found using a variable from our data frame to do the subsetting\n\n\nmy_df['Name'] == 'Alice' #create a boolean array\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nTrue\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n5\nFalse\n\n\n6\nFalse\n\n\n\n\ndtype: bool\n\n\n\nmy_df[my_df['Name'] == 'Alice'] #return just the True rows\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\nmy_df[my_df['Age'] &gt; 21] #return only rows that match\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nCompound Logicals\n\nAll the standard compound logical operators exist\n& (and), | (or), ~ (not), ^ (xor - exclusive or)\n\nFirst, two boolean vectors:\n\n(my_df['Name'] == 'Alice')\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nTrue\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n5\nFalse\n\n\n6\nFalse\n\n\n\n\ndtype: bool\n\n\n\n(my_df['Name'] == 'Greg')\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nFalse\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n5\nFalse\n\n\n6\nTrue\n\n\n\n\ndtype: bool\n\n\n\nGet either/or for these two booleans\n\n\n(my_df['Name'] == 'Alice') | (my_df['Name'] == 'Greg')\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nTrue\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n5\nFalse\n\n\n6\nTrue\n\n\n\n\ndtype: bool\n\n\n\nNow we can subset the data based on this compound condition!\n\n\nmy_df[(my_df['Name'] == 'Alice') | (my_df['Name'] == 'Greg')]\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nWhen doing lots of logicals, you want to be careful and use () to keep things straight!\n\n\nmy_df[((my_df['Name'] == 'Alice') | (my_df['Name'] == 'Greg')) & (my_df['Age'] &gt; 21)]\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n6\nGreg\n22\nStatistics"
  },
  {
    "objectID": "01_Programming_in_python/15-Pandas_Data_Frames.html#operations-on-data-frames",
    "href": "01_Programming_in_python/15-Pandas_Data_Frames.html#operations-on-data-frames",
    "title": "Pandas Data Frames",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\n\n.head() and .tail() methods give the first few and last rows, respectively\n\n\nmy_df.head()\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n1\nBob\n21\nHistory\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nmy_df.tail()\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n5\nFrancesca\n21\nCivil Engineering\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n.shape attribute contains the dimensions of the data frame\n\n\nmy_df.shape\n\n(7, 3)\n\n\n\n.info() method gives information about the data frame\n\n\nmy_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7 entries, 0 to 6\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    7 non-null      object\n 1   Age     7 non-null      int64 \n 2   Major   7 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 296.0+ bytes\n\n\n\nmy_df2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 5 entries, a to e\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   1st     5 non-null      float64\n 1   2nd     5 non-null      float64\n 2   3rd     5 non-null      float64\ndtypes: float64(3)\nmemory usage: 332.0+ bytes\n\n\n\nObtain a quick contingency table with the .value_counts() method on a column (so a .Series method really)\n\n\nmy_df[\"Major\"].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nMajor\n\n\n\n\n\nStatistics\n2\n\n\nHistory\n1\n\n\nChemistry\n1\n\n\nEnglish\n1\n\n\nMath\n1\n\n\nCivil Engineering\n1\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "01_Programming_in_python/13-EDA_Landing.html",
    "href": "01_Programming_in_python/13-EDA_Landing.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The video below discusses the common steps when investigating data. This is usually referred to as an exploratory data analysis (EDA).\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/13-EDA_Landing.html#notes",
    "href": "01_Programming_in_python/13-EDA_Landing.html#notes",
    "title": "Exploratory Data Analysis",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version"
  },
  {
    "objectID": "01_Programming_in_python/13-EDA_Landing.html#additional-readings-for-week-3",
    "href": "01_Programming_in_python/13-EDA_Landing.html#additional-readings-for-week-3",
    "title": "Exploratory Data Analysis",
    "section": "Additional Readings for Week 3",
    "text": "Additional Readings for Week 3\nExploratory Data Analysis (EDA) is often the first step of dealing with data. However, EDA is somewhat of an art and something that you get better at with experience. Often people new to EDA don’t know what they should be looking for! Here are some articles that discuss different strategies for EDA.\n\nShopify page\nIBM page\nWikipedia page\nNIST page\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/11-Dictionaries.html",
    "href": "01_Programming_in_python/11-Dictionaries.html",
    "title": "Dictionaries",
    "section": "",
    "text": "Recall our plan: Go through common data types and\nThis topic, compound objects: - Dictionaries\nDictionaries are data objects that have key-value pairs associated with it\nDictionaries are a flexible data type that is\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/11-Dictionaries.html#creating-a-dictionary",
    "href": "01_Programming_in_python/11-Dictionaries.html#creating-a-dictionary",
    "title": "Dictionaries",
    "section": "Creating a Dictionary",
    "text": "Creating a Dictionary\n\nCreating a dictionary using dict() or {}\n\nUse dict(supply_key_value_pairs) or {supply_key_value_pairs}\n\n\n{} #empty dictionary\nmydict = {\n  \"key1\": [12, -10, \"value1\"],  #key is specified first (must be immutable)\n  \"key2\": [11, \"value2\"],       #value is then given after a : (can be anything, doesn't need to match across keys)\n  \"key3\": \"value3\"\n  }\nmydict\n\n{'key1': [12, -10, 'value1'], 'key2': [11, 'value2'], 'key3': 'value3'}\n\n\n\nmydict2 = dict([\n  (1, ['hee', 'haw']),        #passing key value pairs as a tuple\n  (2, 'fa')\n  ])\nmydict2\n\n{1: ['hee', 'haw'], 2: 'fa'}\n\n\n\n\nCreating a dictionary using lists\n\nYou can create a dictionary using two lists and the zip() function\n\n\nkeys = [x for x in \"abcdefgh\"]\nvalues = [y for y in range(0,8)]\nprint(keys)\nprint(values)\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n[0, 1, 2, 3, 4, 5, 6, 7]\n\n\n\ndict(zip(keys, values))\n\n{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}\n\n\n\n\nCreating a dictionary with dictionary comprehensions\n\nCan create a dictionary using dictionary comprehensions!\nSmilar to list comprehensions but we use { instead of [\n\n\nmydict = {i: i for i in range(0, 6)}\nmydict\n\n{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n\n\n\nmydict = {\"abcdef\"[i]: i**2 for i in range(0, 6)}\nmydict\n\n{'a': 0, 'b': 1, 'c': 4, 'd': 9, 'e': 16, 'f': 25}"
  },
  {
    "objectID": "01_Programming_in_python/11-Dictionaries.html#dictionary-operations",
    "href": "01_Programming_in_python/11-Dictionaries.html#dictionary-operations",
    "title": "Dictionaries",
    "section": "Dictionary Operations",
    "text": "Dictionary Operations\n\nIndexing a Dictionary\n\nIndex with a [key] (remember unordered!)\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"]\n  }\nAFCDivisions[\"North\"]\n\n['Steelers', 'Browns', 'Ravens', 'Bengals']\n\n\n\nYou can access the returned object in the same line of code\n\n\nAFCDivisions[\"North\"][0]\n\n'Steelers'\n\n\n\nWe can add new key/value pairs by simply referencing a key that doesn’t exist\nHere we add the “South” key with a silly value\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"]\n  }\nAFCDivisions[\"South\"] = [1, 2]\nAFCDivisions\n\n{'North': ['Steelers', 'Browns', 'Ravens', 'Bengals'],\n 'East': ['Patriots', 'Jets', 'Dolphins', 'Bills'],\n 'West': ['Raiders', 'Chiefs', 'Chargers', 'Broncos'],\n 'South': [1, 2]}\n\n\n\nYou can iterate over the keys in a dictionary\n\n\nfor key in AFCDivisions:              #keys are what are iterated over\n    print(key, ' : ', AFCDivisions[key]) #we access the value associated with the key\n\nNorth  :  ['Steelers', 'Browns', 'Ravens', 'Bengals']\nEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nSouth  :  [1, 2]\n\n\n\nYou can overwrite the values similar to how you can add a key/value pair after the fact. Here we overwrite the “South” value.\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"]\n  }\nAFCDivisions[\"South\"] = [1, 2]\nAFCDivisions[\"South\"] = [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\nfor key in AFCDivisions:\n    print(key, ' : ', AFCDivisions[key])\n\nNorth  :  ['Steelers', 'Browns', 'Ravens', 'Bengals']\nEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nSouth  :  ['Texans', 'Colts', 'Jaguars', 'Titans']\n\n\n\n\n\nDictionary Packing & Unpacking\n\nWe can pack dictionaries using ** similar to how we packed a list!\nHere we create a dictionary called Divisions where we pack two dictionaries inside it\n\n\nAFCDivisions = {\n  \"AFCNorth\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"AFCEast\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"AFCWest\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"AFCSouth\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\n\nNFCDivisions = {\n  \"NFCNorth\" : [\"Lions\", \"Bears\", \"Packers\", \"Vikings\"],\n  \"NFCEast\"  : [\"Giants\", \"Cowboys\", \"Eagles\", \"Admirals\"]\n}\n\nDivisions = {**AFCDivisions, **NFCDivisions}\nfor key in Divisions:\n    print(key, ' : ', Divisions[key])\n\nAFCNorth  :  ['Steelers', 'Browns', 'Ravens', 'Bengals']\nAFCEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nAFCWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nAFCSouth  :  ['Texans', 'Colts', 'Jaguars', 'Titans']\nNFCNorth  :  ['Lions', 'Bears', 'Packers', 'Vikings']\nNFCEast  :  ['Giants', 'Cowboys', 'Eagles', 'Admirals']"
  },
  {
    "objectID": "01_Programming_in_python/11-Dictionaries.html#dictionary-methods",
    "href": "01_Programming_in_python/11-Dictionaries.html#dictionary-methods",
    "title": "Dictionaries",
    "section": "Dictionary Methods",
    "text": "Dictionary Methods\nMany useful methods\n\nWe saw how to index with [] similar to lists/tuples\nWe can index with .get() instead\n\nAdvantage is that it doesn’t throw an error if the key doesn’t exist\n\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"South\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\nAFCDivisions.get(\"South\")\n\n['Texans', 'Colts', 'Jaguars', 'Titans']\n\n\n\nAFCDivisions.get(\"Northeast\") #doesn't throw an error\n\n\nAFCDivisions[\"Northeast\"] #throws an error\n\nKeyError: 'Northeast'\n\n\n\nReturn keys with .keys(); values with .values()\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"South\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\nAFCDivisions.keys()\n\ndict_keys(['North', 'East', 'West', 'South'])\n\n\n\nAFCDivisions.values()\n\ndict_values([['Steelers', 'Browns', 'Ravens', 'Bengals'], ['Patriots', 'Jets', 'Dolphins', 'Bills'], ['Raiders', 'Chiefs', 'Chargers', 'Broncos'], ['Texans', 'Colts', 'Jaguars', 'Titans']])\n\n\n\nReturn and remove a specified key with .pop() (similar to the list method)\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"South\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\nAFCDivisions.pop(\"North\") #modifies the dictionary\n\n['Steelers', 'Browns', 'Ravens', 'Bengals']\n\n\n\nfor key in AFCDivisions:\n    print(key, ' : ', AFCDivisions[key]) #North no longer exists!\n\nEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nSouth  :  ['Texans', 'Colts', 'Jaguars', 'Titans']\n\n\n\nMerge in another dictionary with .update()\nVery similar to dictionary packing done above but it modifies one of the dictionaries\n\n\nDivisions = {\n  \"AFCNorth\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"AFCEast\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"AFCWest\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"AFCSouth\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\nNFCNorth = {\n  \"NFCNorth\": [\"Lions\", \"Packers\", \"Bears\", \"Vikings\"]\n  }\nDivisions.update(NFCNorth) #combine the dictionaries\nfor key in Divisions.keys():\n    print(key, \" : \", Divisions[key])\n\nAFCNorth  :  ['Steelers', 'Browns', 'Ravens', 'Bengals']\nAFCEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nAFCWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nAFCSouth  :  ['Texans', 'Colts', 'Jaguars', 'Titans']\nNFCNorth  :  ['Lions', 'Packers', 'Bears', 'Vikings']"
  },
  {
    "objectID": "01_Programming_in_python/09-Control_Flow.html",
    "href": "01_Programming_in_python/09-Control_Flow.html",
    "title": "Control Flow",
    "section": "",
    "text": "Note: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/09-Control_Flow.html#conditional-statements",
    "href": "01_Programming_in_python/09-Control_Flow.html#conditional-statements",
    "title": "Control Flow",
    "section": "Conditional Statements",
    "text": "Conditional Statements\n\nChoose which portions of your code to execute by using conditional statements!\nAn if statement changes how a program behaves based on a condition\n\nCondition comes in the form of a boolean\n\nRecall: Booleans are True or False\n\nCan be treated as 1 and 0\nMany functions to create bools (.is_*() methods, bool() function)\n\n\n\nif Syntax\n\nExample of if with no further conditional logic:\n\nif boolean:\n    #If boolean is true, execute the chunk of code that is indented\n    #Four spaces is recommended but any indentation can technically be used\n    statement1\n    statement2\n    \n#code not indented would then execute as normal\n\nif Example\nPrinting different strings using if statements\n\ntemp = 30\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n\n30 degrees is cold.\n\n\n\ntemp = 100\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n\nWe can have multiple statements that are executed within a block if the condition is True.\n\ntemp = 30\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\n\n30 degrees is cold.\nWear a jacket outside!\n\n\n\ntemp = 100\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nif temp &gt;= 50:\n    print(temp, \"degrees is not cold.\")\n\n100 degrees is not cold.\n\n\n\n\n\nif with else Syntax\nWith the last example above, we see something we often want to do: - check a condition (temp &lt; 50), if True execute some code - if that same condition is False - or the opposite is True (temp &gt;= 50) - then execute something else\nThis can be taken care of with the else statement. The else statement immediately following an if block allows for execution of code only when the above condition(s) were (all) False\nif boolean:\n    execute this code\nelse:\n    execute this code\nthis is logically equivalent to\nif boolean:\n    execute this code\nif not boolean:\n    execute this code\n\ntemp = 100\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nelse:\n    print(temp, \"degrees is not cold.\")\n\n100 degrees is not cold.\n\n\n\n\nif, elif, and else\nYou can check additional conditions using elif which stands for ‘else if’.\nThis condition is only checked if all the above conditions were False.\nif boolean1:\n    #if boolean1 is True\n    execute this code  \nelif boolean2:\n    #if boolean1 is False, check if boolean2 is True\n    #if True\n    execute this code\nelif boolean3:\n    #if boolean1 and boolean2 are False, check if boolean3 is True\n    #if True\n    execute this code\nelse:\n    # if no conditions met\n    execute this code\n\ntemp = 40\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nelif temp &lt; 70:\n    print(temp, \"degrees is kind of cold...\")\n    print(\"You may want to bring an umbrella in case it rains!\")\nelse:\n    print(temp, \"degrees is not cold.\")\n\n40 degrees is cold.\nWear a jacket outside!\n\n\n\ntemp = 60\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nelif temp &lt; 70:\n    print(temp, \"degrees is kind of cold...\")\n    print(\"You may want to bring an umbrella in case it rains!\")\nelse:\n    print(temp, \"degrees is not cold.\")\n\n60 degrees is kind of cold...\nYou may want to bring an umbrella in case it rains!\n\n\n\ntemp = 100\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nelif temp &lt; 70:\n    print(temp, \"degrees is kind of cold...\")\n    print(\"You may want to bring an umbrella in case it rains!\")\nelse:\n    print(temp, \"degrees is not cold.\")\n\n100 degrees is not cold."
  },
  {
    "objectID": "01_Programming_in_python/09-Control_Flow.html#loops",
    "href": "01_Programming_in_python/09-Control_Flow.html#loops",
    "title": "Control Flow",
    "section": "Loops",
    "text": "Loops\nConditionally executing code is really useful! Another useful thing is to repeatedly execute code. Each execution you may want to change something in the computation.\n\nLoop Example\n\nSuppose we’ve observed the eye colors of 15 people\nEye color coded as either 1 (blue), 2 (brown), 3 (green), 4 (other)\nWant to create a new variable that has the descriptive values\n\n\n#data stored in a 'list'\neye_color = [3, 2, 2, 1, 2, 1, 2, 4, 3, 2, 2, 1, 2, 2]\n\nGoal: Create a new variable that has the descriptive values - Recall list elements are indexed using [index] (like other sequence type objects such as strings)\n\nprint(eye_color[0])\nprint(eye_color[1])\n\n3\n2\n\n\n\nWe could consider using conditional logic to print out the descriptive string\n\n\neye_color = [3, 2, 2, 1, 2, 1, 2, 4, 3, 2, 2, 1, 2, 2]\n\nif eye_color[0] == 1:\n    print(\"blue\")\nelif eye_color[0] == 2:\n    print(\"brown\")\nelif eye_color[0] == 3:\n    print(\"green\")\nelse:\n    print(\"other\")\n\ngreen\n\n\n\n\n\nLoop Syntax\n\nInstead of repeating and modifying code, use a loop!\n\nfor index in values:\n     code to be run\n\nindex argument defines a counter, or variable, that varies each time the code within the loop is executed\nvalues argument defines which values the index takes on in these iterations\n\nThese values do not have to be numeric!\n\n\n\nLoop Toy Examples\n\nRun the code below and feel free to modify things to see what happens!\n\n\nfor index in [\"cat\", \"hat\", \"worm\"]:\n    print(index)\n\ncat\nhat\nworm\n\n\n\nvalues = list(range(10)) #recall range is an iterator-type object, list gets the values out\nprint(values)\nfor i in values:\n    print(i)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n#we can get the indices of the eye_color object\nprint(list(range(len(eye_color))))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n\n\n\n#alternatively, we can just iterate over the range() object itself\nvalues = iter(range(len(eye_color)))\nprint(next(values))\nprint(next(values))\nprint(next(values))\n\n0\n1\n2\n\n\nBack to our example. We want to print out the more descriptive name depending on what the numeric value is.\nLet’s loop through all the eye_color values using a for loop. To do so, we’ll use the range() function with the len(eye_color) as its argument.\nRecall: range() is an iterator-type object. A for loop will automatically go over the values the range indicates. When we give range() just one argument it defaults to a range of 0 to that value (but that value isn’t included).\n\nfor i in range(len(eye_color)):\n    if eye_color[i] == 1:\n        print(\"blue\")\n    elif eye_color[i] == 2:\n        print(\"brown\")\n    elif eye_color[i] == 3:\n        print(\"green\")\n    else:\n        print(\"other\")\n\ngreen\nbrown\nbrown\nblue\nbrown\nblue\nbrown\nother\ngreen\nbrown\nbrown\nblue\nbrown\nbrown\n\n\nWe don’t really need to use a set of numeric values to iterate over (via range()). We can iterate over anything that is iterable. All sequence type objects are iterable (like lists and strings).\nHere we’ll iterate over the eye_color list itself.\n\nfor i in eye_color:\n    if i == 1:\n        print(\"blue\")\n    elif i == 2:\n        print(\"brown\")\n    elif i == 3:\n        print(\"green\")\n    else:\n        print(\"other\")\n\ngreen\nbrown\nbrown\nblue\nbrown\nblue\nbrown\nother\ngreen\nbrown\nbrown\nblue\nbrown\nbrown\n\n\n\n\n\n\nOther Looping Commands\n\nOccassionally we want to jump out of a for loop. This can be done with break\n\n\nfor i in range(5):\n    if i == 3:\n        break\n    print(i)\n\n0\n1\n2\n\n\n\nThe continue command jumps to the next iteration of the loop without finishing the current iteration\n\n\nfor i in range(5):\n    if i == 3:\n        continue\n    print(i)\n\n0\n1\n2\n4\n\n\n\n\n\nWhile Loops\n\nWhile loops are similar to for loops but they loop until a condition is reached\n\nUseful when we don’t know in advance how many loop iterations we should execute\n\nGeneral syntax of a while loop:\n\nwhile expression:\n    #code block to execute\n    block\n\nthe expression is sometimes called the loop condition\nAt each iteration of the loop, python evaluates the expression\n\nIf the expression evaluates to False, the loop exits\nIf the expression evaluates to True, the loop body is executed again\n\n\n\nWhile Loop Example\nWhen using while loops, we usually modify the condition within the body of the while loop (or use a break to jump out when needed).\n\nrabbits = 3\nwhile rabbits &gt; 0:\n    print(rabbits)\n    rabbits = rabbits - 1\n\n3\n2\n1\n\n\n\n\n\nVideo Demo\nThis quick video demo shows an example of implementing a loop in python. We’ll look at the Fizzbuzz example. Then we’ll write a quick number guessing game! Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=ed3118c1-feb1-4d7f-a98c-b0f800ebfd46&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")"
  },
  {
    "objectID": "01_Programming_in_python/07-Common_Uses_For_Data_Landing.html",
    "href": "01_Programming_in_python/07-Common_Uses_For_Data_Landing.html",
    "title": "Common Uses for Data",
    "section": "",
    "text": "The video below discusses the common ways we use data. We discuss the general idea of statistical learning and using data for descriptive purposes, for predictive purposes, in order to make statistical inferences, and to find patterns in data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/07-Common_Uses_For_Data_Landing.html#notes",
    "href": "01_Programming_in_python/07-Common_Uses_For_Data_Landing.html#notes",
    "title": "Common Uses for Data",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThis wraps up the content for week 1. You should head back to our Moodle site to check out your homework assignment for this week.\nOtherwise, use the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#big-picture",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#big-picture",
    "title": "List Basics & Strings",
    "section": "Big Picture",
    "text": "Big Picture\nWe’ve learned a little about how python and our Jupyterlab coding environment works.\nNext, we’ll go through and look at a number of common data structures used in python. We’ll try to follow a similar introduction for each data struture where we\n\nintroduce the data structure\ndiscuss common functions and methods\ndo some quick examples of using them\n\nAlong the way we’ll learn some things we want to do with data along with control flow operators (if/then/else, looping, etc.)!\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#data-structures",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#data-structures",
    "title": "List Basics & Strings",
    "section": "Data Structures",
    "text": "Data Structures\n\nWe’ll start by discussing the most important built-in data types\n\nStrings, Numeric types, Booleans\nCompound data types (Lists, Tuples, Dictionaries)\n\nThen we’ll move to commonly used data structures from modules we use in statistics/data science\n\nNumPy arrays\nPandas data frames\n\n\nLists, Tuples, Strings, and arrays are all sequences (ish) so they have similar functions and behavior! It is important to recognize these common behaviors"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#constructing-a-list",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#constructing-a-list",
    "title": "List Basics & Strings",
    "section": "Constructing a List",
    "text": "Constructing a List\nFour major ways to create a list - [element1, element2] - list((element1, element2, ...)) - create an empty list and use the append method to add elements - list comprehensions\n\n#first create via [1st_element, 2nd_element, etc]\nx = [10, 15, 10, 100, \"Help!\"]\nprint(type(x))\nx\n\n&lt;class 'list'&gt;\n\n\n[10, 15, 10, 100, 'Help!']\n\n\n\n#create via list()\n#Note the 'extra' set of () needed within\ny = list((\"Python\", \"List\", 5))\ny\n\n['Python', 'List', 5]\n\n\n\n#range() is a function that is 'iterable'. By putting it in a list, we get the values out\nrange(1,10)\n\nrange(1, 10)\n\n\n\n#notice range doesn't give the 'last' value\nz = list(range(1,10))\nz\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nOn sequence type objects, * replicates the object a certain number of times. This is common behavior to remember!\n\nz * 2\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nAs lists don’t really restrict what its elements can be, lists can contain lists!\n\nw = [list(range(1,3)), z, 3]\nw\n\n[[1, 2], [1, 2, 3, 4, 5, 6, 7, 8, 9], 3]"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#list-operations",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#list-operations",
    "title": "List Basics & Strings",
    "section": "List Operations",
    "text": "List Operations\n\nIndexing\nVery often we want to obtain pieces or elements of an object. We can easily do this with lists.\n\nIndex with a [] after the object name\nCounting starts at 0\nNegative index counts from reverse (starting at 1…)\n\n\nx = [10, 15, 10, 100, \"Help!\"]\nprint(x[0])\nprint(x[1])\nprint(x[-1])\nprint(x[-2])\n\n10\n15\nHelp!\n100\n\n\n\nw = [list(range(1,5)), x, 3]\nprint(w)\n#the first element is a list so the list is returned\nprint(w[0])\n#similar with the second element\nprint(w[1])\n\n[[1, 2, 3, 4], [10, 15, 10, 100, 'Help!'], 3]\n[1, 2, 3, 4]\n[10, 15, 10, 100, 'Help!']\n\n\nWe can do more than one level of indexing with a single line of code (when applicable). As w[1] returns a list we can use [] after w[1] to return a specific element or slice from that list.\n\nprint(w[1][0])\n\n10\n\n\n\n\n\nSlicing\nOften we want to return more than one element at a time with our sequence type objects. This is called slicing.\n\nWe can return multiple elements at once with :\n\nLeaving it blank on the left gives everything up until the index prior to the number given\nBlank on the right gives everything after the desired starting index (counting starts at 0)\n\n\n\nx = [10, 15, 10, 100, \"Help!\"]\nx\n\n[10, 15, 10, 100, 'Help!']\n\n\n\nx[:2]\n\n[10, 15]\n\n\n\nx[:3]\n\n[10, 15, 10]\n\n\n\nx[1:]\n\n[15, 10, 100, 'Help!']\n\n\n\nx[1:3]\n\n[15, 10]\n\n\nAgain, if we have a list with lists (or other sequence type objects in them) slicing will still return those objects as a list.\n\nw = [list(range(1,5)), x, 3]\nw\n\n[[1, 2, 3, 4], [10, 15, 10, 100, 'Help!'], 3]\n\n\n\n#here a list of lists\nw[:2]\n\n[[1, 2, 3, 4], [10, 15, 10, 100, 'Help!']]\n\n\n\n#here just the single list\nw[1]\n\n[10, 15, 10, 100, 'Help!']\n\n\n\n#can index what gets returned if that makes sense to do!\nw[1][1:3]\n\n[15, 10]"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#functions-methods",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#functions-methods",
    "title": "List Basics & Strings",
    "section": "Functions & Methods",
    "text": "Functions & Methods\nRecall: Two major ways to do an operation on a variable/object: functions and methods\n\nFunctions: function_name(myvar, other_args)\nWe saw the len() and max() functions earlier\n\n\nmyList = [1, 10, 100, 1000]\nprint(len(myList))\nmax(myList)\n\n4\n\n\n1000\n\n\n\nMethods: myvar.method(other_args)\nRecall that .pop() returns and removes the last element\n\n\nmyList.pop(3)\n\n1000\n\n\n\nmyList\n\n[1, 10, 100]\n\n\n\nThe .append() method adds an element to the end of the list\n\n\nmyList.append(100000)\nmyList\n\n[1, 10, 100, 100000]\n\n\nThe methods for lists are listed at the top of this page of the python 3 documentation.\nSome of the common functions in python are listed on this page of the documentation."
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#constructing-strings",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#constructing-strings",
    "title": "List Basics & Strings",
    "section": "Constructing Strings",
    "text": "Constructing Strings\n\nText is represented as a sequence of characters (letters, digits, and symbols) called a string (Nice reference)\n\nData type: str\nCreated using single or double quotes\n\n\n\n#can use either ' or \" to create a string\n'wolf'\n\n'wolf'\n\n\n\n\"pack\"\n\n'pack'\n\n\n\nx = 'wolf'\nprint(type(x))\nprint(x)\n\n&lt;class 'str'&gt;\nwolf\n\n\n\nInstead of ’ or “, you can use str() to create a string. This is called casting\n\n\nx = str(10)\nx\n\n'10'"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#string-operations",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#string-operations",
    "title": "List Basics & Strings",
    "section": "String Operations",
    "text": "String Operations\n\nIndexing\nRemember that strings and lists are both sequence type objects. Therefore, we have similar operations on these objects.\n\nmy_string = \"wolf pack\"\n\n\nEach element of the my_string variable contains a different character from \"wolf pack\"\nAs with lists, we access these elements using []\nThe first element is indexed by 0\n\n\nmy_string[0]\n\n'w'\n\n\n\nmy_string[1]\n\n'o'\n\n\n\nAccess the elements of the my_string variable in reverse order using a - (start with 1 not 0 for the last element though!)\n\n\nmy_string[-1]\n\n'k'\n\n\n\n\n\nSlicing\n\nmy_string = \"wolf pack\"\n\n\nSlicing a string refers to returning more than one character of a string (similar to lists!)\n\nSlice using :\n\n\n\nmy_string[4:]\n\n' pack'\n\n\n\nmy_string[:3]\n\n'wol'\n\n\n\nmy_string[3:4]\n\n'f'\n\n\n\n#s[:i] + s[i:] gives back s\nmy_string[:3] + my_string[3:]\n\n'wolf pack'\n\n\n\n\n\nConcatenating\nSeveral built-in operations on strings\n\n+ will concatenate two strings together\n\n\n'wolf' + ' pack'\n\n'wolf pack'\n\n\n\n'wolf' + ' pack' + \" is\" + \" cool\"\n\n'wolf pack is cool'\n\n\n\nString literals next to each other are automatically concatenated\n\n\n'wolf' ' pack'\n'wolf' ' pack' ' is' ' cool'\n\n'wolf pack is cool'\n\n\n\nThis won’t work on variables though!\n\n\nx = 'wolf'\n#throws an error\nx ' pack'\n\nSyntaxError: invalid syntax (&lt;ipython-input-48-e8177530793e&gt;, line 3)\n\n\n\nx + ' pack'\n\n'10 pack'\n\n\nThis behavior actually works with lists as well!\n\n[1, 2, 3] + [\"a\", [5, 6,]]\n\n[1, 2, 3, 'a', [5, 6]]\n\n\n\n\n\nNo Implicit Coercion\nYou might wonder what happens when an operator like + is applied to a string and a numeric value.\n\n#throws an error\n'wolfpack' + 2\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nIf you come from R, R does this implicit coercion for you without warning (dangerous but you get used to it!)\nIn python, to join a string and number cast the number as a string!\n\n\n'Four score and ' + str(7) + ' years ago'\n\n'Four score and 7 years ago'\n\n\n\nString Operations (Concatenating Repeats)\nYou can also repeat strings with the * operator and an integer (again similar to a list)\n\nprint('go pack ' * 3)\nprint('go pack ' * 0) #returns an empty string ''\nprint('go pack ' * 5)\n\ngo pack go pack go pack \n\ngo pack go pack go pack go pack go pack"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#functions-methods-1",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#functions-methods-1",
    "title": "List Basics & Strings",
    "section": "Functions & Methods",
    "text": "Functions & Methods\n\nThere are several built-in operations on strings\n\nlen() returns the number of characters\nsorted() returns the sorted values as a list\n\n\n\nlen('wolf pack')\n\n9\n\n\n\nlen('241!')\n\n4\n\n\n\nlen(' ')\n\n1\n\n\n\nlen(\"\")\n\n0\n\n\n\nsorted(\"wolf pack\")\n\n[' ', 'a', 'c', 'f', 'k', 'l', 'o', 'p', 'w']\n\n\n\nMany methods as well. Some common examples are below:\n\n\nmy_string = '  wolf pack  '\n\n\n#create an upper case version of the string\nmy_string.upper()\n\n'  WOLF PACK  '\n\n\n\n#this doesn't overwrite the string though!\nmy_string\n\n'  wolf pack  '\n\n\n\n#remove whitespace from the ends\nmy_string.strip()\n\n'wolf pack'\n\n\n\n#replace elements\nmy_string.replace(\"a\", \"e\")\n\n'  wolf peck  '\n\n\n\n#split the string by a character (here a space) (note this returns a list!)\nmy_string.strip().split(\" \")\n\n['wolf', 'pack']"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#immutability-of-strings",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#immutability-of-strings",
    "title": "List Basics & Strings",
    "section": "Immutability of Strings",
    "text": "Immutability of Strings\n\nWe saw that lists could be modified. That means they are mutable\nStrings are immutable\n\nIndividual characters can’t be modified\n\n\n\nmy_string = \"wolf pack\"\n#this will throw an error\nmy_string[1] = \"a\"\n\nTypeError: 'str' object does not support item assignment"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#inserting-values-into-strings",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#inserting-values-into-strings",
    "title": "List Basics & Strings",
    "section": "Inserting Values Into Strings",
    "text": "Inserting Values Into Strings\nSometimes we want to place certain elements into a string via variables or values. This can be done using the .format() method.\n\nyears = 3\nsalary = 100000\nmyorder = \"I have {1} years of experience and would like a salary of {0}.\"\nprint(myorder.format(salary, years))\n\nI have 3 years of experience and would like a salary of 100000.\n\n\n\nDon’t need the numbers, but then you must position correctly\n\n\nmyorder = \"I have {} years of experience and would like a salary of {}.\"\nprint(myorder.format(years, salary))\n\nI have 3 years of experience and would like a salary of 100000.\n\n\nThere are a few other ways to do this that we’ll visit later on!"
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#video-demo",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#video-demo",
    "title": "List Basics & Strings",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video demonstration shows some quick exercises with strings and lists. Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src = \"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=72bd0292-4c48-4064-8977-b0ef017167f6&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")"
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html",
    "href": "01_Programming_in_python/03-Modules.html",
    "title": "Modules",
    "section": "",
    "text": "Now that we know the basics of how python works and a little bit about how our programming environment functions, we can look at python ‘modules’. If you know R, these are smilar to R packages.\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#what-is-a-module",
    "href": "01_Programming_in_python/03-Modules.html#what-is-a-module",
    "title": "Modules",
    "section": "What is a Module?",
    "text": "What is a Module?\n\nA collection of (related) definitions and statements that are grouped together in a single file (a .py file)\n\nGives access to additional functionality\n\nSome come standard, others must be installed (i.e. downloaded)\n\nModules are then imported into your session\n\nFully imported with import module_name\nSelective import with from module_name import thing1 thing2"
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#commonly-used-built-in-already-downloaded-modules",
    "href": "01_Programming_in_python/03-Modules.html#commonly-used-built-in-already-downloaded-modules",
    "title": "Modules",
    "section": "Commonly Used Built-in (Already Downloaded) Modules",
    "text": "Commonly Used Built-in (Already Downloaded) Modules\nThese modules are already downloaded but not loaded in when starting python or a Jupyterlab notebook.\n\nmath\n\nMath constants (pi, e, etc.)\nFunctions commonly used functions (exp(), sin(), sqrt(), etc.)\n\nrandom\n\nRandom sampling and random number generation\n\nstatistics\n\nSummary stats (but scipy and pandas have a lot more)\n\ndatetime\n\nFunctionality for working with dates"
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#importing-a-module",
    "href": "01_Programming_in_python/03-Modules.html#importing-a-module",
    "title": "Modules",
    "section": "Importing a Module",
    "text": "Importing a Module\n\nFor built-in modules or modules we’ve downloaded ourselves, we can load the entire module into our session with import module_name\n\n\nimport math\ntype(math)\n\nmodule\n\n\n\nWe can see the functionality using help() on the module but it is usually better to find the documentation on the web!\n\n\nhelp(math)\n\n\n\nUsing a Module’s Function/Objects\n\nFunctions contained in our module cannot be called as a built-in function when using import module_name:\nFor instance, the math module contains the sqrt() function\n\n\nsqrt(9)\n\nNameError: name 'sqrt' is not defined\n\n\n\nAn error!\nWe must use the module prefix when calling the function:\n\n\nprint(math.sqrt(9))\nmath.factorial(5)\n\n3.0\n\n\n120\n\n\n\nWith the math module we could do a common statistical computation such as evaulating the Normal distribution PDF\n\n\\[\nf(1;\\mu = 3, \\sigma = 1) = \\frac{1}{\\sqrt{2\\pi}}\\exp^{-\\frac{1}{2}(1 - 3)^2}\n\\]\n\nThis Normal distribution PDF expression can be evaluated using:\n\n\n(1.0/math.sqrt(2*math.pi))*math.exp(-0.5*(1 - 3.0)**2)\n\n0.05399096651318806"
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#module-variables",
    "href": "01_Programming_in_python/03-Modules.html#module-variables",
    "title": "Modules",
    "section": "Module Variables",
    "text": "Module Variables\nModules can contain more than functions!\n\nThe math module also defines variables like e and pi\n\n\nprint(math.e)\nprint(math.pi)\nradius = 5\nprint('The area is', math.pi * radius ** 2)\n\n2.718281828459045\n3.141592653589793\nThe area is 78.53981633974483\n\n\n\nYou can overwrite these values (just like built-in objects) but, you know, don’t do that!\n\n\n\nrandom Module\n\nWe’ll deal with random sampling from time to time\n\nThe random module gives functionality to do so (although we’ll use other modules when we do this later)\n\n\nimport random\nhelp(random)\n\n\nWhen we do random number generation, we are really getting pseudo random values\nThe values are actually generated from some algorithm\nWe can se the starting point of that algorithm by setting a seed\nThis allows us to ensure reproducibility of our process!\n\nImportant: A seed sets a starting point for the ‘random’ number generator. This allows you to get the same ‘random’ numbers the next time you run the code.\n\n\n\nrandom.seed(101)\nprint(random.random())\nprint(random.random())\nprint(random.random())\n\n0.5811521325045647\n0.1947544955341367\n0.9652511070611112\n\n\n\n#notice we can get the same 'random value if we set the seed back to the same starting point\nrandom.seed(101)\nprint(random.random())\n\n0.5811521325045647\n\n\n\nWe can also randomly obtain integers from a particular range of values\nWe use the range() function to return an iterator that is able to produce values\nHere we obtain random integers between 10 and 25 and use random.sample() to obtain four values from that range\n\n\nrandom.sample(range(10, 25), 4)\n\n[13, 18, 15, 17]"
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#importing-as",
    "href": "01_Programming_in_python/03-Modules.html#importing-as",
    "title": "Modules",
    "section": "Importing as",
    "text": "Importing as\n\nOften we want to use a shorter module name for brevity\nWe can do so with as when we use import\n\n\nimport random as ran\nran.seed(101)\nprint(ran.random())\nprint(ran.random())\nprint(ran.random())\n\n0.5811521325045647\n0.1947544955341367\n0.9652511070611112\n\n\n\nObtain random integers between 11 and 26\n\n\nran.sample(range(11, 26), 4)\n\n[25, 16, 18, 11]\n\n\nWe’ll see that most of the commonly used modules have common aliases. For instance, numpy as np, pandas as pd, and pyspark as ps."
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#selective-import",
    "href": "01_Programming_in_python/03-Modules.html#selective-import",
    "title": "Modules",
    "section": "Selective Import",
    "text": "Selective Import\n\nWe can also selectively import functions and variables from a module directly into our main namespace\nThis allows us to call the functions without the module prefix\nUse from module import object1, object2 to do this\n\n\nfrom math import sqrt, pi\nprint(sqrt(9))\npi\n\n3.0\n\n\n3.141592653589793\n\n\n\nAlternatively, if you intend to use a function often you can assign it to a local name\n\n\nsample = random.random\nsample()\n\n0.6634706445300605\n\n\n\nOr import everything from a module into the current namespace. Be careful with this as you can overwrite things you rely on!\n\n\nfrom math import *"
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#installing-modules",
    "href": "01_Programming_in_python/03-Modules.html#installing-modules",
    "title": "Modules",
    "section": "Installing Modules",
    "text": "Installing Modules\n\npip is a package manager for python\nUsed through the command line usually\nWe’ll use it through a code cell with ! first\n\nCan see what modules you have\n\npip list\n\nUse pip install module_name to install new modules\n\npip install scipy\n\nColab has most everything we need for now but we’ll need to do some installing later in the course!\n\n! pip list"
  },
  {
    "objectID": "01_Programming_in_python/01-Course_Goals_Landing.html",
    "href": "01_Programming_in_python/01-Course_Goals_Landing.html",
    "title": "Course Goals & Other Resources",
    "section": "",
    "text": "The video below discusses the course goals.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/01-Course_Goals_Landing.html#notes",
    "href": "01_Programming_in_python/01-Course_Goals_Landing.html#notes",
    "title": "Course Goals & Other Resources",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version"
  },
  {
    "objectID": "01_Programming_in_python/01-Course_Goals_Landing.html#additional-readings-for-topic-1",
    "href": "01_Programming_in_python/01-Course_Goals_Landing.html#additional-readings-for-topic-1",
    "title": "Course Goals & Other Resources",
    "section": "Additional Readings for Topic 1",
    "text": "Additional Readings for Topic 1\n\nPIP Information (Optional)\nWe use Pip to install our modules when needed (although most of the ones we need are already available to us on Google Colab). Here is more information about pip:\n\npip documentation\nMore help on understanding the basics of pip\nUsing pip with Colab (very short)\n\n\n\nJupyterLab Info (Optional)\nThere are a lot of options for the IDE to use.  We’ll use JupyterLab as our programming environment for the course. We’ll start out in Colab, which is built on JupyterLab. JupyterLab Notebooks are replacing the traditional Jupyter Notebook. They really just give a bit more functionality than the traditional notebook!\n\nJupyterLab getting started guide\nNote that these notebooks can be used with an R kernel (that is, you can use them to run R code!)\nMarkdown guide\n\n\n\nPython Help (Optional)\nWe’ll continue to learn python throughout the first couple of topics. You may want more detail or another explanation of concepts. The best place to learn about particular objects, functions, or methods is in the official documentation but that can be difficult to read when you are first starting out. Below are a number of tutorials that may be helpful!\n\nA nice tutorial from python.org (although sometimes they throw in some things that may be a bit out there for beginners).\nA good reference for python basics from w3schools. Generally w3schools is good to just get an idea about lots of languages! But overall the tutorials aren’t great.\nThis is a nice website but a little hard to navigate. I find that searching in the box for something (say ‘lists’) tends to bring up a pretty good article.\nGeeks for geeks is pretty solid!\nnumpy documentation\npandas documentation\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of Big Data",
    "section": "",
    "text": "Welcome to ST 554 - Analysis of Big Data (with python)\nIn this course we’ll look at common issues, analysis, and software used with big data. We’ll discuss the major aspects with the commonly cited ‘5 V’s of Big Data’:\nVolume, Variety, Velocity, Veracity (Variability), and Value\nThe course covers\nUsing python as our programming language we’ll learn about using Jupyter notebooks to share and document our work. We’ll use pyspark as our interface to the Spark software, which is commonly used to handle big data."
  },
  {
    "objectID": "index.html#course-learning-outcomes",
    "href": "index.html#course-learning-outcomes",
    "title": "Analysis of Big Data",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\nAt the end of this course students will be able to\n\nexplain the steps and purpose of python programs (CO 1)\nefficiently read in, combine, and manipulate data in python (CO 2)\nutilize help and other resources to customize programs (CO 3)\nwrite programs using good programming practices (CO 4)\nexplore, manage, and solve common common problems with big data (CO 5)"
  },
  {
    "objectID": "index.html#weekly-to-do-list",
    "href": "index.html#weekly-to-do-list",
    "title": "Analysis of Big Data",
    "section": "Weekly To-do List",
    "text": "Weekly To-do List\nGenerally speaking, each week will have a few videos to watch and readings to do as well as corresponding homework assignments (see the syllabus on Moodle for homework policies).\n\nThere will be two exams and the exam windows (days when you can take the exams) are available on the syllabus and course schedule.\nThere will be three projects, the third of which will count as the final for the course. These will require a reasonably substantial time commitment."
  },
  {
    "objectID": "index.html#getting-help",
    "href": "index.html#getting-help",
    "title": "Analysis of Big Data",
    "section": "Getting Help!",
    "text": "Getting Help!\nTo obtain course help there are a number of options:\n\nDiscussion Forum on Moodle - This should be used for any question you feel comfortable asking and having others view. The TA, other students, and I will answer questions on this board. This will be the fastest way to receive a response!\nE-mail - If there is a question that you don’t feel comfortable asking the whole class you can use e-mail. The TA and I will be checking daily (during the regular work week).\nZoom Office Hour Sessions - These sessions can be used to share screens and have multiple users. You can do text chat, voice, and video. They are great for a class like this!"
  },
  {
    "objectID": "index.html#spring-2025-course-schedule",
    "href": "index.html#spring-2025-course-schedule",
    "title": "Analysis of Big Data",
    "section": "Spring 2025 Course Schedule",
    "text": "Spring 2025 Course Schedule\n\n\n\n\n\n\n\n\nTopic/Week\nLearning Materials\nAssignments\n\n\n\n\nWeek 1\n1/6-1/10\n01 - Course Goals & Other Resources\n02 - Basic Use of Python\n03 - Modules\n04 - JupyterLab Notebooks & Markdown\n05 - List Basics and Strings\n06 - Numeric Types and Booleans\n07 - Common Uses for Data\nHW 1 due W, 1/15\n\n\nWeek 2\n1/13-1/17\n08 - User Defined Functions\n09 - Control Flow\n10 - Lists and Tuples\n11 - Dictionaries\n12 - Numpy\nHW 2 due W, 1/22\n\n\nWeek 3\n1/21-1/24 (Off M)\n13 - Exploratory Data Analysis Concepts\n14 - Pandas Series\n15 - Pandas DataFrames\n16 - Pandas for Reading Data\n17 - Numeric Summaries\nHW 3 due W, 1/29\n\n\nWeek 4\n1/27-1/31\n18 - More Function Writing\n19 - Plotting with Matplotlib\n20 - Plotting with pandas\n21 - Error Handling\nHW 4 due W, 2/5\n\n\nWeek 5\n2/3-2/7\n22 - Big Recap!\n23 - Fitting and Evaluating SLR Models\n24 - Prediction and Training/Test Set Ideas\n25 - Cross-Validation\n26 - Multiple Linear Regression\n27 - LASSO\nExam 1 Th/F 2/6-2/7 - covers weeks 1-4\nProject 1 due W, 2/19\n\n\nWeek 6\n2/10-2/14 (Off T)\nNo new material. Project work time!\n\n\n\nWeek 7\n2/17-2/21\n28 - Big Data Concepts\n29 - Bias and Other Issues with Big Data\n30 - SQL Basics\n31 - SQL Joins\n32 - SQL Readings\nHW 5 due W, 2/26\n\n\nWeek 8\n2/24-2/28\n33 - Data Flow, Data Warehouses, and Data Lakes\n34 - HDFS\nHW 6 due W, 3/5\n\n\nWeek 9\n3/3-3/7\n35 - Connecting to our JupyterHub Environment\n35 - Spark for Dealing with Big Data\n36 - pyspark: RDDs\n37 - pyspark: pandas-on-Spark\n38 - pyspark: Spark SQL\nProject 2 due W, 3/19\n\n\nWeek 10\n3/10-3/14\nNo new material - spring break\n\n\n\nWeek 11\n3/17-3/21\n39 - Modeling Recap\n40 - Modeling Example\n41 - Logistic Regression Basics\n42 - Logistic Regression Extensions\n43 - Regularized Regression\nHW 7 due W, 3/26\n\n\nWeek 12\n3/24-3/28\n44 - Loss Functions & Model Performance\n45 - Classification & Regression Trees\n46 - Bagging Trees & Random Forests\nHW 8 due W, 4/2\n\n\nWeek 13\n3/31-4/4\n47 - Spark MLlib Basics\n48 - Model Pipelines in MLlib\n49 - MLflow\n50 - MLOps\nHW 9 due W, 4/9\n\n\nWeek 14\n4/7-4/11\n51 - Streaming Data Concepts\n52 - Basic Summaries on Streaming Data\n53 - Preprocessing, Sending Alerts, & Combining Streams\nExam 2 Th/F 4/10-4/11 - covers weeks 1-13\n(emphasis on 5-13)\n\n\nWeek 15\n4/14-4/18\n54 - Spark Structured Streaming\n55 - Reading & Writing Streams with Spark Structured Streaming\n56 - Transformations, Windowing, & Aggregations\n57 - Streaming Joins\nFinal Project due M, 4/28\n\n\nWeek 16\n4/21-4/22\nNo new material."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website houses the learning materials for ST 554 - Big Data Analysis at NC State."
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html",
    "title": "Basic Use of Python",
    "section": "",
    "text": "In preparation for dealing with big data we need to learn a programming language and figure out a good coding environment. We’ll learn python and code in Google Colab/JupyterLab.\nWe choose python due to its popularity and the ease of programming in spark (a big data software) through pyspark.\nWe use JupyterLab as it is a widely used software for creating python notebooks. Google Colab is built on JupyterLab!\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#getting-started",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#getting-started",
    "title": "Basic Use of Python",
    "section": "Getting Started",
    "text": "Getting Started\nWhen you open a new notebook in colab by default it will use python to run any ‘code cells’ (this can be changed in the ‘notebook settings’ under the View -&gt; ‘Notebook info’ menu).\nThere are two types of cells: - Code cells: allow you to submit code - Text cells: allow you to write text using ‘markdown’ (we’ll learn more about that shortly!)\nThese can be added in the top left of the notebook (+ Code and + Text). Below is a python code cell. These can be run by clicking ‘shift-enter’ when you click on the cell.\n\n#A comment - this text is not evaluated\n5 + 6\n10 * 2\n5**2\n\n25\n\n\n\nOnly the last bit of code is ‘printed’ unless you specifically print it. We’ll do this much of the time with print() function.\n\n\n# % is mod, // is floor\nprint(10 / 3)\nprint(10 % 3)\nprint(10 // 3)\n\n3.3333333333333335\n1\n3\n\n\n\nOperators are applied left to right, except for exponentiation\n\n\n3 + 4 - 5\n\n2\n\n\n\n(3 + 4) - 5\n\n2\n\n\n\n3**2**4\n\n43046721\n\n\n\n#interpreted this way\n3**(2**4)\n\n43046721\n\n\n\n#not this\n(3**2)**4\n\n6561"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#creating-variables",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#creating-variables",
    "title": "Basic Use of Python",
    "section": "Creating Variables",
    "text": "Creating Variables\nYou can create an object using =. This saves the result in a variable you can call later.\n\nx = \"Hello! \"\ny = 'How are you?'\nprint(x)\nprint(x + y)\n\nHello! \nHello! How are you?\n\n\n\nStrings are automatically concatenated using the + operator. As with most programming languages, there are special characters like \\ which indicate something. For instance, \\n is a line break. These appear differently depending on if you print something or just view the object.\n\n\nx = \"Hello! \\n\"\ny = 'Then I asked, \"How are you?\"'\nx\n\n'Hello! \\n'\n\n\n\nprint(x)\n\nHello! \n\n\n\n\nx + y\n\n'Hello! \\nThen I asked, \"How are you?\"'\n\n\n\nprint(x + y)\n\nHello! \nThen I asked, \"How are you?\"\n\n\n\nVariables can be used to simplify and generalize your code\n\n\ndegrees_celsius = 26.0\nprint(9 / 5 * degrees_celsius + 32)\ndegrees_celsius = 100\nprint(9 / 5 * degrees_celsius + 32)\n\n78.80000000000001\n212.0\n\n\nYou might try to add a code cell to this notebook and\n\nCreate a new string variable\nUse + to concatenate it with the strings above"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#object-types",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#object-types",
    "title": "Basic Use of Python",
    "section": "Object Types",
    "text": "Object Types\nThere are a number of built-in objects you can create. Some important ones are listed below:\n\nText Type: str\n\n\ny = \"text string\"\ntype(y)\n\nstr\n\n\n\nNumeric Types: int, float\n\n\ny = 10\nprint(type(y))\nx = 10.4\nprint(type(x))\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n\n\n\nBoolean Type: bool\n\n\ny = True\ntype(y)\n\nbool\n\n\n\nSequence Types: list, tuple\n\n\nz = [10, \"a\", 11.5, True]\ntype(z)\n\nlist\n\n\n\nMapping Type: dict\n\n\nw = {\"key1\": \"value1\",\n     \"key2\": [\"value2\", 10]}\ntype(w)\n\ndict\n\n\nWe’ll cover these data types and their uses shortly!"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#multiple-assignment",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#multiple-assignment",
    "title": "Basic Use of Python",
    "section": "Multiple Assignment",
    "text": "Multiple Assignment\n\nAssigning multiple variables on one line is easy in python\n\n\nx, y, z = \"Orange\", \"Banana\", \"Cherry\"\nprint(x)\nprint(y)\nprint(z)\n\nOrange\nBanana\nCherry\n\n\n\nx = y = z = \"Orange\"\nprint(x)\nprint(y)\n\nOrange\nOrange\n\n\nThe use of * can allow you to ‘pack’ the remaining values into one object. Placement of the * is important here!\n\nx, *y = \"Orange\", \"Banana\", \"Cherry\"\nprint(x)\nprint(y)\ntype(y)\n\nOrange\n['Banana', 'Cherry']\n\n\nlist\n\n\n\n*x, y = \"Orange\", \"Banana\", \"Cherry\"\nprint(x)\nprint(y)\n\n['Orange', 'Banana']\nCherry\n\n\nWe’ll utilize packing and upnacking to simplify our code in many places!"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#variable",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#variable",
    "title": "Basic Use of Python",
    "section": "_ Variable",
    "text": "_ Variable\nWhen doing python interactively (as with a JupyterLab notebook), the last evaluated expression is assigned to the variable _. This carries across code cells.\n\nx, y, z = \"Orange\", \"Banana\", \"Cherry\"\nx\n\n'Orange'\n\n\n\n_\n\n'Orange'\n\n\n\nx\n\n'Orange'\n\n\n\n#print doesn't count toward the _!\nprint(y)\n\nBanana\n\n\n\n_\n\n'Orange'\n\n\n\ny\n\n'Banana'\n\n\n\n_\n\n'Banana'\n\n\nWe’ll use this _ operator when doing computations where we don’t need to save things. For instance,\n\ndegrees_celcius = 100\n(9 / 5) * degrees_celcius + 32\n\n212.0\n\n\n\n_ - 10\n\n202.0\n\n\n\n(9 / 5) * degrees_celcius + 32 - 10\n\n202.0\n\n\n\n_ * 10\n\n2020.0\n\n\nWhere it really comes in handy is as a placeholder variable when doing computations in a for loop or list comprehension (again covered later more fully!).\nHere we replace the index of the for loop with _.\n\nsum_numbers = 0\n#no need to create a variable for the index\nfor _ in range(1,101):\n  sum_numbers += _\nsum_numbers\n\n5050"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#copying-vs-referencing",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#copying-vs-referencing",
    "title": "Basic Use of Python",
    "section": "Copying vs Referencing",
    "text": "Copying vs Referencing\nCareful when modifying elements of a compound object: ‘assignment statements do not copy objects, they create bindings between a target (a spot in computer memory) and an object’!\nIf you come from R, this is a very different behavior!\n\n#Changing the original compound object (list) modifies both variables\n#First, create a 'list' of four values\nx = [1, 2, 3, \"Cats Rule!\"]\n#Make y an alias for x (reference the same memory - this differs from how R works)\ny = x\n#note that they are the same when printing\nprint(x, y)\n\n[1, 2, 3, 'Cats Rule!'] [1, 2, 3, 'Cats Rule!']\n\n\nWe can modify a list element by using [] after the object name. Note that python starts counting at 0. - Here we access and overwrite the 3 element (fourth element in the list)\n\n#Modifying x here actually modifies y too!\nx[3] = \"Dogs rule!\"\nprint(x, y)\n\n[1, 2, 10, 'Dogs rule!'] [1, 2, 3, 'Dogs rule!']\n\n\n\nIf you want to avoid this behavior, you can create a copy of the object instead of a reference\nTo do so, we use the .copy() method. Methods are like functions but we append them to the rear of the object after a .\n\n\n#Can create a (shallow) copy of the object rather than point to the same object in memory\ny = x.copy()\nx[2] = 10\nx[3]= \"No cats rule!\"\n#Note that y doesn't change its value\nprint(x, y)\n\n[1, 2, 10, 'No cats rule!'] [1, 2, 10, 'Dogs rule!']"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#variable-names",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#variable-names",
    "title": "Basic Use of Python",
    "section": "Variable Names",
    "text": "Variable Names\nVariable names can use letters, digits, and the underscore symbol (but cannot start with a digit)\nOk variable names:\n\nX, species5618, and degrees_celsius\n\nBad variable names:\n\n777 (begins with a digit)\nno-way! (includes punctuation)"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#augmented-assignment",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#augmented-assignment",
    "title": "Basic Use of Python",
    "section": "Augmented Assignment",
    "text": "Augmented Assignment\nPython has lots of shorthand notation!\n\nQuite often we want to take a value, add to it, and replace the old value\n\n\nwinnings = 100\nwinnings = winnings + 20\nwinnings\n\n120\n\n\n\n‘Augmented assignment’ gives a shorthand for doing this\n\n\nwinnings = 100\nwinnings += 20\nwinnings\n\n120\n\n\n\nThis works for all operators except negation\n\n\n#subtraction\nwinnings\nwinnings -= 30\nwinnings\n\n90\n\n\n\n#multiplication\nwinnings *= 40\nwinnings\n\n3600\n\n\n\n#exponentiation\nwinnings **= 1/2\nwinnings\n\n60.0\n\n\n\nAugmented Assignment Execution\nExecuted in the following way:\n\nEvaluate the expression on the right of the = sign to produce a value\nApply the operator to the variable on the left and the value produced\nStore this new value in the memory address of the variable on the left of the =.\n\nThis means the operator is applied after the expression on the right is evaluated.\n\nwinnings = 100\nwinnings += 100*10\nwinnings\n\n1100"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#continuing-a-line-of-code",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#continuing-a-line-of-code",
    "title": "Basic Use of Python",
    "section": "Continuing a Line of Code",
    "text": "Continuing a Line of Code\n\nFor long lines of code, we can break the code across multiple lines using \\ or by wrapping the code in ()\n\n\n10 + 20 - 100 * 60 \\\n/ 20\n\n-270.0\n\n\n\n(10 + 20 - 100 * 60\n/20)\n\n-270.0\n\n\nUsing \\ is going to come in very handy when we want to apply multiple methods on one object later in the semester!"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#functions-methods",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#functions-methods",
    "title": "Basic Use of Python",
    "section": "Functions & Methods",
    "text": "Functions & Methods\nTwo major ways to do an operation on a variable/object:\n\nFunctions: function_name(myvar, other_args)\nMethods: myvar.method(other_args)\n\nFunctions are usually more generic actions that you could take on multiple types of objects. For instance, len() is a function we can run to see the ‘length’ of an object.\n\nmyList = [1, 10, 100, 1000]\n#len function\nlen(myList)\n\n4\n\n\nSimilarly, max() is another function we can use on many types of objects.\n\n#max function\nmax(myList)\n\n1000\n\n\nMethods on the other hand are specific to the type of object you are dealing with. Lists will have different methods than a dictionary, for instance.\nHere we use the .pop() method on a this list. This returns and removes the last element from the list.\n\n#pop method\nmyList.pop(3)\n\n1000\n\n\n\n#last element removed\nmyList\n\n[1, 10, 100]\n\n\nThe .append() method adds an element to the end of the list.\n\nmyList.append(100000)\nmyList\n\n[1, 10, 100, 100000]"
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#video-demo",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#video-demo",
    "title": "Basic Use of Python",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video shows how to open a new Google Colab notebook and run some basic python code. I’d pop the video out into the panopto player using the arrow icon in the bottom right.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src = 'https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=bae161a8-bac0-4c44-a7a1-b0ef0163e90d&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all', width = '620', height = '380')"
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html",
    "title": "Markdown Capabilities",
    "section": "",
    "text": "You’ve seen that we can include text cells that get ‘marked up’ into a nicer format in our notebooks. This notebook discusses this markdown syntax!\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#what-is-markdown",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#what-is-markdown",
    "title": "Markdown Capabilities",
    "section": "What is Markdown?",
    "text": "What is Markdown?\n\nMost have heard of HTML (HyperText Mark-up Language)\n\nThere we write plain text that the browser interprets and renders\n\nMarkdown is a specific ‘mark-up’ language\n\nEasier syntax\n\nNot as powerful\n\nCan be used in ‘Text’ cells\nDouble click a text cell to see the plain text formatting used!\nShift enter to make the cell ‘render’"
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#markdown-syntax",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#markdown-syntax",
    "title": "Markdown Capabilities",
    "section": "Markdown Syntax",
    "text": "Markdown Syntax\nLet’s just go through examples of some of the most commonly used syntax (you can always see the syntax used in our notebooks by opening them in Colab and double clicking a cell!)\n\nGeneral link: [link](URL or relative link)\n\nMarkdown Reference from Jupyter with much more info than what we’ll go through here\n\n*italics* and _italics_\n**bold** and __bold__\n_**italics and bold!**_ is fancy and loud\n~~strikethrough~~ becomes strikethrough\n\n\n\n--- Three dashes for a thematic break\n# Level 1 Header, ## Level 2 Header, to six levels\nImportant!: The use of headers can automatically creates a Table of Contents! This is super important for jumping through your document.\n\nSee top left icon on Colab\n\nInclude an image: ![](path/to/file.png) (double click this cell to see the URL we pull this image from)\n\n\n\nSurround text with backticks to make ‘code font’ (this makes it look like you know what you are doing :)"
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#widgets-and-such",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#widgets-and-such",
    "title": "Markdown Capabilities",
    "section": "Widgets and Such",
    "text": "Widgets and Such\nWe can bring in lots of HTML style widgets into our notebooks! Widget Info\n\nLots of fun widgets that you can add\n\nMaps\nSliders\nText input\n\nCheck boxes\nEtc.\n\nThings won’t always work quite right in Colab but will when we move to JupyterLab later in the semester!"
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#video-demo",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#video-demo",
    "title": "Markdown Capabilities",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video demo shows how we can include some interactive HTML content in our Colab notebook. (Of course you can see we are bringing in a video here as well!) Remember to pop the video out into the full player using the button in the bottom right.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src = 'https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=32a49a33-bc92-49ab-b767-b0ef0167e1b3&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all', width = '720', height = '405')"
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "",
    "text": "Next up, we look at two basic built-in data types: numeric types and booleans\nAs with lists and strings, we’ll go through and look at how to create them, common methods and functions, and look at some examples along the way.\nOnce we’re through this part, we’re ready to start thinking about compound data types (other than lists) and how we might store and summarize data. For dealing with data, we’ll look at two common python modules and their data types:\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#ints-floats",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#ints-floats",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Ints & Floats",
    "text": "Ints & Floats\n\n(Real) Numbers are stored as int or float types\n\nPython generally figures out which to use and changes to float when needed\n\n\n\nx = 10\ntype(x)\n\nint\n\n\n\ny = 10.4\ntype(y)\n\nfloat\n\n\n\nz = y - 0.4\nprint(z)\ntype(z)\n\n10.0\n\n\nfloat\n\n\n\ntype(x + 0.5)\n\nfloat\n\n\n\nYou can cast things (or explicitly coerce them) using int() and float()\n\n\nx = 10\nprint(type(x))\nx\n\n&lt;class 'int'&gt;\n\n\n10\n\n\n\nx = float(x)\nprint(type(x))\nx\n\n&lt;class 'float'&gt;\n\n\n10.0\n\n\n\nint(10.9) #returns just the integer part (no rounding done)\n\n10"
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#functions-operators",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#functions-operators",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Functions & Operators",
    "text": "Functions & Operators\n\nWe have all the numeric operators discussed previously and a few handy functions built in too\n\n\n#divide and discard remainder\n100 // 3\n\n33\n\n\n\n#modulus or remainder\n100 % 3\n\n1\n\n\n\n#whole number division and modulus\ndivmod(100, 3) #returns a 'tuple' (a sort of immutable list)\n\n(33, 1)\n\n\n\n#raise to a power 4^3\npow(4, 3)\n\n64\n\n\n\n#equivalent to\n4 ** 3\n\n64\n\n\n\nabs(-100)\n\n100\n\n\n\nround(10.4242, 2)\n\n10.42\n\n\n\nmath module\n\nAs we saw, the math module has a number of useful functions\nRecall we can import the math module to gain access to its functions. We then preface functions/objects from the module with math.\n\n\nx = 10.55\n#a boolean function (more on this shortly)\nx.is_integer()\n\nFalse\n\n\n\nimport math\nmath.floor(x)\n\n10\n\n\n\nmath.ceil(x)\n\n11\n\n\n\nmath.factorial(10)\n\n3628800"
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#things-to-be-aware-of",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#things-to-be-aware-of",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Things To Be Aware Of",
    "text": "Things To Be Aware Of\nFloats are not stored precisely!\n\n1.2-1.0\n\n0.19999999999999996\n\n\n\nComes from using a binary representation of floats\nNot worth getting into, but if you see something weird like this, that is why!\nMore info here"
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#things-to-noteremember",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#things-to-noteremember",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Things to Note/Remember",
    "text": "Things to Note/Remember\n\nAugmented assignment operators are useful\n\n\nx = 100\nx += 200\nx\n\n300\n\n\n\nMultiple assignment can be done\n\n\nx = y = z = 40\nprint(x, y)\n\n40 40\n\n\n\nx, y, z = 40, 50, 60\nprint(x, y)\nprint(z)\n\n40 50\n60"
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#more-formatting-strings",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#more-formatting-strings",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "More Formatting Strings",
    "text": "More Formatting Strings\nWe saw how to format strings earlier. Let’s revit that year!\n\nJob = \"Professor\"\nYears = 10.23\nmy_string = \"I am a {job} and I've been teaching for {years:d} years\"\n\nWe can use the .format() method on the string to place values in the placeholders. The years:d above specifies the type of formatting to use on the number, d stands for integer\n\nmy_string.format(job = Job, years = Years) #throws an error as it expects an integer for years\n\nValueError: Unknown format code 'd' for object of type 'float'\n\n\n\nmy_string.format(job = Job, years = int(Years)) #cast years as an integer via int()\n\n\"I am a Professor and I've been teaching for 10 years\"\n\n\nWe can specify the type of number input for the .format() method to use via this name:number_type syntax:\n\nd - Integers\nf - Floating point numbers\n.f - Floating point numbers with a fixed amount of digits to the right of the dot.\n\n\nprint(\"I am a {job} and I've been teaching for {years:f} years\".format(job = Job, years = Years))\nprint(\"I am a {job} and I've been teaching for {years:.1f} years\".format(job = Job, years = Years))\n\nI am a Professor and I've been teaching for 10.230000 years\nI am a Professor and I've been teaching for 10.2 years\n\n\nActually four different ways to substitute into a string (if you are interested!)"
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#booleaninteger-relationship",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#booleaninteger-relationship",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Boolean/Integer Relationship",
    "text": "Boolean/Integer Relationship\n\nBooleans are actually a subtype of integers\n\nTrue treated as 1\nFalse treated as 0\n\n\n\nprint(3 + True, 3 * False)\n\n4 0\n\n\nOne thing of note is that when you do math on True or False it converts the result. Note the last computation result below.\n\nprint(str(True), str(False), str(True + 0))\n\nTrue False 1"
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#video-demo",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#video-demo",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video shows some useful functions from the .math module for dealing with integers, floats, and booleans. Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=d59f4288-1a98-446e-a82f-b0f0013e8445&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")"
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html",
    "title": "User Defined Functions",
    "section": "",
    "text": "Quick recap - This course is split into a few topics\nWe’re working through learning how to program in python. We’ve seen - how to program through Google Colab - how to bring in modules - common data types: strings, lists, numeric types (and booleans)\nWe’ve also seen a bit about how to think about data. Now, we’ll focus on improving our programs a bit before we get back to handling data!\nIn order to get the most out of any programming language, we need to understand how to write our own functions. User-Defined functions allow us to streamline our code, simplify large sections of code, and make our code easier to generalize to other situations.\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#function-creation-syntax",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#function-creation-syntax",
    "title": "User Defined Functions",
    "section": "Function Creation Syntax",
    "text": "Function Creation Syntax\nTo create our own functions, we just need to\n\nuse the keyword def and give the function name with arguments\ntab in (four spaces) our function body (code that the function runs).\nat the top of the function body we usually add a multi-line string (via triple quotes) explaining the function purpose and arguments (called a doc string)\nwe use return to return an object\n\ndef function_name(arg1, arg2, arg3 = default_arg3):\n    \"\"\"\n    Documentation string\n    \"\"\"\n    Function body\n    return object"
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#write-our-own-mean-function",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#write-our-own-mean-function",
    "title": "User Defined Functions",
    "section": "Write Our Own Mean Function",
    "text": "Write Our Own Mean Function\nWe discussed common tasks for data. Of course one was simply describing a data set that we have. One way to describe the center of a numeric variable’s distribution is through the sample mean.\n\nGiven data points labeled as \\(y_1, y_2, ..., y_n\\) (\\(n\\) is the number of observations), the sample mean is\n\n\\[\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i\\]\nLet’s write a function to calculate the mean of a list of numbers using the sum() and len() functions.\n\ndef find_mean(y):\n    \"\"\"\n    Quick function to find the mean of a list\n    Assumes we have a list with individual numeric type data elements\n    \"\"\"\n    return sum(y)/len(y)\n\nNow let’s apply our function to a list of numeric values. We can create a sequence of values using the range() function. This function takes two arguments, the starting point and the ending point (which isn’t included).\nrange() itself is an immutable iterable type object. It isn’t the values themselves but an object that can be used to create the values. In the case of range() it can be described as a lazy list. We’ll discuss iterators more shortly.\nOne way to get the range() object to create its values is by running list() on it. This tells python to iterate over the range() object and produce the numbers.\n\nseq = range(0,11) #same as range(11)\nseq #doesn't show values\n\nrange(0, 11)\n\n\n\nlist(seq)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\nfind_mean(list(seq))\n\n5.0\n\n\nIterators (and iterator type objects) are often used to save memory as you often don’t need the entire sequence, but do want to use them in some kind of order.\nBy iterating across the elements and not saving the entire object, we can save memory. We only need to know where we are on the iteration and how the iteration should be done!"
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#add-a-default-argument",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#add-a-default-argument",
    "title": "User Defined Functions",
    "section": "Add a Default Argument",
    "text": "Add a Default Argument\nOften we want to give default arguments to our function. That is, arguments that are used unless the user specifies something else.\n\nSuppose we want to add in a trimmed mean functionality\nThis is a mean where we first remove the smallest p% of values and the largest p% of values. We then take the mean of the remaining numbers.\nA trimmed mean is more robust to outliers. For instance,\n\n\nfind_mean([1,2,3,4,5,100]) #the mean is greatly affected by the large value\n\n19.166666666666668\n\n\n\nfind_mean([1,2,3,4,5]) #remove the large value to get a better idea about 'most' of the data values\n\n3.0\n\n\nTo create a trimmed mean function (or option at least), we need to do the following:\n\nSort the observations\nRemove the lowest p% and highest p%\nFind mean on the remaining values\n\n\n#can pull in the floor and sqrt functions from math to help us out\nfrom math import floor, sqrt\n#generate 50 random values from the standard normal distribution (covered shortly)\nimport numpy as np\ny = np.random.default_rng(1).standard_normal(50)\n#convert to a list just so we are working with an object we've studied\ny = list(y)\ny[0:10]\n\n[0.345584192064786,\n 0.8216181435011584,\n 0.33043707618338714,\n -1.303157231604361,\n 0.9053558666731177,\n 0.4463745723640113,\n -0.5369532353602852,\n 0.5811181041963531,\n 0.36457239618607573,\n 0.294132496655526]\n\n\nNote that lists have a .sort() method but this modifies the list in place. Instead we can use the sorted() function which returns a new sorted version of the list.\n\nsort_y = sorted(y)\nprint(sort_y[0:10])\n\n[-2.7111624789659685, -1.8890132459676727, -1.6480751708556527, -1.303157231604361, -1.2273520542445742, -1.1120207626922813, -0.9447516230607774, -0.7819084623568421, -0.7364540870016669, -0.6832266617805622]\n\n\nNow, given a value of p, we can remove the lowest and high p% of values. We can do this with the floor() function. This gives us the largest interger below a given value.\n\nprint(floor(4))\nprint(floor(4.2))\nprint(floor(4.9))\n\n4\n4\n4\n\n\nGiven a p (for proportion) we can determine the number of observations corresponding to that proportion using the length of y.\n\np = 0.11\nprint(p*len(sort_y))\nto_remove = floor(p*len(sort_y))\nto_remove\n\n5.5\n\n\n5\n\n\nWe can remove observations by simply subsetting our list using the : operator we studied (slicing). Remember that this operator doesn’t include the last value. (i.e. 2:5 gives the 2, 3, and 4 values)\n\nprint([to_remove, len(sort_y)-to_remove])#values we want to keep are between these\n\n[5, 45]\n\n\n\nRemember, counting starts at 0\nWe want the remove the first 5 values so we should start with the 5th index (the 6th actual value!)\nWith a length 50 list, we want to remove the 46-50th elements which correspond to the 45-49 indices\nSince we don’t include our last index, we can end on 45\n\n\n#elements we want for a 11% trimmed mean\nsort_y[to_remove:(len(sort_y)-to_remove)]\n\n[-1.1120207626922813,\n -0.9447516230607774,\n -0.7819084623568421,\n -0.7364540870016669,\n -0.6832266617805622,\n -0.5369532353602852,\n -0.5140063716874629,\n -0.5062916583143148,\n -0.48211931267997826,\n -0.42219041157635356,\n -0.37760500712699807,\n -0.2924567509650886,\n -0.2756029052993704,\n -0.2571922406188707,\n -0.17477209205516195,\n -0.16290994799305278,\n -0.09826996785221727,\n -0.07204367972722743,\n 0.008142180518343508,\n 0.02842224131579679,\n 0.03558623705548571,\n 0.03972210748165899,\n 0.09548302746945433,\n 0.10901408782154753,\n 0.16746474422274113,\n 0.2136429974986111,\n 0.21732193102256359,\n 0.294132496655526,\n 0.33043707618338714,\n 0.345584192064786,\n 0.36457239618607573,\n 0.4463745723640113,\n 0.5467129866124469,\n 0.5811181041963531,\n 0.5937480717858228,\n 0.5988462126346276,\n 0.6467029962018469,\n 0.6630633723762617,\n 0.8216181435011584,\n 0.8911669542823284]\n\n\n\nModify the function arguments\nNow that we have the process down (this is a good way to write functions by the way, write them outside of a function first and then put the pieces into the function), we can add our arguments/calculations.\nWe’ll add a - method = argument with a default value of None. None is a special name that defines no value in python + If this argument takes on Trim, we’ll do a trimmed mean. + This can be done using if Boolean: with the resulting code to execute tabbed in four spaces (covered shortly!) - a p = argument to specify the proportion to remove with a default value set to 0.\n\ndef find_mean(y, method = None, p = 0):\n    \"\"\"\n    Quick function to find the mean\n    Assumes we have a list with only numeric type data\n    If method is set to Trim, will remove outer most p values off the data\n    before finding the mean\n    \"\"\"\n    if method == \"Trim\": #we'll cover if shortly! The indented code only runs if this condition is met\n      sort_y = sorted(y)\n      to_remove = floor(p*len(sort_y))\n      y = sort_y[to_remove:(len(sort_y)-to_remove)] #replace y with the modified version\n    return sum(y)/len(y)\n\nLet’s test the function!\n\nfind_mean(y, method = \"Trim\", p = 0) #usual mean\n\n-0.03607807742830818\n\n\n\nfind_mean(y, method = \"Trim\", p = 0.05) #5% trimmed mean\n\n-0.029659532804894563\n\n\n\nfind_mean(y, method = \"trim\", p = 0.05) #usual mean not trimmed if method is not set correctly\n\n-0.03607807742830817"
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#positional-vs-named-arguments",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#positional-vs-named-arguments",
    "title": "User Defined Functions",
    "section": "Positional vs Named Arguments",
    "text": "Positional vs Named Arguments\n\nA function can be called using positional or named args\n\n\n#def find_mean(y, method = None, p = 0):\nprint(find_mean(y, None))\nprint(find_mean(method = \"Trim\", p = 0.1, y = y))\nprint(find_mean(y, \"Trim\", 0.1))\n\n-0.03607807742830817\n-0.009797451217442077\n-0.009797451217442077\n\n\n\nYou can’t place positional args after a keyword though!\n\n\nfind_mean(y = x, \"Trim\") #throws an error\n\nSyntaxError: positional argument follows keyword argument (&lt;ipython-input-20-39dc4eceb262&gt;, line 1)"
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#defining-the-type-of-argument",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#defining-the-type-of-argument",
    "title": "User Defined Functions",
    "section": "Defining the Type of Argument",
    "text": "Defining the Type of Argument\n\nA function definition may look like:\n\ndef f(pos1, pos2, /, pos_or_kwd, *, kwd1, kwd2):\n           -----------    ----------     ----------\n           |              |                  |\n           |         Positional or keyword   |\n           |                                 - Keyword only\n           -- Positional only\n\ndef print_it(x, y, /):\n    print(\"Must pass x and y positionally!\" + x + y)\n\ndef print_it(x, /, y):\n    print(\"x must be passed positionally.  y can be positional or named\" + x + y)\n\ndef print_it(x, /, y, *, z):\n    print(\"Now z must be passed as a named argument\" + x + y + z)\n\nLet’s modify our mean function and show this.\n\n#with this, y must be passes positionally!\ndef find_mean(y, /, method = None, p = 0):\n    \"\"\"\n    Quick function to find the mean\n    Assumes we have a list with only numeric type data\n    If method is set to Trim, will remove outer most p values off the data\n    before finding the mean\n    \"\"\"\n    if method == \"Trim\": #we'll cover if shortly! The indented code only runs if this condition is met\n      sort_y = sorted(y)\n      to_remove = floor(p*len(sort_y))\n      y = sort_y[to_remove:(len(sort_y)-to_remove)] #replace y with the modified version\n    return sum(y)/len(y)\n\n\nfind_mean(y, \"Trim\", p = 0.1)\n\n-0.009797451217442077\n\n\n\nfind_mean(y = y, method = \"Trim\", p = 0.1) #this won't work!\n\nTypeError: find_mean() got some positional-only arguments passed as keyword arguments: 'y'"
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#write-our-own-correlation-function",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#write-our-own-correlation-function",
    "title": "User Defined Functions",
    "section": "Write Our Own Correlation Function",
    "text": "Write Our Own Correlation Function\nJust to demonstrate something more complicated, let’s write our own function to compute the (usual) sample correlation between two variables, call them x and y.\n\nPearson’s correlation:\n\n\\[r = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\]\nwhere - \\((x_i, y_i)\\) are numeric variables observed on the same \\(n\\) units, \\(i=1,...,n\\)\n\nPlan\nFunction inputs: - \\(x\\), \\(y\\), lists with numeric entries only\nFunction body: - Find sample means for \\(x\\) and \\(y\\) - Compute numerator sum and denominator sums - Find quotient and return that value\n\nFinding Means\nLet’s create some example data. \\(x\\) and \\(y\\) won’t be related here so the sample correlation shoudl be near 0!\n\nx = list(range(1,51))\nprint(x[1:10])\nxbar = find_mean(x)\nxbar\n\n[2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n25.5\n\n\n\n#use same y as before\ny = list(np.random.default_rng(1).standard_normal(50))\nprint(y[1:10])\nybar = find_mean(y)\nybar\n\n[0.8216181435011584, 0.33043707618338714, -1.303157231604361, 0.9053558666731177, 0.4463745723640113, -0.5369532353602852, 0.5811181041963531, 0.36457239618607573, 0.294132496655526]\n\n\n-0.03607807742830817\n\n\nAgiain, these two vectors are not related and should have a near 0 correlation!\nNext, we need to find the numerator and denominator sums. Finding the sums will be easier once we learn arrays, but for now we’ll peak at a for loop and the zip() function.\nLet’s start with computation of \\[\\sum_{i=1}^n(x_i-\\bar{x})^2\\]\n\n#computation in one of our sums (we want this across all 50 values, then added up)\n(x[0]-xbar)**2\n\n600.25\n\n\nSo really we want to find all of these values:\n(x[0]-xbar)**2\n(x[1]-xbar)**2\n...\n(x[49]-xbar)**2\nWe can use for to iterate over the values of 0, 1, …, 49. Similar to function definitions and if statements, we just tab in (four spaces) the code to be executed at each iteration of the for loop.\n\n#initialize a value to store the sum in\nden_x = 0\n#use a for loop to iterate across values (studies more later!)\nfor i in x:\n    den_x += (i-xbar)**2\nden_x\n\n10412.5\n\n\nWe can very easily get a similar computation for \\(y\\)’s portion of the denominator.\nTo get the numerator, that’s a bit more work. We really need to find\n(x[0]-xbar)(y[0]-ybar)\n(x[1]-xbar)(y[1]-ybar)\n...\n(x[49]-xbar)(y[49]-ybar)\nWe can zip() the \\(x\\) and \\(y\\) lists together. This essentially just pairs the 0th elements, the 1st elements, etc. Then we can iterate over the values together.\n\nnum = 0\nfor i, j in zip(x, y): #i corresponds to the x elements and j the y elements\n    num += (i-xbar)*(j-ybar)\nnum\n\n-51.69981003655184\n\n\nOk, now we are ready to put these together and calculate our correlation!\n\ndef find_corr(x, y):\n    \"\"\"\n    Compute Pearson's Correlation Coefficient\n    x and y are assumed to be lists with numeric values\n    Data is assumed to have no missing values\n    \"\"\"\n    xbar = find_mean(x)\n    ybar = find_mean(y)\n    num = 0\n    den_x = 0\n    den_y = 0\n    for i, j in zip(x, y):\n        num +=(i-xbar)*(j-ybar)\n        den_x +=(i-xbar)**2\n        den_y +=(j-ybar)**2\n    return num/sqrt(den_x*den_y)\n\nLet’s test our function on our data!\n\nfind_corr(x, y) #near 0!\n\n-0.0813179110596017\n\n\nNote that all functions with a doc string have a .__doc__ attribute that you can look at to understand that function (assuming the doc string is useful!).\n\nprint(find_corr.__doc__)\n\n\n    Compute Pearson's Correlation Coefficient\n    x and y are assumed to be lists with numeric values\n    Data is assumed to have no missing values\n    \n\n\n\nprint(len.__doc__) #another example on a common function\n\nReturn the number of items in a container.\n\n\n\nprint(np.random.default_rng.__doc__) #another example\n\ndefault_rng(seed=None)\nConstruct a new Generator with the default BitGenerator (PCG64).\n\n    Parameters\n    ----------\n    seed : {None, int, array_like[ints], SeedSequence, BitGenerator, Generator}, optional\n        A seed to initialize the `BitGenerator`. If None, then fresh,\n        unpredictable entropy will be pulled from the OS. If an ``int`` or\n        ``array_like[ints]`` is passed, then it will be passed to\n        `SeedSequence` to derive the initial `BitGenerator` state. One may also\n        pass in a `SeedSequence` instance.\n        Additionally, when passed a `BitGenerator`, it will be wrapped by\n        `Generator`. If passed a `Generator`, it will be returned unaltered.\n\n    Returns\n    -------\n    Generator\n        The initialized generator object.\n\n    Notes\n    -----\n    If ``seed`` is not a `BitGenerator` or a `Generator`, a new `BitGenerator`\n    is instantiated. This function does not manage a default global instance.\n\n    See :ref:`seeding_and_entropy` for more information about seeding.\n    \n    Examples\n    --------\n    ``default_rng`` is the recommended constructor for the random number class\n    ``Generator``. Here are several ways we can construct a random \n    number generator using ``default_rng`` and the ``Generator`` class. \n    \n    Here we use ``default_rng`` to generate a random float:\n \n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; rng = np.random.default_rng(12345)\n    &gt;&gt;&gt; print(rng)\n    Generator(PCG64)\n    &gt;&gt;&gt; rfloat = rng.random()\n    &gt;&gt;&gt; rfloat\n    0.22733602246716966\n    &gt;&gt;&gt; type(rfloat)\n    &lt;class 'float'&gt;\n     \n    Here we use ``default_rng`` to generate 3 random integers between 0 \n    (inclusive) and 10 (exclusive):\n        \n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; rng = np.random.default_rng(12345)\n    &gt;&gt;&gt; rints = rng.integers(low=0, high=10, size=3)\n    &gt;&gt;&gt; rints\n    array([6, 2, 7])\n    &gt;&gt;&gt; type(rints[0])\n    &lt;class 'numpy.int64'&gt;\n    \n    Here we specify a seed so that we have reproducible results:\n    \n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n    &gt;&gt;&gt; print(rng)\n    Generator(PCG64)\n    &gt;&gt;&gt; arr1 = rng.random((3, 3))\n    &gt;&gt;&gt; arr1\n    array([[0.77395605, 0.43887844, 0.85859792],\n           [0.69736803, 0.09417735, 0.97562235],\n           [0.7611397 , 0.78606431, 0.12811363]])\n\n    If we exit and restart our Python interpreter, we'll see that we\n    generate the same random numbers again:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n    &gt;&gt;&gt; arr2 = rng.random((3, 3))\n    &gt;&gt;&gt; arr2\n    array([[0.77395605, 0.43887844, 0.85859792],\n           [0.69736803, 0.09417735, 0.97562235],\n           [0.7611397 , 0.78606431, 0.12811363]])\n\n    \n\n\nAttributes are another important thing we’ll learn about, especially when we get into pyspark. We now have\n\nfunctions() which go prior to the object\n.methods() that go on the end of the object\n\nand\n\n.attributes that also go on the end of an object just with no ()."
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#other-things-to-note",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#other-things-to-note",
    "title": "User Defined Functions",
    "section": "Other Things to Note",
    "text": "Other Things to Note\n\nWhen executing a function, a new symbol table is used for the local variables\nThis keeps us from accidentally overwriting something\n\n\nimport numpy as np\ny = np.array(range(1,11))\n\ndef square(z):\n    y = z**2\n    print(\"In the function environment, z = \" + str(z) + \" and y = \" + str(y))\n    return(y)\n\nprint(square(y))\nprint(y) #y is not changed\n\nIn the function environment, z = [ 1  2  3  4  5  6  7  8  9 10] and y = [  1   4   9  16  25  36  49  64  81 100]\n[  1   4   9  16  25  36  49  64  81 100]\n[ 1  2  3  4  5  6  7  8  9 10]\n\n\n\nprint(z) #z isn't defined outside the function call! error\n\nNameError: name 'z' is not defined\n\n\n\nYou can define global variables from within a function using global\n\n\ndef square(z):\n    global y\n    y = z**2\n    print(\"In the function environment, z = \" + str(z) + \" and y = \" + str(y))\n    return(y)\n\nprint(square(y))\nprint(y) #modified globally now\n\nIn the function environment, z = [ 1  2  3  4  5  6  7  8  9 10] and y = [  1   4   9  16  25  36  49  64  81 100]\n[  1   4   9  16  25  36  49  64  81 100]\n[  1   4   9  16  25  36  49  64  81 100]\n\n\n\nIf nothing is returned from a function (with return) then it actually returns the special None\n\n\ndef square_it(a):\n    if (type(a) == int) or (type(a) == float):\n      return a**2\n    else:\n      return\n\nprint(square_it(10))\nprint(square_it(10.5))\nprint(square_it(\"10\"))\n\n100\n110.25\nNone\n\n\n\nDefault values are only evaluated once - at the time of the function definition\nMutable objects can cause an issue! (Lists are mutable as they can be changed, some objects, like tuples, are immutable and can’t be modified.)\n\n\n#append a value to a list but give a default empty list if not given\ndef my_append(value, L = []):\n    L.append(value)\n    return L\n\n#correctly appends \"A\" to the list\nprint(my_append(\"A\"))\n#appends \"B\" to the previous list as L = [] was only evaluated at the time the function was created!\nprint(my_append(\"B\"))\n\n['A']\n['A', 'B']\n\n\n\nTo avoid this behavior, instead define the default value as None and take care of things within the function body\n\n\ndef my_append(value, L = None):\n    if L is None:\n        L = []\n    L.append(value)\n    return L\n\nprint(my_append(\"A\"))\nprint(my_append(\"B\"))\n\n['A']\n['B']"
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#video-demo",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#video-demo",
    "title": "User Defined Functions",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video demo gives another example of creating our own function! Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=ae1858b3-74cf-4065-8ec7-b0f800e4f827&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html",
    "href": "01_Programming_in_python/10-Lists_Tuples.html",
    "title": "Lists and Tuples",
    "section": "",
    "text": "Now we’ll look a bit more at lists and introduce tuples - basically an immutable list.\nRecall our plan as we go through our common data types: - Learn how to create - Consider commonly used functions and methods - See control flow and other tricks along the way\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#constructing-a-list",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#constructing-a-list",
    "title": "Lists and Tuples",
    "section": "Constructing a List",
    "text": "Constructing a List\nEarlier we saw there were four ways to construct lists (although we didn’t go through them all) - [element1, element2] - list((element1, element2, ...)) - an empty list and use the .append() method to add elements - list comprehensions\n\n\nConstructing a list from and empty list\n\nCreate an empty list and use the append method to add elements\n\n\nmylist = []\n# or\nmylist = list()\n\n\nAdd elements with .append()\n\n\nmylist.append(\"Dog\")\nmylist.append(\"Cat\")\nmylist\n\n['Dog', 'Cat']\n\n\n\n\n\nConstructing a List using []\n\nCreate an empty list and use the append method to add elements\nOften used with a for loop\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nmylist = []\n\nfor x in animals:\n    if \"o\" in x:\n        mylist.append(x)\n\nmylist\n\n['Dog', 'Horse', 'Frog', 'Cow', 'Buffalo', 'Fox', 'Racoon']\n\n\n\n\n\nConstructing a list using list comprehensions\n\nRather than write the loop out, you can use list comprehensions (shorthand!)\nThe general syntax for list comprehensions is:\n[expression for member in iterable]\nLet’s do a quick example. First, the for loop way:\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nmylist = []\nfor x in animals:\n    mylist.append(x)\n\n\nNow, we can do the same thing with shorthand!\n\n\nmylist = [x for x in animals] #for x in animals, return x (essentially)\nmylist\n\n['Dog',\n 'Cat',\n 'Horse',\n 'Frog',\n 'Cow',\n 'Buffalo',\n 'Deer',\n 'Fish',\n 'Bird',\n 'Fox',\n 'Racoon']\n\n\n\nYou can do more complicated things with list comprehensions as well. For instance, we can include condition logic.\n[expression for member in iterable (if conditional)]\nFirst the for loop way:\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nfor x in animals:\n    if \"o\" in x:\n        mylist.append(x)\n\n\nNow using a list comprehension:\n\n\nmylist = [x for x in animals if \"o\" in x]\nmylist\n\n['Dog', 'Horse', 'Frog', 'Cow', 'Buffalo', 'Fox', 'Racoon']\n\n\n\nWe can also modify the thing that gets put into the loop. Check out this example where we upper case the string.\nFirst the for loop way:\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nmylist = []\n\nfor x in animals:\n    if \"o\" in x:\n        mylist.append(x.upper()) #upper case prior to appending\n\nmylist\n\n['DOG', 'HORSE', 'FROG', 'COW', 'BUFFALO', 'FOX', 'RACOON']\n\n\n\nNow using a list comprehension:\n\n\nmylist = [x.upper() for x in animals if \"o\" in x]\nmylist\n\n['DOG', 'HORSE', 'FROG', 'COW', 'BUFFALO', 'FOX', 'RACOON']"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#reminder-about-strings",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#reminder-about-strings",
    "title": "Lists and Tuples",
    "section": "Reminder About Strings",
    "text": "Reminder About Strings\n\nStrings are a sequence type object (so you can iterate over them naturally)\n\n\nmylist = []\nfor x in \"Man do I love learning all this python!\":\n    if x in \"aeiou\":\n        mylist.append(x)\nmylist\n\n['a', 'o', 'o', 'e', 'e', 'a', 'i', 'a', 'i', 'o']\n\n\n\nThat means we can do something like the for loop above using list comprehensions!\n\n\nmylist = [x for x in \"Man do I love learning all this python!\" if x in \"aeiou\"]\nmylist\n\n['a', 'o', 'o', 'e', 'e', 'a', 'i', 'a', 'i', 'o']"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#list-operations-indexing-slicing",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#list-operations-indexing-slicing",
    "title": "Lists and Tuples",
    "section": "List Operations (Indexing & Slicing)",
    "text": "List Operations (Indexing & Slicing)\nRecall:\n\nIndex with a [] (just like strings)\nCounting starts at 0\n\n\nx = [10, 15, 10, 100, \"Help!\"]\nx[0]\n\n10\n\n\n\nx[1]\n\n15\n\n\n\nx[-1]\n\n'Help!'\n\n\n\nMultiple elements at once with :\nRemember the last number isn’t included and the counting starts at 0\n\n:2 is really giving you 0 and 1\n\n\n\nx[:2]\n\n[10, 15]\n\n\n\nx[1:]\n\n[15, 10, 100, 'Help!']\n\n\n\nx[1:3]\n\n[15, 10]\n\n\n\nx[1:4:2]\n\n[15, 100]"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#lists-are-mutable",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#lists-are-mutable",
    "title": "Lists and Tuples",
    "section": "Lists are Mutable",
    "text": "Lists are Mutable\n\nThat is, we can replace or change elements of a list\n\n\nx = [10, 15, 10, 100, \"Help!\"]\nx[0] = 11\nx\n\n[11, 15, 10, 100, 'Help!']\n\n\n\nx[1] = [\"hi\", \"ho\"]\nx\n\n[11, ['hi', 'ho'], 10, 100, 'Help!']\n\n\n\nx[1:3] = [1, 2]\nx\n\n[11, 1, 2, 100, 'Help!']"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#list-methods",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#list-methods",
    "title": "Lists and Tuples",
    "section": "List Methods",
    "text": "List Methods\nMany useful methods to modify lists:\n\nmylist.append(object_to_add)\n\n\nx = [x for x in range(1,10)]\ny = [y for y in \"abcde\"]\nx.append(y) #modifies x\nx\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, ['a', 'b', 'c', 'd', 'e']]\n\n\n\nmylist.extend(object_to_add)\n\n\nx = [x for x in range(1,10)]\ny = [y for y in \"abcde\"]\nx.extend(y) #modifies x and iterates over list elements rather than appending a list into x\nx\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 'a', 'b', 'c', 'd', 'e']\n\n\n\nUsing + is similar but doesn’t overwrite x\n\n\nx + y\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 'a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e']\n\n\n\nmylist.insert(index, object_to_add)\n\n\ny = [y for y in \"abcde\"]\ny.insert(2, 30) #modifies y\ny\n\n['a', 'b', 30, 'c', 'd', 'e']\n\n\n\nmylist.remove(element_to_remove)\n\n\ny.remove(\"d\") #modifies y\ny\n\n['a', 'b', 30, 'c', 'e']\n\n\n\nmylist.count(value)\n\n\nw = [x for x in range(0, 4)] * 4\nprint(w)\nw.count(1) #count the number of times 1 occurs\n\n[0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n\n\n4\n\n\n\nmylist.index(value)\n\n\nv = [y for y in \"abcde\"]\nv.extend([\"z\", \"y\"] * 3)\nprint(v)\nprint(v.index(\"y\")) #get the index corresponding to the first 'y'\nprint(v.index(\"y\", v.index(\"y\") + 1)) #index corresponding to second 'y'\n\n['a', 'b', 'c', 'd', 'e', 'z', 'y', 'z', 'y', 'z', 'y']\n6\n8\n\n\n\n\nList Packing & Unpacking\n\nWe can pack a list. That is, put a bunch of values in a list in one line of code.\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nshort_animals = animals[:3]\n\n#pack the values first, second, and third\nfirst, second, third = short_animals\nprint(first + \" \" + second + \" \" + third)\n\nDog Cat Horse\n\n\n\nWe can also pack leftover elements into a list using *name\n\n\nfirst, second, third, *other = animals\nprint(first + \" \" + second + \" \" + third)\nprint(other)\n\nDog Cat Horse\n['Frog', 'Cow', 'Buffalo', 'Deer', 'Fish', 'Bird', 'Fox', 'Racoon']\n\n\n\nYou can pack in different orders as well!\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\n*other, second_last, last = animals\nprint(other)\nprint(second_last + \" \" + last)\n\n['Dog', 'Cat', 'Horse', 'Frog', 'Cow', 'Buffalo', 'Deer', 'Fish', 'Bird']\nFox Racoon\n\n\n\nIf we want to ignore some of the values we can use our _ temporary variable with packing and *:\n\n\nfirst, *_, last = animals\nprint(first + \" \" + last)\n\nDog Racoon\n\n\n\nLater we’ll look at unpacking a list when calling functions\n\n\ndef my_fun(a, b, c):\n    print(a, b, c)\n\nfav_animals = [\"cat\", \"dog\", \"cow\"]\nmy_fun(*fav_animals)\n\ncat dog cow"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#constructing-tuples",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#constructing-tuples",
    "title": "Lists and Tuples",
    "section": "Constructing Tuples",
    "text": "Constructing Tuples\n\nWe create by separating elements with a ,, ( ), or tuple(())\n\n\ntup1 = 3, 10, \"word\", True\ntup1\n\n(3, 10, 'word', True)\n\n\n\ntup2 = (1, 2, \"word\", False)\ntup2\n\n(1, 2, 'word', False)\n\n\n\ntup3 = tuple((tup1, tup2))\ntup3\n\n((3, 10, 'word', True), (1, 2, 'word', False))\n\n\n\ntup4 = tup1 + tup2 #like other sequence type objects we can concatenate them with +\ntup4\n\n(3, 10, 'word', True, 1, 2, 'word', False)\n\n\n\ntup5 = (1, [1, 3])\ntup5\n\n(1, [1, 3])\n\n\n\ntup5 * 3\n\n(1, [1, 3], 1, [1, 3], 1, [1, 3])\n\n\nOne interesting thing is that although we can’t modify the tuple, we can modify mutable elements within the tuple!\n\ntup5[1][1] = 5\n\n\ntup5\n\n(1, [1, 5])"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#constructing-tuples-from-list-comprehensions",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#constructing-tuples-from-list-comprehensions",
    "title": "Lists and Tuples",
    "section": "Constructing Tuples from list comprehensions",
    "text": "Constructing Tuples from list comprehensions\n\nTo populate a tuple we can wrap a list comprehension with tuple()\n\n\ny = [x for x in range(1, 10)]\ny = tuple(y)\ny\n\n(1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n\n\nCan sort of edit a tuple by switching it between a list and a tuple… but this isn’t really editing it!\n\n\ny = list(y)\ny.append(\"new element\")\ny = tuple(y)\ny\n\n(1, 2, 3, 4, 5, 6, 7, 8, 9, 'new element')"
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#tuple-operations-methods",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#tuple-operations-methods",
    "title": "Lists and Tuples",
    "section": "Tuple Operations & Methods",
    "text": "Tuple Operations & Methods\nAs with strings and lists:\n\nWe can index and slice using [:]\nConcatenate with + and *\nSome similar functions like len() and count()\nSome similar methods like .index() and .count()\nCan do packing and unpacking"
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html",
    "href": "01_Programming_in_python/12-Numpy.html",
    "title": "NumPy",
    "section": "",
    "text": "One of the most famous modules for statistics is called numpy\nimport numpy as np\nAs with our other data types, let’s go through and…\nThis topic, compound objects: - numpy array\nRecall: functions & methods act on objects. We’ll see how to obtain attributes here as well!\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html#creating-an-array",
    "href": "01_Programming_in_python/12-Numpy.html#creating-an-array",
    "title": "NumPy",
    "section": "Creating an Array",
    "text": "Creating an Array\n\nArrays are like lists but process much faster\nThey also require that the data be of the same type\nThey can be multidimensional (like a matrix or even higher dimension\n\nThe picture below from https://predictivehacks.com/tips-about-numpy-arrays/ shows a 1D, 2D, and 3D array visually.\n\n\nTo create an ndarray object, pass a list, tuple, or any array-like object to np.array()\n\n\na = np.array(1)\na\n\narray(1)\n\n\n\ntype(a)\n\nnumpy.ndarray\n\n\n\nndarrays have a shape attribute\nAttributes can be accessed like methods except we don’t use () at the end\nWe did this with the .__doc__ attribute on functions\n\n\na.shape\n\n()\n\n\n\nb = np.array([1, 2, 3])\nprint(b)\nprint(type(b))\nprint(b.shape)\n\n[1 2 3]\n&lt;class 'numpy.ndarray'&gt;\n(3,)\n\n\n\n\nArray Dimension\n\n0D arrays are a scalar (sort of… see here for discussion)\n1D arrays are vectors\n2D arrays are matrices\n3D and up are just called arrays\n.shape attribute returns the dimensions of an array as a tuple\n\n\nc = np.array([1, \"a\", True])\nprint(c)\nc.shape\n\n['1' 'a' 'True']\n\n\n(3,)\n\n\n\nd = np.array([\n  [1, 2, 3],\n  [4, 5, 6]]\n  )\nprint(d)\nd.shape\n\n[[1 2 3]\n [4 5 6]]\n\n\n(2, 3)\n\n\n\n\nFunctions for Fillling/Creating Arrays\nCreating a vector or matrix of all zeros\n\nRow vector\n\n\nA0 = np.zeros(4) #row vector of length 4\nA0\n\narray([0., 0., 0., 0.])\n\n\n\nColumn vector\n\n\nA0 = np.zeros((4,1)) #column vector of length 4\nA0\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\n\nMatrix of zeros\n\n\nA = np.zeros((4,2)) #matrix with dimension 4, 2, given as a tuple\nA\n\narray([[0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.]])\n\n\n\nA.shape\n\n(4, 2)\n\n\n\nRow of all ones\n\n\nb = np.ones(4) #row vector\nb\n\narray([1., 1., 1., 1.])\n\n\n\nMatrix of all ones\n\n\nB = np.ones((2,3))\nB\n\narray([[1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\nMatrix of 10’s\n\n\nC = np.ones((2, 3)) * 10\nC\n\narray([[10., 10., 10.],\n       [10., 10., 10.]])\n\n\n\nnp.full() does this automatically\n\n\nC = np.full((2,3), 10) #specify the value to fill with after the tuple giving dimension\nC\n\narray([[10, 10, 10],\n       [10, 10, 10]])\n\n\n\nBe careful! C is an integer valued array\n\n\nC = np.full((2,3), 10)\nC[0,0] = 6.5                 #replace the top left element\nC\n\narray([[ 6, 10, 10],\n       [10, 10, 10]])\n\n\n\nAvoid by creating the matrix with a float instead\n\n\nC = np.full((2,3), 10.0)  #or C = np.ones((2, 3)) * 10.0\nC[0,0] = 6.5\nC\n\narray([[ 6.5, 10. , 10. ],\n       [10. , 10. , 10. ]])\n\n\n\nCreate an identity matrix with np.eye() (this has 1’s on the diagonal and 0’s elsewhere)\n\n\nD = np.eye(3)\nD\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nCreate a random matrix (values between 0 and 1) with np.random.random()\n\n\nE = np.random.random((3,5))\nE\n\narray([[0.82782158, 0.92669984, 0.28811706, 0.8048095 , 0.31863604],\n       [0.43125583, 0.95565594, 0.81946103, 0.96181153, 0.10190225],\n       [0.92238437, 0.66130983, 0.8828503 , 0.06677584, 0.78615673]])\n\n\n\nMany more ways to create!"
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html#reshaping-an-array",
    "href": "01_Programming_in_python/12-Numpy.html#reshaping-an-array",
    "title": "NumPy",
    "section": "Reshaping an Array",
    "text": "Reshaping an Array\n\nReshape an array with the .reshape() method\nChanges the dimension in some way\nWe’ll need to do this type of thing when fitting models!\n\n\nF = np.random.random((10,1))\nF\n\narray([[0.38620732],\n       [0.02246848],\n       [0.75057807],\n       [0.64596504],\n       [0.9782189 ],\n       [0.3074028 ],\n       [0.20987403],\n       [0.73177229],\n       [0.8167644 ],\n       [0.03675048]])\n\n\n\nF.shape\n\n(10, 1)\n\n\n\nG = F.reshape(1, -1) #-1 flattens to a 1D array\nG\n\narray([[0.38620732, 0.02246848, 0.75057807, 0.64596504, 0.9782189 ,\n        0.3074028 , 0.20987403, 0.73177229, 0.8167644 , 0.03675048]])\n\n\n\nG.shape\n\n(1, 10)\n\n\n\nG = F.reshape(2, 5)\nG\n\narray([[0.38620732, 0.02246848, 0.75057807, 0.64596504, 0.9782189 ],\n       [0.3074028 , 0.20987403, 0.73177229, 0.8167644 , 0.03675048]])\n\n\n\nCareful! G is actually a view of the original array\nView means that we haven’t created a new array, just a different way of viewing the values (essentially). The data is still stored in the same memory\n.base attribute will tell you whether you are referencing another array\n\n\nG.base\n\narray([[0.38620732],\n       [0.02246848],\n       [0.75057807],\n       [0.64596504],\n       [0.9782189 ],\n       [0.3074028 ],\n       [0.20987403],\n       [0.73177229],\n       [0.8167644 ],\n       [0.03675048]])\n\n\n\nG.base is None #a way to return a bool based on whether it is a view or not\n\nFalse\n\n\n\n\nCopying an Array\n\nTo avoid getting a view, copy the array with .copy() method\n\n\nH = F.reshape(2, 5).copy()\nH.base is None\n\nTrue\n\n\n\nH.base"
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html#indexing-an-array",
    "href": "01_Programming_in_python/12-Numpy.html#indexing-an-array",
    "title": "NumPy",
    "section": "Indexing an Array",
    "text": "Indexing an Array\n\nAccess in the same was as lists []\nWith multiple dimensions, separate the indices you want with a ,\n\n\nb = np.array([1, 2, 3]) #row vector\nb\n\narray([1, 2, 3])\n\n\n\nprint(b[0], b[1], b[2])\n\n1 2 3\n\n\n\nb[0] = 5 #overwrite the 0 element\nb\n\narray([5, 2, 3])\n\n\n\nDepending on the dimensions, you add the required commas\nHere we have a 3D array so we have three slots\nNotation: array[1stD, 2ndD, 3rdD]\n\n\nE = np.random.random((3, 2, 2))\nE\n\narray([[[0.44271423, 0.36194369],\n        [0.67811074, 0.36893479]],\n\n       [[0.18957687, 0.89085357],\n        [0.40869827, 0.1685411 ]],\n\n       [[0.28849053, 0.65884175],\n        [0.71058619, 0.41460453]]])\n\n\n\nE[0, 0, 0]\n\n0.4427142296735752\n\n\n\nE[0, 1, 0]\n\n0.6781107434665593\n\n\n\nE[1, 0, 1]\n\n0.8908535743830176\n\n\n\n\nSlicing an Array\n\nRecall [start:end] for slicing sequence type objects. We can do that with arrays as well\n\nReturns everything from start up to and excluding end\nLeaving start blank implies a 0\nLeaving end blank returns everything from start through the end of the array\n\n\n\nA = np.array([\n  [1,2,3,4],\n  [5,6,7,8],\n  [9,10,11,12]])\nA\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\n\nB = A[:2, 1:3]\nB\n\narray([[2, 3],\n       [6, 7]])\n\n\n\nCareful with modifying! We have a view here so the values in both A and B are referencing the same computer memory\nChanging an element of B changes A!\n\n\nB[0, 0] = 919\nA\n\narray([[  1, 919,   3,   4],\n       [  5,   6,   7,   8],\n       [  9,  10,  11,  12]])\n\n\n\nReturning All of One Index\nUse a : with nothing else\n\n\nA = np.array([\n  [1,2,3,4],\n  [5,6,7,8],\n  [9,10,11,12]])\nA1 = A[1, :]\nA1\n\narray([5, 6, 7, 8])\n\n\n\nA1.shape\n\n(4,)\n\n\n\nA2 = A[1:3, :]\nA2\n\narray([[ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\n\nA2.shape\n\n(2, 4)"
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html#operations-on-arrays",
    "href": "01_Programming_in_python/12-Numpy.html#operations-on-arrays",
    "title": "NumPy",
    "section": "Operations on Arrays",
    "text": "Operations on Arrays\n\nWe saw that multiplying by a constant was performed elementwise\n\nAll basic functions act elementwise\n\n\nx = np.array([\n  [1,2],\n  [3,4]])\ny = np.array([\n  [5,6],\n  [7,8]])\n\nx\n\narray([[1, 2],\n       [3, 4]])\n\n\n\ny\n\narray([[5, 6],\n       [7, 8]])\n\n\n\nx + 10\n\narray([[11, 12],\n       [13, 14]])\n\n\n\nLots of methods exist such as the .add() method for adding arrays elementwise\n\n\nnp.add(x, y)\n\narray([[ 6,  8],\n       [10, 12]])\n\n\n\nIf we just do something like x * y we get elementwise multiplication\n\n\nx * y\n\narray([[ 5, 12],\n       [21, 32]])\n\n\n\nThe .multiply() method does elementwise multiplication too\nCan also add in conditions on when to multiply though!\n\nwhere = argument gives the condition on when to do the multiplication\nout = tells it which values to use if you don’t do the multiplication\n\n\n\nnp.multiply(x, y, where = (x &gt;= 3), out = x)\n\narray([[ 1,  2],\n       [21, 32]])\n\n\n\nElementwise division\n\n\nx / y\n\narray([[0.2       , 0.33333333],\n       [3.        , 4.        ]])\n\n\n\nWe can do matrix multiplication (if you are familiar with that) using the .matmul() method\n\n\nnp.matmul(x, y)\n\narray([[ 19,  22],\n       [329, 382]])\n\n\n\nsqrt() function can be used to find the square roots of the elements of a matrix\n\n\nnp.sqrt(x)\n\narray([[1.        , 1.41421356],\n       [4.58257569, 5.65685425]])\n\n\n\nnp.linalg.inv() will provide the inverse of a square matrix (if you’re familiar with that type of thing!)\n\n\nnp.linalg.inv(x)\n\narray([[-3.2,  0.2],\n       [ 2.1, -0.1]])\n\n\n\nComputations on Arrays\n\nNumPy has some useful functions for performing basic computations on arrays\n\n\nx = np.array([\n  [1,2,10],\n  [3,4,11]])\nnp.sum(x)\n\n31\n\n\n\nColumn-wise and row-wise sums\n\n\nx.shape\n\n(2, 3)\n\n\n\nnp.sum(x, axis=0)\n\narray([ 4,  6, 21])\n\n\n\nnp.sum(x, axis=1)\n\narray([13, 18])\n\n\n\nCombine arrays (appropriately sized)\n\n\nx = np.array([\n  [1,2],\n  [3,4]])\ny = np.array([\n  [5,6],\n  [7,8]])\n\nnp.hstack((x, y))\n\narray([[1, 2, 5, 6],\n       [3, 4, 7, 8]])\n\n\n\nnp.vstack((x, y))\n\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]])\n\n\n\nLots of other operations!"
  },
  {
    "objectID": "01_Programming_in_python/14-Pandas_Series.html",
    "href": "01_Programming_in_python/14-Pandas_Series.html",
    "title": "Pandas Series",
    "section": "",
    "text": "&gt; Image from https://www.altexsoft.com/blog/pandas-library/\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/14-Pandas_Series.html#creating-a-pandas-series",
    "href": "01_Programming_in_python/14-Pandas_Series.html#creating-a-pandas-series",
    "title": "Pandas Series",
    "section": "Creating a pandas Series",
    "text": "Creating a pandas Series\n\nCreate a series using the pd.Series() function\n\n\nimport numpy as np\nimport pandas as pd\nrng = np.random.default_rng(2) #set a seed\ns = pd.Series(rng.normal(size = 10, loc = 2, scale = 4)) #mean of 2 and std of 4\ns\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n2.756214\n\n\n1\n-0.090994\n\n\n2\n0.347746\n\n\n3\n-7.765870\n\n\n4\n9.198830\n\n\n5\n6.576663\n\n\n6\n0.698309\n\n\n7\n5.095226\n\n\n8\n3.124843\n\n\n9\n-0.215291\n\n\n\n\ndtype: float64"
  },
  {
    "objectID": "01_Programming_in_python/14-Pandas_Series.html#indexing-a-series",
    "href": "01_Programming_in_python/14-Pandas_Series.html#indexing-a-series",
    "title": "Pandas Series",
    "section": "Indexing a Series",
    "text": "Indexing a Series\n\nLike lists, the ordering starts at 0\nLike numpy arrays, all elements in a Series must be of the same type\nUnlike numpy arrays, Series can be indexed by an index attribute (not just the numeric index)\n.index attribute returns just these indices\n\n\ns.index\n\nRangeIndex(start=0, stop=10, step=1)\n\n\n\ns[0] #is both the numeric index and the value of an index here\n\n2.756213527174132\n\n\n\ns2 = pd.Series(rng.normal(size = 10, loc = 2, scale = 4),\n               index = [x for x in \"abcdefghij\"])\ns2\n\n\n\n\n\n\n\n\n0\n\n\n\n\na\n5.910270\n\n\nb\n0.757774\n\n\nc\n0.684704\n\n\nd\n-1.168587\n\n\ne\n3.819832\n\n\nf\n1.603208\n\n\ng\n4.181155\n\n\nh\n-0.428743\n\n\ni\n2.507311\n\n\nj\n-1.569096\n\n\n\n\ndtype: float64\n\n\n\ns2.index\n\nIndex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'], dtype='object')\n\n\nWe can access elements with the numeric index or the index value itself but this behavior will go away soon and the .iloc[] method should be used instead (we discuss the similar DataFrames .iloc[] method shortly).\n\ns2[2]\n\nFutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  s2[2]\n\n\n0.6847043837681492\n\n\n\ns2[\"c\"]\n\n0.6847043837681492\n\n\n\nWe can obtain just the values with of a Series using the .values attribute\n\n\ns.values\n\narray([ 2.75621353, -0.09099377,  0.34774583, -7.76586953,  9.19882953,\n        6.57666349,  0.69830865,  5.09522635,  3.12484268, -0.21529135])\n\n\n\ns2.values\n\narray([ 5.9102698 ,  0.75777381,  0.68470438, -1.16858702,  3.81983228,\n        1.60320779,  4.18115486, -0.4287428 ,  2.50731139, -1.56909617])\n\n\n\nNote that when you return the values you get back just a numpy array!\n\n\ntype(s2.values)\n\nnumpy.ndarray"
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html",
    "title": "Pandas for Reading Raw Data",
    "section": "",
    "text": "Justin Post\nimport pandas as pd\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#data-formats",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#data-formats",
    "title": "Pandas for Reading Raw Data",
    "section": "Data Formats",
    "text": "Data Formats\nRaw data comes in many different formats. Understanding the raw data format is essential for reading that data into python. Some raw data types include:\n\n‘Delimited’ data: Character (such as ‘,’ , ‘&gt;’, or [’ ’]) separated data\nFixed field data\nExcel data\nFrom other statistical software, Ex: SPSS formatted data or SAS data sets\nFrom an Application Programming Interface (API) (often returned as a JSON file - key/value pairs, similar to a dictionary)\nFrom a database"
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#delimited-data",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#delimited-data",
    "title": "Pandas for Reading Raw Data",
    "section": "Delimited Data",
    "text": "Delimited Data\nLet’s start with delimited data.\n\nOne common format for raw data is delimited data\n\nData that has a character or characters that separates the data values\nCharacter(s) is (are) called delimiter(s)\n\nUsing pandas the read_csv() function can read in this kind of data (although csv stands for ‘comma separated value’, this function is used for reading most delimited data via pandas)\n\nIf the raw data is well-formatted, we just need to tell python where to find it!"
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#locating-a-file",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#locating-a-file",
    "title": "Pandas for Reading Raw Data",
    "section": "Locating a File",
    "text": "Locating a File\n\nHow does python locate the file?\n\nNot in colab\n\nYou can give file full path name\n\nex: ‘S:/Documents/repos/ST-554/datasets/data.csv’\n\nex: ‘S:\\Documents\\repos\\ST-554\\datasets\\data.csv’\n\n\n\nOr use local paths!\n\nDetermine your working directory\nUse a path relative to that\nIf your working directory is ‘S:/Documents/repos/ST-554’ you can get to ‘data.csv’ via ‘datasets/data.csv’\n\nThe os module gives you access to function for finding and setting your working directory\n\nUsing a cloud-based platform complicates things a bit - In colab you can + Mount your google drive + Read files from URLs + Upload files via the menu on the left (folder icon, then upload a file via the icons there)\n\nimport os\n#getcwd() stands for get current working directory\nos.getcwd() #shows the directory you can get to via the folder icon on the left\n#chdir() stands for change current directory\n#os.chdir(\"S:/Documents/repos/ST-554\") #won't work in colab but would work on a local python session\n\n'/content'\n\n\nThis /content refers to the main folder on the left hand side of Colab!"
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-files-locally-in-colab",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-files-locally-in-colab",
    "title": "Pandas for Reading Raw Data",
    "section": "Reading Files ‘Locally’ in Colab",
    "text": "Reading Files ‘Locally’ in Colab\n\nNicely formatted .csv files can be read in with the read_csv() function from pandas\nneuralgia.csv has been loaded into the folder on colab in my session. Therefore, it exists in my working directory. This won’t be the case for you unless you upload the data during your session! You can click on the folder icon on the left, then click the upload button to upload  this data set.\n\n\nneuralgia_data = pd.read_csv(\"neuralgia.csv\") #neuralgia.csv file was uploaded to colab for my session\nneuralgia_data.head() #this code block won't work unless you upload the data in your session\n\n\n  \n    \n\n\n\n\n\n\nTreatment\nSex\nAge\nDuration\nPain\n\n\n\n\n0\nP\nF\n68\n1\nNo\n\n\n1\nB\nM\n74\n16\nNo\n\n\n2\nP\nF\n67\n30\nNo\n\n\n3\nP\nM\n66\n26\nYes\n\n\n4\nB\nF\n67\n28\nNo\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nneuralgia_data.shape\n\n(60, 5)"
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-from-a-url",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-from-a-url",
    "title": "Pandas for Reading Raw Data",
    "section": "Reading From a URL",
    "text": "Reading From a URL\n\nNicely formatted .csv files can be read in with the read_csv() function from pandas\nscoresFull.csv file at a URL given by ‘https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv’\n\n\nscores_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\")\nscores_data.head()\n\n\n  \n    \n\n\n\n\n\n\nweek\ndate\nday\nseason\nawayTeam\nAQ1\nAQ2\nAQ3\nAQ4\nAOT\n...\nhomeFumLost\nhomeNumPen\nhomePenYds\nhome3rdConv\nhome3rdAtt\nhome4thConv\nhome4thAtt\nhomeTOP\nHminusAScore\nhomeSpread\n\n\n\n\n0\n1\n5-Sep\nThu\n2002\nSan Francisco 49ers\n3\n0\n7\n6\n-1\n...\n0\n10\n80\n4\n8\n0\n1\n32.47\n-3\n-4.0\n\n\n1\n1\n8-Sep\nSun\n2002\nMinnesota Vikings\n3\n17\n0\n3\n-1\n...\n1\n4\n33\n2\n6\n0\n0\n28.48\n4\n4.5\n\n\n2\n1\n8-Sep\nSun\n2002\nNew Orleans Saints\n6\n7\n7\n0\n6\n...\n0\n8\n85\n1\n6\n0\n1\n31.48\n-6\n6.0\n\n\n3\n1\n8-Sep\nSun\n2002\nNew York Jets\n0\n17\n3\n11\n6\n...\n1\n10\n82\n4\n8\n2\n2\n39.13\n-6\n-3.0\n\n\n4\n1\n8-Sep\nSun\n2002\nArizona Cardinals\n10\n3\n3\n7\n-1\n...\n0\n7\n56\n6\n10\n1\n2\n34.40\n8\n6.0\n\n\n\n\n\n5 rows × 82 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nscores_data.shape\n\n(3471, 82)\n\n\n\nOddly, to read other types of delimited data, we also use read_csv()!\n\nSpecify the sep = argument\n\nchemical.txt file (space delimiter) stored at “https://www4.stat.ncsu.edu/~online/datasets/chemical.txt”\n\n\nchem_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/chemical.txt\", sep=\" \")\nchem_data.head()\n\n\n  \n    \n\n\n\n\n\n\ntemp\nconc\ntime\npercent\n\n\n\n\n0\n-1.0\n-1.0\n-1.0\n45.9\n\n\n1\n1.0\n-1.0\n-1.0\n60.6\n\n\n2\n-1.0\n1.0\n-1.0\n57.5\n\n\n3\n1.0\n1.0\n-1.0\n58.6\n\n\n4\n-1.0\n-1.0\n1.0\n53.3\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ncrabs.txt file (tab delimiter) stored at “https://www4.stat.ncsu.edu/~online/datasets/crabs.txt”\n\nTab is \\t\n\n\n\ncrabs_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/crabs.txt\", sep=\"\\t\")\ncrabs_data.head()\n\n\n  \n    \n\n\n\n\n\n\ncolor\nspine\nwidth\nsatell\nweight\ny\n\n\n\n\n0\n3\n3\n28.3\n8\n3050\n1\n\n\n1\n4\n3\n22.5\n0\n1550\n0\n\n\n2\n2\n1\n26.0\n9\n2300\n1\n\n\n3\n4\n3\n24.8\n0\n2100\n0\n\n\n4\n4\n3\n26.0\n4\n2600\n1\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\numps2012.txt file (&gt; delimiter) stored at “https://www4.stat.ncsu.edu/~online/datasets/umps2012.txt”\n\nNo column names in raw file\nCan specify header = None and give column names when reading (via names = [list of names])\n\n\n\nump_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/umps2012.txt\",\n                      sep=\"&gt;\",\n                      header=None,\n                      names=[\"Year\", \"Month\", \"Day\", \"Home\", \"Away\", \"HPUmpire\"])\nump_data.head()\n\n\n  \n    \n\n\n\n\n\n\nYear\nMonth\nDay\nHome\nAway\nHPUmpire\n\n\n\n\n0\n2012\n4\n12\nMIN\nLAA\nD.J. Reyburn\n\n\n1\n2012\n4\n12\nSD\nARI\nMarty Foster\n\n\n2\n2012\n4\n12\nWSH\nCIN\nMike Everitt\n\n\n3\n2012\n4\n12\nPHI\nMIA\nJeff Nelson\n\n\n4\n2012\n4\n12\nCHC\nMIL\nFieldin Culbreth"
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-excel-data",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-excel-data",
    "title": "Pandas for Reading Raw Data",
    "section": "Reading Excel Data",
    "text": "Reading Excel Data\n\nUse the ExcelFile() function from pandas\ncensusEd.xlsx file located at “https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx”\n\n\ned_data = pd.ExcelFile(\"https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx\")\ned_data\n\n&lt;pandas.io.excel._base.ExcelFile at 0x7d6ef1627880&gt;\n\n\n\nUnfortunately, there are different attributes associated with this data object!\n\n\n#ed_data.head(), ed_data.info() won't work!\ntype(ed_data)\n\n\n    pandas.io.excel._base.ExcelFiledef __init__(path_or_buffer, engine: str | None=None, storage_options: StorageOptions | None=None, engine_kwargs: dict | None=None) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.pyClass for parsing tabular Excel sheets into DataFrame objects.\n\nSee read_excel for more documentation.\n\nParameters\n----------\npath_or_buffer : str, bytes, path object (pathlib.Path or py._path.local.LocalPath),\n    A file-like object, xlrd workbook or openpyxl workbook.\n    If a string or path object, expected to be a path to a\n    .xls, .xlsx, .xlsb, .xlsm, .odf, .ods, or .odt file.\nengine : str, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Supported engines: ``xlrd``, ``openpyxl``, ``odf``, ``pyxlsb``, ``calamine``\n    Engine compatibility :\n\n    - ``xlrd`` supports old-style Excel files (.xls).\n    - ``openpyxl`` supports newer Excel file formats.\n    - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n    - ``pyxlsb`` supports Binary Excel files.\n    - ``calamine`` supports Excel (.xls, .xlsx, .xlsm, .xlsb)\n      and OpenDocument (.ods) file formats.\n\n    .. versionchanged:: 1.2.0\n\n       The engine `xlrd &lt;https://xlrd.readthedocs.io/en/latest/&gt;`_\n       now only supports old-style ``.xls`` files.\n       When ``engine=None``, the following logic will be\n       used to determine the engine:\n\n       - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n         then `odf &lt;https://pypi.org/project/odfpy/&gt;`_ will be used.\n       - Otherwise if ``path_or_buffer`` is an xls format,\n         ``xlrd`` will be used.\n       - Otherwise if ``path_or_buffer`` is in xlsb format,\n         `pyxlsb &lt;https://pypi.org/project/pyxlsb/&gt;`_ will be used.\n\n       .. versionadded:: 1.3.0\n\n       - Otherwise if `openpyxl &lt;https://pypi.org/project/openpyxl/&gt;`_ is installed,\n         then ``openpyxl`` will be used.\n       - Otherwise if ``xlrd &gt;= 2.0`` is installed, a ``ValueError`` will be raised.\n\n       .. warning::\n\n        Please do not report issues when using ``xlrd`` to read ``.xlsx`` files.\n        This is not supported, switch to using ``openpyxl`` instead.\nengine_kwargs : dict, optional\n    Arbitrary keyword arguments passed to excel engine.\n\nExamples\n--------\n&gt;&gt;&gt; file = pd.ExcelFile('myfile.xlsx')  # doctest: +SKIP\n&gt;&gt;&gt; with pd.ExcelFile(\"myfile.xls\") as xls:  # doctest: +SKIP\n...     df1 = pd.read_excel(xls, \"Sheet1\")  # doctest: +SKIP\n      \n      \n\n\n\ned_data.sheet_names\n\n['EDU01A',\n 'EDU01B',\n 'EDU01C',\n 'EDU01D',\n 'EDU01E',\n 'EDU01F',\n 'EDU01G',\n 'EDU01H',\n 'EDU01I',\n 'EDU01J']\n\n\n\nUse .parse() method with sheet to obtain a usual DataFrame\n\n\ned_data.parse('EDU01A').head()\n\n/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n\n\n\n  \n    \n\n\n\n\n\n\nArea_name\nSTCOU\nEDU010187F\nEDU010187D\nEDU010187N1\nEDU010187N2\nEDU010188F\nEDU010188D\nEDU010188N1\nEDU010188N2\n...\nEDU010194N1\nEDU010194N2\nEDU010195F\nEDU010195D\nEDU010195N1\nEDU010195N2\nEDU010196F\nEDU010196D\nEDU010196N1\nEDU010196N2\n\n\n\n\n0\nUNITED STATES\n0\n0\n40024299\n0\n0\n0\n39967624\n0\n0\n...\n0\n0\n0\n43993459\n0\n0\n0\n44715737\n0\n0\n\n\n1\nALABAMA\n1000\n0\n733735\n0\n0\n0\n728234\n0\n0\n...\n0\n0\n0\n727989\n0\n0\n0\n736825\n0\n0\n\n\n2\nAutauga, AL\n1001\n0\n6829\n0\n0\n0\n6900\n0\n0\n...\n0\n0\n0\n7568\n0\n0\n0\n7834\n0\n0\n\n\n3\nBaldwin, AL\n1003\n0\n16417\n0\n0\n0\n16465\n0\n0\n...\n0\n0\n0\n19961\n0\n0\n0\n20699\n0\n0\n\n\n4\nBarbour, AL\n1005\n0\n5071\n0\n0\n0\n5098\n0\n0\n...\n0\n0\n0\n5017\n0\n0\n0\n5053\n0\n0\n\n\n\n\n\n5 rows × 42 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAlternatively, use the read_excel() function from pandas\nThis reads things in to a standard DataFrame but you have to specify a sheet to read in (or it defaults to the 1st)\ncensusEd.xlsx file located at “https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx”\n\n\ned_data = pd.read_excel(\"https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx\",\n                        sheet_name = 0) #or \"EDU01A\"\ned_data.head()\n\n/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n\n\n\n  \n    \n\n\n\n\n\n\nArea_name\nSTCOU\nEDU010187F\nEDU010187D\nEDU010187N1\nEDU010187N2\nEDU010188F\nEDU010188D\nEDU010188N1\nEDU010188N2\n...\nEDU010194N1\nEDU010194N2\nEDU010195F\nEDU010195D\nEDU010195N1\nEDU010195N2\nEDU010196F\nEDU010196D\nEDU010196N1\nEDU010196N2\n\n\n\n\n0\nUNITED STATES\n0\n0\n40024299\n0\n0\n0\n39967624\n0\n0\n...\n0\n0\n0\n43993459\n0\n0\n0\n44715737\n0\n0\n\n\n1\nALABAMA\n1000\n0\n733735\n0\n0\n0\n728234\n0\n0\n...\n0\n0\n0\n727989\n0\n0\n0\n736825\n0\n0\n\n\n2\nAutauga, AL\n1001\n0\n6829\n0\n0\n0\n6900\n0\n0\n...\n0\n0\n0\n7568\n0\n0\n0\n7834\n0\n0\n\n\n3\nBaldwin, AL\n1003\n0\n16417\n0\n0\n0\n16465\n0\n0\n...\n0\n0\n0\n19961\n0\n0\n0\n20699\n0\n0\n\n\n4\nBarbour, AL\n1005\n0\n5071\n0\n0\n0\n5098\n0\n0\n...\n0\n0\n0\n5017\n0\n0\n0\n5053\n0\n0\n\n\n\n\n\n5 rows × 42 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nYou can read all sheets with sheet_name = None\nThis gets read into a dictionary!\n\nKeys are the sheet name\nValues are the DataFrames from each sheet\n\n\n\ned_data = pd.read_excel(\"https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx\",\n                        sheet_name = None)\ntype(ed_data)\n\n/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n\n\ndict\n\n\n\ned_data.keys()\n\ndict_keys(['EDU01A', 'EDU01B', 'EDU01C', 'EDU01D', 'EDU01E', 'EDU01F', 'EDU01G', 'EDU01H', 'EDU01I', 'EDU01J'])\n\n\n\ned_data.get(\"EDU01A\").head() #get one DataFrame using its key!\n\n\n  \n    \n\n\n\n\n\n\nArea_name\nSTCOU\nEDU010187F\nEDU010187D\nEDU010187N1\nEDU010187N2\nEDU010188F\nEDU010188D\nEDU010188N1\nEDU010188N2\n...\nEDU010194N1\nEDU010194N2\nEDU010195F\nEDU010195D\nEDU010195N1\nEDU010195N2\nEDU010196F\nEDU010196D\nEDU010196N1\nEDU010196N2\n\n\n\n\n0\nUNITED STATES\n0\n0\n40024299\n0\n0\n0\n39967624\n0\n0\n...\n0\n0\n0\n43993459\n0\n0\n0\n44715737\n0\n0\n\n\n1\nALABAMA\n1000\n0\n733735\n0\n0\n0\n728234\n0\n0\n...\n0\n0\n0\n727989\n0\n0\n0\n736825\n0\n0\n\n\n2\nAutauga, AL\n1001\n0\n6829\n0\n0\n0\n6900\n0\n0\n...\n0\n0\n0\n7568\n0\n0\n0\n7834\n0\n0\n\n\n3\nBaldwin, AL\n1003\n0\n16417\n0\n0\n0\n16465\n0\n0\n...\n0\n0\n0\n19961\n0\n0\n0\n20699\n0\n0\n\n\n4\nBarbour, AL\n1005\n0\n5071\n0\n0\n0\n5098\n0\n0\n...\n0\n0\n0\n5017\n0\n0\n0\n5053\n0\n0\n\n\n\n\n\n5 rows × 42 columns"
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-json-data",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-json-data",
    "title": "Pandas for Reading Raw Data",
    "section": "Reading JSON Data",
    "text": "Reading JSON Data\n\nJSON data has a structure similar to a dictionary\n\nKey-value pairs\n\n\n{  \n  {  \n    \"name\": \"Barry Sanders\"  \n    \"games\" : 153  \n    \"position\": \"RB\"  \n  },  \n  {  \n    \"name\": \"Joe Montana\"  \n    \"games\": 192  \n    \"position\": \"QB\"  \n  }  \n}\n\nread_json() function from pandas will work!\nRead in data from URL: “https://api.exchangerate-api.com/v4/latest/USD”\n\n\nurl = \"https://api.exchangerate-api.com/v4/latest/USD\"\nusd_data = pd.read_json(url)\nusd_data.head()\n\n\n  \n    \n\n\n\n\n\n\nprovider\nWARNING_UPGRADE_TO_V6\nterms\nbase\ndate\ntime_last_updated\nrates\n\n\n\n\nUSD\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n1.00\n\n\nAED\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n3.67\n\n\nAFN\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n70.58\n\n\nALL\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n94.12\n\n\nAMD\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n396.72\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nEvery API is different so contacting them and returning data differs depending on the website you use. This is an important way we obtain data these days!"
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html",
    "href": "01_Programming_in_python/18-More_Function_Writing.html",
    "title": "More on Writing Functions",
    "section": "",
    "text": "We’ve gone through a lot with python already!\nNext up we’ll cover a bit more about writing our own functions that will greatly increase their usefulness! After that we’ll talk about how to plot our data using matplotlib and pandas.\nThen we’ll really be ready to start talking about modeling, data sources, and generally moving towards doing fun things with big data!\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!"
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#writing-functions-recap",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#writing-functions-recap",
    "title": "More on Writing Functions",
    "section": "Writing Functions Recap",
    "text": "Writing Functions Recap\n\nWriting functions is super cool!\nRecall the basic syntax\n\n\ndef func_name(args):\n    \"\"\"\n    Doc string\n    \"\"\"\n    body\n    return object\n\n\nWe saw that there were many ways to set up your function arguments and to call your function\nRemember that variables defined within the function are not generally available outside of the function\n\nThat is, a new symbol table is used when the function is called\nWe can define global variables if we really want to\n\nWe return what we want from the function with return\n\nIf we don’t return anything then None is returned!\n\n\nThe topics we’ll cover in this notebook are: - Packing and unpacking with functions + Catching extra arguments given to a function + Passing your arguments to a function from an object - lambda functions - map(), filter(), and functools.reduce()\nIn a later notebook we’ll talk about how to handle errors or exceptions!"
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#packing-and-unpacking",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#packing-and-unpacking",
    "title": "More on Writing Functions",
    "section": "Packing and Unpacking",
    "text": "Packing and Unpacking\nReminder: We can pack a list by separating variables to create with commas:\nfirst, second, third = ...\nLet’s look at an example:\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nshort_animals = animals[:3]\n\nfirst, second, third = short_animals\nprint(first + \" \" + second + \" \" + third)\n\nDog Cat Horse\n\n\nWe saw that we can pack leftover elements into a list using *variable:\n\nfirst, second, third, *other = animals\nprint(first + \" \" + second + \" \" + third)\nprint(other)\n\nDog Cat Horse\n['Frog', 'Cow', 'Buffalo', 'Deer', 'Fish', 'Bird', 'Fox', 'Racoon']\n\n\n\n\nUnlimited Positional Arguments\n\nThis idea can be used when writing a function!\nIn this case we define an argument to our function with *variable\nThis allows us to pass unlimited positional arguments to our function (variadic arguments)\nThe inputs are handled as a tuple in the function!\n\nLet’s write a silly function to print out all arguments passed via this idea\n\ndef basic_print(*args):\n  print(type(args))\n  print(args)\n  return None\n\nWe can pass this function as many things as we’d like and it will be accessible within the function body as a tuple. We can see this as the printed values are surrounded by ( and ), which implies we are printing a tuple!\n\nbasic_print(\"hi\", [\"a list\", \"how fun\"], 3, 10)\n\n&lt;class 'tuple'&gt;\n('hi', ['a list', 'how fun'], 3, 10)\n\n\nAs tuples are iterable, we can iterate across these elements via a loop!\n\ndef basic_print_elements(*args):\n  for i in args:\n    print(type(i),i)\n  return None\n\n\nbasic_print_elements(\"hi\", [\"a list\", \"how fun\"], 3, 10)\n\n&lt;class 'str'&gt; hi\n&lt;class 'list'&gt; ['a list', 'how fun']\n&lt;class 'int'&gt; 3\n&lt;class 'int'&gt; 10\n\n\nLet’s define a function that takes in as many 1D numpy arrays or pandas Series the user would like and returns the means for each input.\nWe’ll also take an argument for the number of decimal places to return for the means.\n\ndef find_means(*args, decimals = 4):\n    \"\"\"\n    Assume that args will be a bunch of numpy arrays (1D) or pandas series\n    Return the mean of each, rounded to `decimals` places\n    \"\"\"\n    means = []\n    for x in args: #iterate over the tuple values\n        means.append(np.mean(x).round(decimals))\n    return means\n\n\nCreate some data with numpy to send to this\n\n\nimport numpy as np\nfrom numpy.random import default_rng\nrng = default_rng(3) #set seed to 3\n\n#generate a few means from standard normal data\nn5 = rng.standard_normal(5)       #sample size of 5\nn25 = rng.standard_normal(25)     #sample size of 25\nn100 = rng.standard_normal(100)   #sample size of 100\nn1000 = rng.standard_normal(1000) #sample size of 1000\n\nLet’s pass these to our function!\n\nfind_means(n5, n25, n100, n1000, decimals = 2)\n\n[-0.22, 0.11, -0.01, 0.04]\n\n\nAwesome! This gives us a lot more functionality with our function writing.\n\n\n\nUnlimited Keyword Arguments\n\nYou can also pass unlimited keyword arguments if you define the arg with a **\nHandled as a dictionary in the function\n\nLet’s write a basic function to print out the keywords with their values.\n\ndef print_key_value_pairs(**kwargs):\n    \"\"\"\n    key word args can be anything\n    \"\"\"\n    print(type(kwargs), kwargs)\n    for x in kwargs:\n        print(x + \" : \" + str(kwargs.get(x))) #cast the value to a string for printing\n\nNow we pass as many named arguments as we’d like!\n\nprint_key_value_pairs(\n  name = \"Justin\",\n  job = \"Professor\",\n  phone = 9195150637)\n\n&lt;class 'dict'&gt; {'name': 'Justin', 'job': 'Professor', 'phone': 9195150637}\nname : Justin\njob : Professor\nphone : 9195150637"
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#unpacking-arguments",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#unpacking-arguments",
    "title": "More on Writing Functions",
    "section": "Unpacking Arguments",
    "text": "Unpacking Arguments\n\nSuppose we want to call our function but our arguments are stored in a list or tuple\n\nWe’ll do this a bit when we do our machine learning models!\n\nWe can unpack this list or tuple to be our function arguments by calling our function in a particular way.\n\n\n#We want to call our find_means function with these arguments\ncall_args = [n5, n25, n100, n1000]\n\n\nCall the function using *call_args (unpacking)\n\n\nfind_means(*call_args, decimals = 3)\n\n[-0.223, 0.114, -0.014, 0.04]\n\n\nNice! Now we can more easily call our function too!\n\nWe can do the same thing with our keyword arguments.\nSuppose our keyword arguments are stored in a dictionary\nCan call the function using **kw_call_args (unpacking)\n\nDefine a quick function.\n\ndef print_items(name, job, number):\n  print(\"Name is: \", name)\n  print(\"Job is: \", job)\n  print(\"Phone number is: \", number)\n  return\n\nCreate a dictionary with key-value pairs corresponding to our inputs.\n\nkw_call_args = {\"name\": \"Justin Post\", \"job\": \"Professor\", \"number\": \"9195150637\"}\nkw_call_args\n\n{'name': 'Justin Post', 'job': 'Professor', 'number': '9195150637'}\n\n\nCall our function using ** with our dictionary!\n\nprint_items(**kw_call_args)\n\nName is:  Justin Post\nJob is:  Professor\nPhone number is:  9195150637\n\n\n\nPassing named and unnamed arguments can both be done at once!\nRecall our find_means function inputs: def find_means(*args, decimals = 4):\n\n\ndec_dictionary = {\"decimals\": 6}\nfind_means(*call_args, **dec_dictionary)\n\n[-0.223413, 0.114454, -0.014443, 0.039762]"
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#lambda-functions",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#lambda-functions",
    "title": "More on Writing Functions",
    "section": "Lambda Functions",
    "text": "Lambda Functions\nAnother thing that comes up a lot is that we need to create a quick function for a single purpose that we don’t want to reuse for later.\nRather than define a function and storing it as an object the way we’ve been doing it, we can create a lambda function (also sometimes called an in-line function or an anonymous function)\n\nUse keyword lambda\nDefine arguments followed by a :\nGive the action for the function to perform\n\nSyntax requires a single line. Cannot use return or some other keywords\n\n\n\nsquare_it = lambda x : x**2\nsquare_it(10)\n\n100\n\n\n\nsquare_then_add = lambda x, y : x**2 + y\nsquare_then_add(10, 5)\n\n105\n\n\n\nCan still define the arguments in many ways\n\n\nmy_print = lambda x, y = \"ho\": print(x, y)\nmy_print(\"hi\")\n\nhi ho\n\n\n\nCan create the arbitrary positional arguments too (but then why would we be using lambda function!? This is just to show the functionality.)\n\n\nmy_print = lambda *x: [print(\"Input: \" + str(z)) for z in x]\nmy_print(\"hi\", \"ho\", \"off\", \"to\", \"work\", \"we\", \"go\")\n\nInput: hi\nInput: ho\nInput: off\nInput: to\nInput: work\nInput: we\nInput: go\n\n\n[None, None, None, None, None, None, None]\n\n\nNow, saving the function function in an object is really kind of counter to the point of an anonymous (lambda) function. We don’t usually save these for later use! We’ll see many uses for lambda functions. Let’s cover one of those here.\n\nmap()\nUsing lambda functions comes up a lot in the map/reduce idea. This is important for some of the legacy big data operations that we’ll do!\nMap/reduce idea: - Apply (or map) a function to each element of an iterable object - Combine (or reduce) the results where able\nExample: Counting words - Want to take a list of words and create a tuple with the word and the value 1 - Syntax for map: + map(function, object_to_apply_function_to)\n\nres = map(\n    lambda word: (word, 1),\n    [\"these\", \"are\", \"my\", \"words\", \"these\", \"words\", \"rule\"]\n    )\n\nSimilar to other functions like range or zip, we don’t get back the actual object we think we would. Instead we get a map object that can be used to find the mapped values.\n\nprint(type(res))\nres\n\n&lt;class 'map'&gt;\n\n\n&lt;map at 0x7f615a9b1990&gt;\n\n\nWe can convert the map object to a list using list()\n\nlist(res)\n\n[('these', 1),\n ('are', 1),\n ('my', 1),\n ('words', 1),\n ('these', 1),\n ('words', 1),\n ('rule', 1)]\n\n\nLet’s return the square of some values without defining a square function via map()\n\nmap(lambda r: r **2, range(0,5))\n\n&lt;map at 0x7f615a9b39d0&gt;\n\n\n\nlist(map(lambda r: r **2, range(0,5)))\n\n[0, 1, 4, 9, 16]\n\n\nNote: this can equivalently be done using a list comprehension!\n\n[r ** 2 for r in range(0,5)]\n\n[0, 1, 4, 9, 16]\n\n\nAnother example of using map with a lambda function might be to quickly uppercase a list of strings.\n\nlist(map(lambda x: x.upper(), ['cat', 'dog', 'wolf', 'bear', 'parrot']))\n\n['CAT', 'DOG', 'WOLF', 'BEAR', 'PARROT']\n\n\nAgain, this could be done with a list comprehension!\n\n[x.upper() for x in ['cat', 'dog', 'wolf', 'bear', 'parrot']] #equivalent\n\n['CAT', 'DOG', 'WOLF', 'BEAR', 'PARROT']\n\n\nOne interesting use of a lambda function is through the creation of a function generator\n\nCreate a function that generates functions!\nHere a function to raise a number to a given power\n\n\ndef raise_power(k):\n    return lambda r: r ** k\n\n\nsquare = raise_power(2) #creates a function!\nsquare(10)\n\n100\n\n\n\ncube = raise_power(3)\ncube(5)\n\n125\n\n\nWe can put this together with our packing idea and map!\n\nident, square, cube = map(raise_power, range(1,4))\nident(4)\n\n4\n\n\n\nsquare(4)\n\n16\n\n\n\ncube(4)\n\n64\n\n\n\n\n\nfilter()\n\nLambda functions can be used with filter()\n\nfilter() takes a predicate (statement to return what you want) as the first arg and an iterable as the second\nWe can give the first argument as a lambda function\n\n\nHere we want to return only vowels from a string (an iterable).\n\nfilter(lambda x: x in \"aeiou\", \"We want to return just the vowels.\")\n\n&lt;filter at 0x7f615a9b3f40&gt;\n\n\n\n#return in list form!\nlist(filter(lambda x: x in \"aeiou\", \"We want to return just the vowels.\"))\n\n['e', 'a', 'o', 'e', 'u', 'u', 'e', 'o', 'e']\n\n\nEquivalent to doing a list comprehension with an if in there!\n\n[x for x in \"We want to return just the vowels.\" if x in \"aeiou\"] #equivalent\n\n['e', 'a', 'o', 'e', 'u', 'u', 'e', 'o', 'e']\n\n\nThis time let’s use filter to only return even numbers from an iterable object.\n\nRecall the mod operator %, which returns the remainder\nA number is even if, when we divide by 2, we get 0 as the remainder\n\n\nlist(filter(lambda x: (x % 2) != 0, range(0, 10)))\n\n[1, 3, 5, 7, 9]\n\n\nEquivalent to this list comprehension with an if:\n\n[x for x in range(0, 10) if (x % 2) != 0]\n\n[1, 3, 5, 7, 9]\n\n\n\n\n\nfunctools.reduce()\nLambda functions can be used with functools.reduce()\n\nreduce() takes in a function of two variables and an iterable\nIt applies the function repetitively over the iterable, and returns the result\n\nHere, we’ll find the cumulative sum of a bunch of numbers (given as an iterable)\n\nfrom functools import reduce\nreduce(lambda x, y: x + y, range(1,11)) # sum first 10 numbers\n\n55\n\n\nHere, reduce() works like this: - Takes the first two arguments of the iterable (1 and 2) and adds them - Takes the result of that (3) and adds it to the next argument of the iterable (3) - Repeats until the iterable is exhausted\nAgain, we could do this kind of thing with a list comprehension. Here we just use the sum function on the result.\n\nsum([x for x in range(1,11)])\n\n55\n\n\nWe can also provide an initial value to reduce() to start the computation at. Here we supply 45.\n\n#add an initial value to the computation\nreduce(lambda x, y: x + y, range(1,11), 45) # sum first 10 numbers + 45\n\n100\n\n\nOk, that’s a bit silly. We can do more interesting things with this. For instance, here we write a reduce function to find the largest value in a list.\n\n#create a list of numbers to find the max of\nmy_list = [53, 13, 103, 2, 15, -10, 201, 6]\nreduce(lambda x, y: x if x &gt; y else y, my_list)\n\n201\n\n\nHow does that work? - Take x (53) and y (13), if x &gt; y take x (53), otherwise take y (13) - With the result of that as x, take y as the next value in the iterable (103) - Repeat that step. Here it would keep 103 since it is larger - Keep going until the iterable is exhausted\nThis works with a starting value as well!\n\nreduce(lambda x, y: x if x &gt; y else y, my_list, 500)\n\n500"
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#quick-video",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#quick-video",
    "title": "More on Writing Functions",
    "section": "Quick Video",
    "text": "Quick Video\nThis video shows an example of writing more involved functions including the use of lambda and map(). Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=55af749d-fd9d-4b0d-b2b4-b10301614c9c&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")"
  },
  {
    "objectID": "01_Programming_in_python/20-Plotting_pandas.html",
    "href": "01_Programming_in_python/20-Plotting_pandas.html",
    "title": "Plotting with pandas",
    "section": "",
    "text": "Let’s see the basic functionality that pandas provides for plotting Series (columns essentially) and DataFrames\nLooking through the help files is really useful. This link is for the .plot() method but you can search for other methods pretty easily! The documentation will provide you all of the options you can pass. We’ll only discuss a few and the big picture ideas.\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#readin data\ntitanic_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/titanic.csv\")\n\n#remove some columns and a bad row\nsub_titanic_data = titanic_data.drop(columns = [\"body\", \"cabin\", \"boat\"], axis = 1) \\\n                               .iloc[:(titanic_data.shape[0]-1)]\n#create category versions of the variables (some code omitted)\nsub_titanic_data[\"embarkedC\"] = sub_titanic_data.embarked.astype(\"category\")\nsub_titanic_data.embarkedC = sub_titanic_data.embarkedC.cat.rename_categories(\n                                    [\"Cherbourg\", \"Queenstown\", \"Southampton\"])\nsub_titanic_data[\"sexC\"] = sub_titanic_data.sex.astype(\"category\")\nsub_titanic_data.sexC = sub_titanic_data.sexC.cat.rename_categories([\"Female\", \"Male\"])\nsub_titanic_data[\"survivedC\"] = sub_titanic_data.survived.astype(\"category\")\nsub_titanic_data.survivedC = sub_titanic_data.survivedC.cat.rename_categories([\"Died\", \"Survived\"])"
  },
  {
    "objectID": "01_Programming_in_python/20-Plotting_pandas.html#barplots-with-pandas",
    "href": "01_Programming_in_python/20-Plotting_pandas.html#barplots-with-pandas",
    "title": "Plotting with pandas",
    "section": "Barplots with pandas",
    "text": "Barplots with pandas\nWe saw the barplot for summarizing categorical data. We’ll cover two different methods to create bar plots in pandas:\n\n.plot.bar() method on a series or dataframe\n.plot() method with kind = 'bar' specified\n\n\ntable = sub_titanic_data.embarkedC.value_counts()\nprint(type(table))\ntable\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\n\ncount\n\n\nembarkedC\n\n\n\n\n\nSouthampton\n914\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\n\n\ndtype: int64\n\n\nNote that this is a pandas series. We can use the .plot.bar() method on this series to get a bar plot.\n\ntable.plot.bar()\n\n&lt;Axes: xlabel='embarkedC'&gt;\n\n\n\n\n\nWe can then apply the matplotlib functionality to update/modify the plot (notice we already read in matplotlib.pyplot as plt.\n\ntable.plot.bar()\nplt.xticks(rotation = 0)\n\n(array([0, 1, 2]),\n [Text(0, 0, 'Southampton'),\n  Text(1, 0, 'Cherbourg'),\n  Text(2, 0, 'Queenstown')])\n\n\n\n\n\nAlternatively, we can use the slightly more flexible .plot() method on a series where we specify the kind= of the plot to create.\n\ntable.plot(kind = \"bar\", rot = 0) #can use additional arg rather than additional function call\n\n&lt;Axes: xlabel='embarkedC'&gt;\n\n\n\n\n\n\nWhere we really gain is when trying to bring in a multivariate relationship\n\nFor instance, we can color the bars by another categorical variable in the DataFrame pretty easily!\n\nFirst, create the contingency table for two variables (remember this returns a DataFrame)\n\ntable = pd.crosstab(sub_titanic_data[\"embarkedC\"], sub_titanic_data[\"survivedC\"])\nprint(type(table))\ntable\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\n\n\nembarkedC\n\n\n\n\n\n\nCherbourg\n120\n150\n\n\nQueenstown\n79\n44\n\n\nSouthampton\n610\n304\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nNow let’s use the .plot.bar() method on this DataFrame with the stacked = True argument.\n\ntable.plot.bar(stacked = True, rot = 0)\n\n&lt;Axes: xlabel='embarkedC'&gt;\n\n\n\n\n\nWe can do this with the .plot() method as well.\n\ntable.plot(stacked = True, kind = \"bar\", rot = 0)\n\n&lt;Axes: xlabel='embarkedC'&gt;\n\n\n\n\n\nIf we want side-by-side bar plots, we can just remove the stacked = True argument.\n\ntable.plot.bar(rot = 0)\n\n&lt;Axes: xlabel='embarkedC'&gt;"
  },
  {
    "objectID": "01_Programming_in_python/20-Plotting_pandas.html#plotting-numeric-variables",
    "href": "01_Programming_in_python/20-Plotting_pandas.html#plotting-numeric-variables",
    "title": "Plotting with pandas",
    "section": "Plotting Numeric Variables",
    "text": "Plotting Numeric Variables\nRecall: Numeric variable have entries that are a numerical value where math can be performed\nGoal: describe the shape, center, and spread of the distribution\n\nShape can be described well via a histogram or density plot\nBoxplots provide a good summary of the distribution as well\n\n\n\nHistogram with pandas\nHistogram - Bin data to show distribution of observations - Done via .plot.hist() or .plot(kind = \"hist\") method on a series or data frame - A .hist() method also exists!\nFirst, the .plot.hist() method on a series (we’ll also fix it up a bit using matplotlib.pyplot functionality.\n\nsub_titanic_data[\"age\"].plot.hist()\nplt.xlabel(\"Age\")\nplt.title(\"Histogram of Age for Titanic Passengers\")\n\nText(0.5, 1.0, 'Histogram of Age for Titanic Passengers')\n\n\n\n\n\n\nSpecify # of bins with bins =\nNote we also return the series in a different way here (just to show you can use either)\n\n\n#can add label/title here (xlabel doesn't seem to work as intended...)\n#instead we'll use the .set() method on the histogram to set the xlabel\nsub_titanic_data.age.plot.hist(bins = 20, title = \"Histogram of Age for Titanic Passengers\") \\\n    .set(xlabel = \"Age\")\n\n[Text(0.5, 0, 'Age')]\n\n\n\n\n\n\nOverlaying Two Histograms\n\nTo overlay two histograms on the same graph, create two histograms and use alpha = 0-1 value. This sets the transparency.\n\nalpha = 1 is not transparent at all\nalpha = 0 is completely transparent\n\n\nLet’s create histograms of age for those that Survived and those that Died. - We should also set up the bins manually so they are the same bin widths and locations (for better comparison) - bins can be specified via the bins = argument\n\nbin_ends = 10\nbins = [i*max(sub_titanic_data.age)/bin_ends for i in range(0, bin_ends + 1)]\nprint(bins)\n\n[0.0, 8.0, 16.0, 24.0, 32.0, 40.0, 48.0, 56.0, 64.0, 72.0, 80.0]\n\n\n\nObtain subsets of data needed\n\n\nage_died = sub_titanic_data.loc[sub_titanic_data.survivedC == \"Died\", \"age\"] #series for died\nage_survived = sub_titanic_data.loc[sub_titanic_data.survivedC == \"Survived\", \"age\"] #series for survived\n\nCreate the plot using the .plot.hist() method. By creating two plots in the same cell, they will be overlayed. - Notice the use of label() to automatically create a legend (similar to what we did with matplotlib\n\nage_died.plot.hist(bins = bins, alpha = 0.5, label = \"Died\",\n                   title = \"Ages for those that survived vs those that died\") \\\n                   .set(xlabel = \"Age\")\nage_survived.plot.hist(bins = bins, alpha = 0.5, label = \"Survived\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x788606f92950&gt;\n\n\n\n\n\n\npandas will automatically overlay data from different columns of the same data frame\nThat is, if we use the plot.hist() method on a data frame with two numeric variables, it will plot both of those on the same plot\n\nTo use that here we need to make that kind of data frame…\nNeed two columns, one representing ages for those that survived and one for those that died\n\n\n\nage_died = sub_titanic_data.loc[sub_titanic_data.survivedC == \"Died\", \"age\"] #809 values\nage_survived = sub_titanic_data.loc[sub_titanic_data.survivedC == \"Survived\", \"age\"] #500 values\n\n\nNote the difference in the number of observations! This means that putting them together into a data frame isn’t super seamless.\n\n\ntemp = pd.DataFrame(zip(age_died, age_survived), columns = [\"Died\", \"Survived\"])\nprint(temp.shape)\n#only has 500 rows instead of 809!\ntemp.plot.hist(alpha = 0.5)\n\n(500, 2)\n\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\nHow do we fix that?\n\nWe can fill in NaN values for the shorter series so they end up the same length.\n\n\n\nage_survived = pd.concat([age_survived, pd.Series([np.nan for _ in range(308)])])\nage_survived\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n29.0000\n\n\n1\n0.9167\n\n\n5\n48.0000\n\n\n6\n63.0000\n\n\n8\n53.0000\n\n\n...\n...\n\n\n303\nNaN\n\n\n304\nNaN\n\n\n305\nNaN\n\n\n306\nNaN\n\n\n307\nNaN\n\n\n\n\n808 rows × 1 columns\ndtype: float64\n\n\n\nNow we can zip these together into a data frame and plot as we’d like!\n\n\nplotting_df = pd.DataFrame(zip(age_died, age_survived),\n                      columns = [\"Died\", \"Survived\"])\nprint(plotting_df.shape)\nplotting_df.plot.hist(alpha = 0.5, title = \"Ages for those that survived vs those that died\") \\\n    .set(xlabel = \"Age\")\n\n(808, 2)\n\n\n[Text(0.5, 0, 'Age')]\n\n\n\n\n\n\n\nSide-by-side Histograms\n\nCan place two graphs next to each other with .hist() method (notice this is a different method!)\n\nSpecify a column variable and a by variable\n\nThese don’t have the same bin widths\n\n\nsub_titanic_data.hist(column = \"age\", by = \"survivedC\")\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.hist(column = \"age\", by = \"survivedC\")\n\n\narray([&lt;Axes: title={'center': 'Died'}&gt;,\n       &lt;Axes: title={'center': 'Survived'}&gt;], dtype=object)\n\n\n\n\n\n\nWe could also use the .groupby() functionality but the result is a bit subpar as it doesn’t label the graphs.\n\n\nsub_titanic_data[[\"age\", \"survivedC\"]].groupby(\"survivedC\").hist()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data[[\"age\", \"survivedC\"]].groupby(\"survivedC\").hist()\n\n\n\n\n\n\n\n\n\n0\n\n\nsurvivedC\n\n\n\n\n\nDied\n[[Axes(0.125,0.11;0.775x0.77)]]\n\n\nSurvived\n[[Axes(0.125,0.11;0.775x0.77)]]\n\n\n\n\ndtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nKernel smoother with pandas\n\nKernel Smoother - Smoothed version of a histogram\n\n‘Kernel’ determines weight given to nearby points\n\nUse .plot.density() or plot(kind = \"density\") method\nbw_method = # specifies how ‘smooth’ you want the graph to be\n\nsmaller values imply using a smaller bandwidth (more variability)\nlarger values imply using a larger bandwidth (more smooth)\n\n\n\n\nsub_titanic_data.age.plot.density(bw_method = 0.1, label = \"bw = 0.1\",\n                                  title = \"Density Plots of Age for Titanic Passengers\")\nsub_titanic_data.age.plot.density(bw_method = 0.25, label = \"bw = 0.25\")\nsub_titanic_data.age.plot.density(bw_method = 0.5, label = \"bw = 0.5\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x78860547a950&gt;\n\n\n\n\n\n\n\n\nBoxplots with pandas\n\nBoxplot - Provides the five number summary in a graph\n\nMin, Q1, Median, Q3, Max\n\nOften show possible outliers as well\n\nUse .plot.box() or plot(kind = \"box\") method\nA .boxplot() method also exists!\n\n\nFirst the .plot.box() method on a series\n\nsub_titanic_data.age.plot.box()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nFine.. but usually we want to compare these boxplots across another variable. To do this the .boxplot() method on a data frame is very useful!\nSimilar to the .hist() method we specify a column and by variable\n\n\nsub_titanic_data.boxplot(column = [\"age\"], by = \"survivedC\")\n\n&lt;Axes: title={'center': 'age'}, xlabel='survivedC'&gt;\n\n\n\n\n\n\n\n\nScatter Plots with pandas\n\nScatter Plot - graphs points corresponding to each observation\n\nUse .plot.scatter() or plot(kind = \"scatter\") method on a data frame with x =, and y =\n\n\n\nsub_titanic_data.plot.scatter(x = \"age\", y = \"fare\", title = \"Scatter plots rule!\")\n\n&lt;Axes: title={'center': 'Scatter plots rule!'}, xlabel='age', ylabel='fare'&gt;\n\n\n\n\n\n\nEasy to modify! Check the help for arguments (specifically the keyword arguments that get passed to .plot()) but we can specify different marker values, a title, and more!\n\n\n#c = color, marker is a matplotlib option\nsub_titanic_data.plot.scatter(x = \"age\", y = \"fare\", c = \"Red\", marker = \"v\", title = \"Oh, V's!\")\n\n&lt;Axes: title={'center': \"Oh, V's!\"}, xlabel='age', ylabel='fare'&gt;\n\n\n\n\n\n\nWe can easily modify aspects of the plot based on a variable as well!\nThis is great as it allows us to bring a third varaible in\nHere we color by a category variable\n\n\n#s for size (should be a numeric column), cmap can be used with c for specifying color scales\nsub_titanic_data.plot.scatter(x = \"age\", y = \"fare\", c = \"survivedC\", cmap = \"viridis\", s = 10)\n\n&lt;Axes: xlabel='age', ylabel='fare'&gt;\n\n\n\n\n\n\n\nMatrix of Scatter Plots\n\n.plotting.scatter_matrix() function will produce basic graphs showing relationships!\nHere we grab the numeric variables from the data frame\n\n\npd.plotting.scatter_matrix(sub_titanic_data[[\"age\", \"fare\", \"survived\", \"sibsp\"]])\n\narray([[&lt;Axes: xlabel='age', ylabel='age'&gt;,\n        &lt;Axes: xlabel='fare', ylabel='age'&gt;,\n        &lt;Axes: xlabel='survived', ylabel='age'&gt;,\n        &lt;Axes: xlabel='sibsp', ylabel='age'&gt;],\n       [&lt;Axes: xlabel='age', ylabel='fare'&gt;,\n        &lt;Axes: xlabel='fare', ylabel='fare'&gt;,\n        &lt;Axes: xlabel='survived', ylabel='fare'&gt;,\n        &lt;Axes: xlabel='sibsp', ylabel='fare'&gt;],\n       [&lt;Axes: xlabel='age', ylabel='survived'&gt;,\n        &lt;Axes: xlabel='fare', ylabel='survived'&gt;,\n        &lt;Axes: xlabel='survived', ylabel='survived'&gt;,\n        &lt;Axes: xlabel='sibsp', ylabel='survived'&gt;],\n       [&lt;Axes: xlabel='age', ylabel='sibsp'&gt;,\n        &lt;Axes: xlabel='fare', ylabel='sibsp'&gt;,\n        &lt;Axes: xlabel='survived', ylabel='sibsp'&gt;,\n        &lt;Axes: xlabel='sibsp', ylabel='sibsp'&gt;]], dtype=object)"
  },
  {
    "objectID": "01_Programming_in_python/20-Plotting_pandas.html#quick-video",
    "href": "01_Programming_in_python/20-Plotting_pandas.html#quick-video",
    "title": "Plotting with pandas",
    "section": "Quick Video",
    "text": "Quick Video\nThis video shows an example of using pandas for plotting. Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=f4e2bc16-1757-4f1e-8df9-b103016c97d2&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")"
  },
  {
    "objectID": "01_Programming_in_python/22-Big_Recap_Landing.html",
    "href": "01_Programming_in_python/22-Big_Recap_Landing.html",
    "title": "Big Recap!",
    "section": "",
    "text": "The video below attempts to help place us in our goals for the course. We discuss what we’ve done so far and where we are going. I highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/22-Big_Recap_Landing.html#notes",
    "href": "01_Programming_in_python/22-Big_Recap_Landing.html#notes",
    "title": "Big Recap!",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version"
  },
  {
    "objectID": "01_Programming_in_python/22-Big_Recap_Landing.html#additional-readings-for-week-4",
    "href": "01_Programming_in_python/22-Big_Recap_Landing.html#additional-readings-for-week-4",
    "title": "Big Recap!",
    "section": "Additional Readings for Week 4",
    "text": "Additional Readings for Week 4\nHere are some useful resources for learning about modeling, metrics, etc.\n\nISLR book: Read sections\n\n(Big picture ideas) 2.1, 2.2\n(Basic regression) 3.1, 3.2, 3.3\n\n5.1\n\n(Regularized models) 6.2\n\nGoogle’s open course materials have information about these models, loss functions, metrics, etc:\nLASSO basics in python\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/24-Prediction_Testing_Training_Landing.html",
    "href": "01_Programming_in_python/24-Prediction_Testing_Training_Landing.html",
    "title": "Prediction and Training/Test Set Ideas",
    "section": "",
    "text": "The video below elaborates on the ways that we evaluate predictive models. We ideally want our models to perform well on data it isn’t trained on. In order to understand that behavior, we can split our data into a training and a test set. These ideas are vital for understanding how well our model works and applying more complicated methods like cross validation.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/24-Prediction_Testing_Training_Landing.html#notes",
    "href": "01_Programming_in_python/24-Prediction_Testing_Training_Landing.html#notes",
    "title": "Prediction and Training/Test Set Ideas",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/26-Multiple_Linear_Regression_Landing.html",
    "href": "01_Programming_in_python/26-Multiple_Linear_Regression_Landing.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The video below goes into the ideas of mutliple linear regression (MLR). This is the extension of the SLR model that allows for multiple predictors, qualitative predictors, interactions, and more!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "01_Programming_in_python/26-Multiple_Linear_Regression_Landing.html#notes",
    "href": "01_Programming_in_python/26-Multiple_Linear_Regression_Landing.html#notes",
    "title": "Multiple Linear Regression",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#basics-of-python",
    "href": "01_Programming_in_python/Learning_Python.html#basics-of-python",
    "title": "Week 1",
    "section": "Basics of python",
    "text": "Basics of python\nCreate a list!\n\n[1, \"a\", 3, 60]\n\n[1, 'a', 3, 60]\n\n\nSave this as an object!\n\nmyList = [1, \"a\", 3, 60]\nprint(myList)\nmyList\n\n[1, 'a', 3, 60]\n\n\n[1, 'a', 3, 60]\n\n\n\nmyList + [2, 4, 5]\n\n[1, 'a', 3, 60, 2, 4, 5]\n\n\n\nmyList * 4\n\n[1, 'a', 3, 60, 1, 'a', 3, 60, 1, 'a', 3, 60, 1, 'a', 3, 60]"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#markdown",
    "href": "01_Programming_in_python/Learning_Python.html#markdown",
    "title": "Week 1",
    "section": "Markdown",
    "text": "Markdown\nWe’ll add some HTML widgets and play around with that.\n\n#reading in modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\n\nLet’s generate 100 random data values using numpy.\n\nrng = np.random.default_rng(12)\nvals = rng.standard_normal(100)\nvals\n\narray([-6.82677987e-03,  1.04614329e+00,  7.41588421e-01,  7.23956542e-01,\n        1.61877622e+00, -1.20555814e+00, -6.26955471e-01, -1.32066321e+00,\n       -1.07752508e-01,  9.98763655e-01, -2.19478863e-02,  4.95880066e-01,\n       -1.91076866e+00,  1.47064166e-01, -9.06943251e-01,  1.77538939e+00,\n        8.86849076e-01,  9.49349483e-01, -5.78549625e-02,  6.12862274e-01,\n        6.57890162e-01, -3.44402666e-01, -4.97372035e-01, -1.14772783e-01,\n       -6.05452009e-01, -5.94339418e-01, -2.83375376e-01, -7.28417727e-01,\n        7.66327786e-01, -1.59608633e+00,  8.23562129e-01, -6.25566470e-01,\n       -5.45939956e-01, -1.35084714e+00, -1.44242119e-01, -2.47661509e-01,\n        1.91455831e-01, -5.33774296e-01,  9.37561793e-02,  1.81969184e+00,\n        4.08999694e-01, -5.73690037e-01,  9.53109595e-01, -1.28801134e-01,\n        5.93874468e-01,  6.12747429e-01, -3.91065717e-01, -1.93028750e+00,\n       -3.47653623e-01,  5.51455424e-01, -3.80110407e-01,  4.38969315e-01,\n        9.79540372e-01, -5.44113415e-01,  1.23135175e+00,  1.62180446e+00,\n        1.07904594e+00,  1.16546284e+00,  1.09679645e+00,  2.25453906e+00,\n        1.85860905e-01,  2.08408285e-03,  6.03107305e-01, -9.08195119e-01,\n       -1.55317676e+00, -8.82007973e-01,  3.72565350e-01,  4.73053341e-01,\n       -1.53635872e+00, -1.88346371e+00, -3.16142243e-01, -1.88056516e-01,\n       -4.92008330e-02,  6.71362447e-01,  1.22883418e+00,  2.30896430e-01,\n        6.12684704e-01, -1.09520169e+00, -1.17630801e+00,  2.36238334e-01,\n        6.64663103e-01,  7.26243456e-01,  5.92741991e-01,  7.84173709e-01,\n        8.09828030e-01, -1.75218536e+00, -7.34165769e-01,  4.55159742e-01,\n        5.96671965e-01, -1.51209241e+00,  1.17306137e+00, -4.38831924e-01,\n       -2.32424749e-01,  2.73818645e-01,  7.49635232e-01, -1.43311699e+00,\n        9.89912577e-01,  7.72934677e-02, -7.70813495e-01, -2.75273142e-01])\n\n\n\nplt.hist(vals)\n\n(array([ 8.,  6.,  6., 17., 15., 12., 24.,  7.,  4.,  1.]),\n array([-1.9302875 , -1.51180484, -1.09332219, -0.67483953, -0.25635687,\n         0.16212578,  0.58060844,  0.99909109,  1.41757375,  1.8360564 ,\n         2.25453906]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\nplt.hist(vals, bins = 12)\n\n(array([ 5.,  6.,  5., 11., 13., 12., 11., 19., 11.,  2.,  4.,  1.]),\n array([-1.9302875 , -1.58155195, -1.2328164 , -0.88408086, -0.53534531,\n        -0.18660976,  0.16212578,  0.51086133,  0.85959687,  1.20833242,\n         1.55706797,  1.90580351,  2.25453906]),\n &lt;BarContainer object of 12 artists&gt;)\n\n\n\n\n\n\ndef myPlot(bins):\n    plt.hist(vals, bins)\n\nmyPlot(4)\n\n\n\n\n\nwidgets.interactive(myPlot, bins =(1,20))"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#strings-practice",
    "href": "01_Programming_in_python/Learning_Python.html#strings-practice",
    "title": "Week 1",
    "section": "Strings Practice",
    "text": "Strings Practice\nHere we’ll look at some quick string methods and functions including string formatting!\n\nString formatting\n\nOne way to do formatting is to use {} within a string and then use the .format() method. Let’s create a string that we can place values into. Something like this:\n“The probability of being less than or equal to 2 for a normal distribution with mean 4 and standard deviation 1 is …”\n\nprob_string = \"The probability of being less than or equal to {0} for a normal distribution with mean {1} and standard deviation {2} is {3}\"\n\n\nmean = 1\nstd = 2\ny = 1.55\n\n'The probability of being less than or equal to 1.55 for a normal distribution with mean 1 and standard deviation 2 is 0.6083'\n\n\nNow pull the scipy.stats stuff from above and insert the numbers using the .format() method.\n\nimport scipy.stats as stats\nstats.norm.cdf(y, mean, std)\n\n0.6083418808463948\n\n\n\nprob_string.format(y, mean, std, round(stats.norm.cdf(y, mean, std), 4))\n\n'The probability of being less than or equal to 1.55 for a normal distribution with mean 1 and standard deviation 2 is 0.6083'\n\n\n\nLet’s look at splitting up a big string of text into ‘words’. This is often a first step done when doing a basic sentiment analysis of some text.\n\nFirst we need a string of text to break up. Project Gutenberg has some classic open works that you can get the entire text from. Here we’ll pull from a cookbook called Practical Vegetarian Cookery. We’ll just grab two recipes:\n\nWINTER VEGETABLE PIE.\n\n\nPlace in baking dish, slices of cold boiled potatoes, onions, celery, and carrot, then add one scant cupful of stewed tomatoes and one half can of peas. Cover with stock, thickened to a gravy with butter and flour, cover with plain crust, and bake. A pie of this nature can be made with a great variety of ingredients; apples, boiled chestnuts, onions, and potatoes make a good combination. Rice, with a grating of cheese, celery, onion, and tomato, another variety.\n\n\nVEGETABLE HASH.\n\n\nOf cooked and chopped vegetables, use one carrot, one blood beet, two turnips, two quarts of finely sliced potatoes, one onion, and a stalk of celery; one sprig of parsley; put them in a stew pan, cover tight, and set in the oven. When thoroughly heated pour over a gravy of drawn butter and cream. Stir together and serve.\n\nLet’s create a string with these and count variables that occur.\n\nrecipe = \"\"\"\nWINTER VEGETABLE PIE.\n\nPlace in baking dish, slices of cold boiled potatoes, onions, celery,\nand carrot, then add one scant cupful of stewed tomatoes and one half\ncan of peas. Cover with stock, thickened to a gravy with butter and\nflour, cover with plain crust, and bake. A pie of this nature can be\nmade with a great variety of ingredients; apples, boiled chestnuts,\nonions, and potatoes make a good combination. Rice, with a grating of\ncheese, celery, onion, and tomato, another variety.\n\nVEGETABLE HASH.\n\nOf cooked and chopped vegetables, use one carrot, one blood beet, two\nturnips, two quarts of finely sliced potatoes, one onion, and a stalk\nof celery; one sprig of parsley; put them in a stew pan, cover tight,\nand set in the oven. When thoroughly heated pour over a gravy of drawn\nbutter and cream. Stir together and serve.\n\"\"\"\nrecipe\n\n'\\nWINTER VEGETABLE PIE.\\n\\nPlace in baking dish, slices of cold boiled potatoes, onions, celery,\\nand carrot, then add one scant cupful of stewed tomatoes and one half\\ncan of peas. Cover with stock, thickened to a gravy with butter and\\nflour, cover with plain crust, and bake. A pie of this nature can be\\nmade with a great variety of ingredients; apples, boiled chestnuts,\\nonions, and potatoes make a good combination. Rice, with a grating of\\ncheese, celery, onion, and tomato, another variety.\\n\\nVEGETABLE HASH.\\n\\nOf cooked and chopped vegetables, use one carrot, one blood beet, two\\nturnips, two quarts of finely sliced potatoes, one onion, and a stalk\\nof celery; one sprig of parsley; put them in a stew pan, cover tight,\\nand set in the oven. When thoroughly heated pour over a gravy of drawn\\nbutter and cream. Stir together and serve.\\n'\n\n\nReplace the \\n, ,, and . values with a space instead. Lowercase everything with the .lower() method and then .strip() to remove the excess whitespace. Finally, the .split() method gives us a list with each element being a word from the document.\n\nrecipe_split = recipe.replace(\"\\n\", \" \") \\\n  .replace(\".\", \" \") \\\n  .replace(\",\", \" \") \\\n  .replace(\";\", \" \") \\\n  .lower() \\\n  .strip() \\\n  .split(\" \")\n\nNow we can see how often different words come up.\n\ntype(recipe_split)\nrecipe_split.count(\"potatoes\")\n\n3"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#math-types-including-booleans",
    "href": "01_Programming_in_python/Learning_Python.html#math-types-including-booleans",
    "title": "Week 1",
    "section": "Math Types (Including Booleans)",
    "text": "Math Types (Including Booleans)\nRemember that things can’t be stored precisely so you may sometimes run into issues where two things should be equal but don’t resolve as equal.\n\nfrom math import sqrt\nsqrt(2)**2 == 2\n\nFalse\n\n\nUse the isclose() from numpy to check things like this.\n\nimport numpy as np\nnp.isclose(sqrt(2)**2, 2)\n\nTrue\n\n\nWe should have an idea about inf, -inf, and nan values in python. nan values can be returned when a mathematical function is used outside of its domain for instance. Here is the norm.ppf() function. This returns the value from the standard normal with a certain proportion of the distribution to the left of it (proportion specified by the value you feed the function). These values should of course be between 0 and 1.\n\nimport scipy.stats as stats\nstats.norm.ppf(0.5)\n\n0.0\n\n\n\n\n\nnan\n\n\nWhat happens if we give a value like -1?\n\nstats.norm.ppf(-0.1)\nnp.isnan(stats.norm.ppf(-0.1))\n\nTrue\n\n\nInfinite values can pop up too. There are lots of ways to create an infinite value via casting as well.\n\nfloat(1)\nfloat(\"Inf\")\n-float(\"Inf\")\n\n-inf\n\n\nLet’s update our string from before with digits! (Note that using floating point for z and integers for mean and standard deviation doesn’t really make sense, I’m just showing the functionality!)\n\nprob_string = \"The probability of being less than or equal to {y:.2f} for a normal distribution with mean {mean:d} and standard deviation {std:d} is {prob:.4f}\"\n\nprob_string.format(y = y, mean = mean, std = std, prob = stats.norm.cdf(y, mean, std))\n\n'The probability of being less than or equal to 1.55 for a normal distribution with mean 1 and standard deviation 2 is 0.6083'"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#user-defined-functions",
    "href": "01_Programming_in_python/Learning_Python.html#user-defined-functions",
    "title": "Week 1",
    "section": "User Defined Functions",
    "text": "User Defined Functions\nLet’s create our own plotting function that will plot a Binomial random variables probabilities with a Normal approximation overlay.\nIf you aren’t familiar, a Binomial random variable measures the number of successes in a fixed number of Success/Failure trials (denoted by \\(n\\)). These trials need to be indpendent and have the same probability of success (called \\(p\\)).\nWe denote this via \\[Y\\sim Bin(n, p)\\] As \\(Y\\) measures the number of successes in \\(n\\) trials, the values \\(Y\\) can take on are \\(y = 0, 1, 2, ..., n\\). The probabilities associated with each of these outcomes is given by the Probability Mass Function (PMF) of the Binomial distribution:\n\\[p_Y(y)= P(Y=y) = \\binom{n}{y}p^y(1-p)^{n-y}\\]\nwhere \\(\\binom{n}{y} = \\frac{n!}{y!(n-y)!}\\) is the binomial coefficient.\nWe can find these values using the scipy.stats module! Let’s read in the appropriate modules for finding these values and doing our plotting.\n\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nWhen writing a function, it is usually easier to write the code for the body of the function outside of a function definition first and then put it into a function.\nLet’s start by creating code to find the probabilities associated with the Binomial for a given \\(n\\) and \\(p\\). The scipy.stats module has a binom.pmf() function that will return probabilities from the PMF.\n\nprint(stats.binom.pmf.__doc__)\n\nProbability mass function at k of the given RV.\n\n        Parameters\n        ----------\n        k : array_like\n            Quantiles.\n        arg1, arg2, arg3,... : array_like\n            The shape parameter(s) for the distribution (see docstring of the\n            instance object for more information)\n        loc : array_like, optional\n            Location parameter (default=0).\n\n        Returns\n        -------\n        pmf : array_like\n            Probability mass function evaluated at k\n\n        \n\n\nNote: the k argument can be ‘array like’. That is, we can pass something like a list of values we want the probabilities for. As a binomial measures the number of successes in \\(n\\) trials, we need to consider all of the integers from 0 to n. This can be done easily with the range() function.\n\nstats.binom.pmf(0, n = 10, p = 0.5)\n\n0.0009765624999999989\n\n\n\nstats.binom.pmf([0,1,2], n = 10, p = 0.5)\n\narray([0.00097656, 0.00976563, 0.04394531])\n\n\nOk, we have the values of the random variable (0 to \\(n\\)) and the probabilities we want to plot. We can use matplotlib’s pyplot module which has a bar() function that can create a basic bar graph for us. It has two arguments:\n\nthe x values for the location of the bars\nthe height for the height of the bars\n\nWe just need to pass two things of the same length that correspond to the pairs of x and height.\n\nn = 30\np = 0.5\nplt.bar(x = range(n+1), height = stats.binom.pmf(range(n+1), n = n, p = p))\n\n&lt;BarContainer object of 31 artists&gt;\n\n\n\n\n\nGreat! Now let’s wrap that up into a function so the user can call it and just specify n and p. We’ll give default values of n=30 and p=0.5.\n\ndef binom_norm_plot(n = 30, p = 0.5):\n    plt.bar(x = range(n+1), height = stats.binom.pmf(range(n+1), n = n, p = p))\n    return None\n\n\nbinom_norm_plot()\n\n\n\n\n\nbinom_norm_plot(n= 10, p = 0.1)\n\n\n\n\nNow we are cooking! We want to add a curve to this graph corresponding to the commonly used Normal approximation to the binomial.\nThe Normal distribution has two parameters: mean and standard deviation (or variance)\nThe mean of the Binomial is \\(n*p\\) and the standard deviation is \\(\\sqrt{n*p*(1-p)}\\). We can plug these values into the norm.pdf() from the scipy.stats module (PDF stands for Probability Density Function and is the continuous analog to the PMF).\n\nprint(stats.norm.pdf.__doc__)\n\nProbability density function at x of the given RV.\n\n        Parameters\n        ----------\n        x : array_like\n            quantiles\n        arg1, arg2, arg3,... : array_like\n            The shape parameter(s) for the distribution (see docstring of the\n            instance object for more information)\n        loc : array_like, optional\n            location parameter (default=0)\n        scale : array_like, optional\n            scale parameter (default=1)\n\n        Returns\n        -------\n        pdf : ndarray\n            Probability density function evaluated at x\n\n        \n\n\n\nstats.norm.pdf(0, loc = 0, scale =1)\n\n0.3989422804014327\n\n\n\nimport math\nstats.norm.pdf(range(31), loc = n*p, scale = math.sqrt(n*p*(1-p)))\n\narray([4.45617467e-08, 3.08033680e-07, 1.86349517e-06, 9.86625663e-06,\n       4.57162515e-05, 1.85388542e-04, 6.57944456e-04, 2.04357059e-03,\n       5.55500081e-03, 1.32151677e-02, 2.75140991e-02, 5.01339573e-02,\n       7.99471056e-02, 1.11575174e-01, 1.36278225e-01, 1.45673124e-01,\n       1.36278225e-01, 1.11575174e-01, 7.99471056e-02, 5.01339573e-02,\n       2.75140991e-02, 1.32151677e-02, 5.55500081e-03, 2.04357059e-03,\n       6.57944456e-04, 1.85388542e-04, 4.57162515e-05, 9.86625663e-06,\n       1.86349517e-06, 3.08033680e-07, 4.45617467e-08])\n\n\nGreat, now let’s add those values to the plot using the plot() function from pyplot. We’ll learn about this later but if we use matplotlib and create a plot in a cell followed by another plot, it will place those on the same plot by default. Therefore, we just want to create the bar plot and then this other plot in the same cell.\n\nprint(plt.plot.__doc__)\n\nPlot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n&gt;&gt;&gt; plot(x, y)        # plot x and y using default line style and color\n&gt;&gt;&gt; plot(x, y, 'bo')  # plot x and y using blue circle markers\n&gt;&gt;&gt; plot(y)           # plot y using x as index array 0..N-1\n&gt;&gt;&gt; plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n&gt;&gt;&gt; plot(x, y, 'go--', linewidth=2, markersize=12)\n&gt;&gt;&gt; plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n&gt;&gt;&gt; plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  &gt;&gt;&gt; plot(x1, y1, 'bo')\n  &gt;&gt;&gt; plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  &gt;&gt;&gt; x = [1, 2, 3]\n  &gt;&gt;&gt; y = np.array([[1, 2], [3, 4], [5, 6]])\n  &gt;&gt;&gt; plot(x, y)\n\n  is equivalent to:\n\n  &gt;&gt;&gt; for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  &gt;&gt;&gt; plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    &gt;&gt;&gt; plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    &gt;&gt;&gt; plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `.Bbox`\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: unknown\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'&lt;'``         triangle_left marker\n``'&gt;'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).\n\n\n\nbinom_norm_plot(n = 30, p = 0.5)\nn = 30\np = 0.5\nplt.plot(range(n+1), stats.norm.pdf(range(n+1), loc = n*p, scale = math.sqrt(n*p*(1-p))))\n\n\n\n\nAdd that to the function!\n\ndef binom_norm_plot(n = 30, p = 0.5):\n    plt.bar(x = range(n+1), height = stats.binom.pmf(range(n+1), n = n, p = p))\n    plt.plot(range(n+1), stats.norm.pdf(range(n+1), loc = n*p, scale = math.sqrt(n*p*(1-p))))\n    return None\n\n\nbinom_norm_plot()\n\n\n\n\n\nbinom_norm_plot(n= 100, p = 0.2)"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#control-flow-examples",
    "href": "01_Programming_in_python/Learning_Python.html#control-flow-examples",
    "title": "Week 1",
    "section": "Control Flow Examples",
    "text": "Control Flow Examples\n\nfor loop and if/then/else\nFirst, let’s do an if/then/else logic example. Fizz buzz coding!\nThe logic for the fizz buzz programming problem: - If a number is divisible by 15 (both 3 and 5), then print fizz buzz - If a number is only divisible by 3, then print fizz - If a number is only divisible by 5, then print buzz - Otherwise print the number itself\nWe can use the % (modulus operator) to get the remainder from a division calculation.\n\n10 % 3\n\n1\n\n\n\n11 % 3\n\n2\n\n\n\n12 % 3\n\n0\n\n\nLet’s use if/then/else logic to go through and print the correct statement.\n\nvalue = 45\nif ((value % 5) == 0) and ((value % 3) == 0):\n    print(\"fizz buzz\")\nelif (value % 3) == 0:\n    print(\"fizz\")\nelif (value % 5) == 0:\n    print(\"buzz\")\nelse:\n    print(value)\n\nfizz buzz\n\n\nNow wrap it in a for loop to test all numbers between 1 and 100.\n\nfor value in range(1, 101):\n    if ((value % 5) == 0) and ((value % 3) == 0):\n        print(\"fizz buzz\")\n    elif (value % 3) == 0:\n        print(\"fizz\")\n    elif (value % 5) == 0:\n        print(\"buzz\")\n    else:\n        print(value)\n\n1\n2\nfizz\n4\nbuzz\nfizz\n7\n8\nfizz\nbuzz\n11\nfizz\n13\n14\nfizz buzz\n16\n17\nfizz\n19\nbuzz\nfizz\n22\n23\nfizz\nbuzz\n26\nfizz\n28\n29\nfizz buzz\n31\n32\nfizz\n34\nbuzz\nfizz\n37\n38\nfizz\nbuzz\n41\nfizz\n43\n44\nfizz buzz\n46\n47\nfizz\n49\nbuzz\nfizz\n52\n53\nfizz\nbuzz\n56\nfizz\n58\n59\nfizz buzz\n61\n62\nfizz\n64\nbuzz\nfizz\n67\n68\nfizz\nbuzz\n71\nfizz\n73\n74\nfizz buzz\n76\n77\nfizz\n79\nbuzz\nfizz\n82\n83\nfizz\nbuzz\n86\nfizz\n88\n89\nfizz buzz\n91\n92\nfizz\n94\nbuzz\nfizz\n97\n98\nfizz\nbuzz\n\n\n\n\nwhile loop Example\nLet’s do a quick interactive number guessing game. We’ll use the input function to ask the user for an input.\nA while loop allows us to keep executing until the user guesses correctly and we update the condition.\n\ninput(\"give me a value \")\n\ngive me a value 514kjdaf\n\n\n'514kjdaf'\n\n\n\ntruth = 18\ncorrect = False\nwhile not correct:\n    guess = int(input(\"Give me an integer value between 1 and 20: \"))\n    if guess == truth:\n        print(\"Way to go!\")\n        correct = True\n    elif guess &lt; truth:\n        print(\"Something larger\")\n    elif guess &gt; truth:\n        print(\"something smaller\")\n\nGive me an integer value between 1 and 20: 1\nSomething larger\nGive me an integer value between 1 and 20: 20\nsomething smaller\nGive me an integer value between 1 and 20: 19\nsomething smaller\nGive me an integer value between 1 and 20: 18\nWay to go!"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#lists-and-tuples-practice",
    "href": "01_Programming_in_python/Learning_Python.html#lists-and-tuples-practice",
    "title": "Week 1",
    "section": "Lists and Tuples Practice",
    "text": "Lists and Tuples Practice\nList comprehensions can be confusing at first. Let’s do a little practice. First, let’s write a list comprehension to return a list with values 0, 1, …, 9.\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n[x for x in range(10)]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nOk, that is a bit silly but we can do more with this. For instance, we can return \\(x^2\\) instead of just \\(x\\).\n\n[x**2 for x in range(10)]\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nWe can also return more than one value if we return a list! Let’s return \\(x\\), \\(x+1\\), and \\(x+2\\).\n\n[[x, x+1, x+2] for x in range(10)]\n\n[[0, 1, 2],\n [1, 2, 3],\n [2, 3, 4],\n [3, 4, 5],\n [4, 5, 6],\n [5, 6, 7],\n [6, 7, 8],\n [7, 8, 9],\n [8, 9, 10],\n [9, 10, 11]]\n\n\n:Next we’ll consider the map() function, which allows us to apply a function to every element of list (along with other things we’ll look at later!).\nConsider applying the sum() function to the first (0th) and second (1st) elements of our list. This should sum the subsequent lists.\n\ny = [[x, x+1, x+2] for x in range(10)]\n\n\ny[0]\n\n[0, 1, 2]\n\n\n\nsum(y[0])\n\n3\n\n\nTo do this to every list element would be tedious. Instead we can map() the sum() function to each element.\n\nmap(sum, y)\n\n&lt;map at 0x7d5be5260b20&gt;\n\n\nThis doesn’t actually return the values! Instead this is an iterator that allows us to find the values when needed.\nAn easy way to get at these is to just wrap the map in a list().\n\nz = list(map(sum, y))\nz\n\n[3, 6, 9, 12, 15, 18, 21, 24, 27, 30]\n\n\nNext we consider the zip() function, which is quite useful for looking at multiple lists (and other data structures) at once. Here we have two lists of the same length, which we can check with the code below.\n\nprint((len(z), len(y)))\nzip(z, y)\n\n(10, 10)\n\n\n&lt;zip at 0x7d5be521bb80&gt;\n\n\nNow we can write a for loop to iterate over the 0th (first) element of each list at the same time, then the 1st (second) element, …, etc.\n\nfor i, j in zip(z, y):\n    print(i, j)\n\n3 [0, 1, 2]\n6 [1, 2, 3]\n9 [2, 3, 4]\n12 [3, 4, 5]\n15 [4, 5, 6]\n18 [5, 6, 7]\n21 [6, 7, 8]\n24 [7, 8, 9]\n27 [8, 9, 10]\n30 [9, 10, 11]\n\n\nThis can be done via a list comprehension as well!\n\n[[i,j] for i, j in zip(z,y)]\n\n[[3, [0, 1, 2]],\n [6, [1, 2, 3]],\n [9, [2, 3, 4]],\n [12, [3, 4, 5]],\n [15, [4, 5, 6]],\n [18, [5, 6, 7]],\n [21, [6, 7, 8]],\n [24, [7, 8, 9]],\n [27, [8, 9, 10]],\n [30, [9, 10, 11]]]\n\n\n\n[[i,j] for i, j in zip(z,y) if i &gt;12]\n\n[[15, [4, 5, 6]],\n [18, [5, 6, 7]],\n [21, [6, 7, 8]],\n [24, [7, 8, 9]],\n [27, [8, 9, 10]],\n [30, [9, 10, 11]]]"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#dictionaries",
    "href": "01_Programming_in_python/Learning_Python.html#dictionaries",
    "title": "Week 1",
    "section": "Dictionaries",
    "text": "Dictionaries\nLet’s construct a dictionary from a tuple and a list. The tuple will act as the keys for our dictionary and the list as the values corresponding to those keys. (We could use lists for both!)\n\nkeys = tuple(x for x in \"abcdef\")\nkeys\n\n('a', 'b', 'c', 'd', 'e', 'f')\n\n\nNow we’ll use another list comprehension to return a list where each element is a list itself of length four based on different mathematical functions.\n\nimport math\nvalues = [[x, pow(x, 3), math.exp(x), x + 10] for x in range(10, 16)]\nvalues\n\n[[10, 1000, 22026.465794806718, 20],\n [11, 1331, 59874.14171519782, 21],\n [12, 1728, 162754.79141900392, 22],\n [13, 2197, 442413.3920089205, 23],\n [14, 2744, 1202604.2841647768, 24],\n [15, 3375, 3269017.3724721107, 25]]\n\n\nUsing the zip() function on the tuple an dlist, we can easily create a dictionary using the dict() function.\n\nmy_dict = dict(zip(keys, values))\nmy_dict\n\n{'a': [10, 1000, 22026.465794806718, 20],\n 'b': [11, 1331, 59874.14171519782, 21],\n 'c': [12, 1728, 162754.79141900392, 22],\n 'd': [13, 2197, 442413.3920089205, 23],\n 'e': [14, 2744, 1202604.2841647768, 24],\n 'f': [15, 3375, 3269017.3724721107, 25]}\n\n\n\nmy_dict[\"a\"]\n\n[10, 1000, 22026.465794806718, 20]\n\n\nWhat happens if we convert this dictionary into a list?\n\nlist(my_dict)\n\n['a', 'b', 'c', 'd', 'e', 'f']\n\n\n\nlist(my_dict.values())\n\n[[10, 1000, 22026.465794806718, 20],\n [11, 1331, 59874.14171519782, 21],\n [12, 1728, 162754.79141900392, 22],\n [13, 2197, 442413.3920089205, 23],\n [14, 2744, 1202604.2841647768, 24],\n [15, 3375, 3269017.3724721107, 25]]\n\n\n\nlist(zip(list(my_dict), list(my_dict.values())))\n\n[('a', [10, 1000, 22026.465794806718, 20]),\n ('b', [11, 1331, 59874.14171519782, 21]),\n ('c', [12, 1728, 162754.79141900392, 22]),\n ('d', [13, 2197, 442413.3920089205, 23]),\n ('e', [14, 2744, 1202604.2841647768, 24]),\n ('f', [15, 3375, 3269017.3724721107, 25])]\n\n\nWe can use any immutable object as our keys. Below we’ll use a tuple as one of our keys and see how to reference that key.\n\nmy_dict[(1, \"this\")] = \"this is a string\"\n\n\nmy_dict\n\n{'a': [10, 1000, 22026.465794806718, 20],\n 'b': [11, 1331, 59874.14171519782, 21],\n 'c': [12, 1728, 162754.79141900392, 22],\n 'd': [13, 2197, 442413.3920089205, 23],\n 'e': [14, 2744, 1202604.2841647768, 24],\n 'f': [15, 3375, 3269017.3724721107, 25],\n (1, 'this'): 'this is a string'}\n\n\nA really useful operator is the in operator. This easily allows us to see if a key exists in a dictionary.\n\n\"a\" in my_dict\n\nTrue\n\n\n\n\"o\" in my_dict\n\nFalse\n\n\n\n\"o\" not in my_dict\n\nTrue"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#numpy",
    "href": "01_Programming_in_python/Learning_Python.html#numpy",
    "title": "Week 1",
    "section": "NumPy",
    "text": "NumPy\nWe’ll load in the numpy module and create a two-dimensional array using random values from a standard normal distribution.\n\nimport numpy as np\nrng = np.random.default_rng(10)\nvalues = rng.standard_normal(30)\nvalues\n\narray([-1.10333845, -0.72502464, -0.78180526,  0.26697586, -0.24858073,\n        0.12648305,  0.84304257,  0.85793655,  0.47518364, -0.4507686 ,\n       -0.75493228, -0.81481411, -0.34385486, -0.05138009, -0.97227368,\n       -1.13448753,  0.30570522, -1.85168503, -0.17705351,  0.42582567,\n       -0.98535561, -1.11295413, -0.76062603,  0.64802459, -0.12983136,\n       -1.86959723, -0.42334911,  1.0138968 ,  0.98371534,  0.63004195])\n\n\n\nvalues.shape\n\n(30,)\n\n\n\nvalues.reshape((15, 2))\n\narray([[-1.10333845, -0.72502464],\n       [-0.78180526,  0.26697586],\n       [-0.24858073,  0.12648305],\n       [ 0.84304257,  0.85793655],\n       [ 0.47518364, -0.4507686 ],\n       [-0.75493228, -0.81481411],\n       [-0.34385486, -0.05138009],\n       [-0.97227368, -1.13448753],\n       [ 0.30570522, -1.85168503],\n       [-0.17705351,  0.42582567],\n       [-0.98535561, -1.11295413],\n       [-0.76062603,  0.64802459],\n       [-0.12983136, -1.86959723],\n       [-0.42334911,  1.0138968 ],\n       [ 0.98371534,  0.63004195]])\n\n\n\nvalues.reshape((15, 2)).shape\n\n(15, 2)\n\n\n\nmy_array = values.reshape((15,2))\nmy_array.base\n\narray([-1.10333845, -0.72502464, -0.78180526,  0.26697586, -0.24858073,\n        0.12648305,  0.84304257,  0.85793655,  0.47518364, -0.4507686 ,\n       -0.75493228, -0.81481411, -0.34385486, -0.05138009, -0.97227368,\n       -1.13448753,  0.30570522, -1.85168503, -0.17705351,  0.42582567,\n       -0.98535561, -1.11295413, -0.76062603,  0.64802459, -0.12983136,\n       -1.86959723, -0.42334911,  1.0138968 ,  0.98371534,  0.63004195])\n\n\nWe can iterate over values in an array using nditer(). Be careful here: the iteration doesn’t always go through the values in the same pattern as it relies on the memory layout of the array. You can change this behavior though. See https://numpy.org/doc/stable/reference/arrays.nditer.html#arrays-nditer for a discussion and information on how to specify the order.\n\nfor i in np.nditer(my_array):\n    print(math.exp(i))\n\n0.33176166344167046\n0.4843126352266208\n0.45757921701143867\n1.306008914185862\n0.7799068968474736\n1.1348302173565885\n2.3234254197779323\n2.3582894560408887\n1.6083095234626505\n0.6371382601360868\n0.4700424440698853\n0.44272161841215146\n0.709031831025442\n0.9499175441491969\n0.3782221046899628\n0.3215868817920055\n1.3575820587101297\n0.15697244006051864\n0.8377349576565134\n1.5308538739450661\n0.37330645316600336\n0.32858683744892436\n0.4673737442809796\n1.911760583275141\n0.8782435285725788\n0.1541857502671811\n0.6548499786487098\n2.7563209456441498\n2.6743740273453316\n1.877689348050752\n\n\nMany of the functions we’ll apply to arrays are ufuncs or universal functions. These act on an array without requiring the use of explicit looping. They tend to be computationally superior to looping so their use is encouraged! Here we apply the \\(e\\) function to each element of our array y.\n\nnp.exp(my_array)\n\narray([[0.33176166, 0.48431264],\n       [0.45757922, 1.30600891],\n       [0.7799069 , 1.13483022],\n       [2.32342542, 2.35828946],\n       [1.60830952, 0.63713826],\n       [0.47004244, 0.44272162],\n       [0.70903183, 0.94991754],\n       [0.3782221 , 0.32158688],\n       [1.35758206, 0.15697244],\n       [0.83773496, 1.53085387],\n       [0.37330645, 0.32858684],\n       [0.46737374, 1.91176058],\n       [0.87824353, 0.15418575],\n       [0.65484998, 2.75632095],\n       [2.67437403, 1.87768935]])\n\n\nIf you haven’t seen matrices and the matrix representation of the linear model, you can ignore all of this and just see how we are doing some array computations below.\nLet’s fit a simple linear regression model! Suppose we have a 1D response \\(y\\) and a single predictor (call the vector \\(x\\)). Then we can find the usual least squares regression line using arrays!\nDefine \\[y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_n\\end{pmatrix}\\mbox{ our response}\\] \\[x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{pmatrix}\\mbox{ our predictor}\\] We can consider the simple linear regression model: \\[y_i = \\beta_0+\\beta_1 x_i + E_i\\] which can be fit by minimizing the sum of squared residuals. Let \\(\\hat{y}_i = \\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) denote the prediction for the \\(i^{th}\\) observation. Then we want to find the \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize \\[SS(E) = \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\\] This can be minimized by considering the matrix view of the model: Define \\[X = \\begin{pmatrix} 1 & x_1\\\\ 1 & x_2 \\\\ ... \\\\ 1 & x_n\\end{pmatrix}\\] and \\[\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1\\end{pmatrix}\\] Then using matrix multiplication \\[X\\beta = \\begin{pmatrix} \\beta_0*1 + \\beta_1*x_1\\\\\\beta_0*1 + \\beta_1*x_2\\\\...\\\\\\beta_0*1+\\beta_1*x_n\\end{pmatrix}\\] Our model can be written as\n\\[y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_n\\end{pmatrix} = \\begin{pmatrix} \\beta_0*1 + \\beta_1*x_1\\\\\\beta_0*1 + \\beta_1*x_2\\\\...\\\\\\beta_0*1+\\beta_1*x_n\\end{pmatrix} + \\begin{pmatrix} E_1 \\\\ E_2 \\\\ ... \\\\ E_n\\end{pmatrix}\\]\nor \\[y=X\\beta + E\\]\nThen minimizing the \\(SS(E)\\) can be equivalently written as minimizing over the \\(\\beta\\) vector in \\[(y-X\\beta)^T(y-X\\beta)\\] The solution to this equation is \\[\\hat{\\beta} = \\begin{pmatrix}\\hat{\\beta_0} \\\\\\hat{\\beta_1} \\end{pmatrix} = \\left(X^TX\\right)^{-1}X^Ty\\]\n\nx = np.array([3, 2, 6, 10, 11.2])     #x values\ny = -2 + 4*x + rng.standard_normal(5) #create SLR model based response\ny\n\narray([ 9.76194119,  4.15506012, 22.16957773, 37.82402224, 42.87679986])\n\n\nOk, we want to now fit an SLR model between \\(y\\) and \\(x\\) and find estimates of the slope and intercept (true values are -2 and 4).\nLet’s create our ‘design matrix’ by zip 1’s (for the intercept) together with our x values.\n\nX = np.array(list(zip(np.ones(5), x)))\nX\n\narray([[ 1. ,  3. ],\n       [ 1. ,  2. ],\n       [ 1. ,  6. ],\n       [ 1. , 10. ],\n       [ 1. , 11.2]])\n\n\nWe’ll need the transpose of this matrix.\n\nX.transpose()\n\narray([[ 1. ,  1. ,  1. ,  1. ,  1. ],\n       [ 3. ,  2. ,  6. , 10. , 11.2]])\n\n\nAnd we need to multiply these matrices together.\n\nnp.matmul(X.transpose(), X)\n\narray([[  5.  ,  32.2 ],\n       [ 32.2 , 274.44]])\n\n\nLastly, we need to calculate an matrix inverse. We can do that with the np.linalg.inv() function.\n\nXTXinv = np.linalg.inv(np.matmul(X.transpose(), X))\nXTXinv\n\narray([[ 0.81834447, -0.09601622],\n       [-0.09601622,  0.01490935]])\n\n\nFinally, let’s get the least squares solution!\n\nnp.matmul(np.matmul(XTXinv, X.transpose()), y)\n\narray([-3.23545354,  4.12933754])\n\n\nPretty close to the actual values!"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#pandas-data-frames",
    "href": "01_Programming_in_python/Learning_Python.html#pandas-data-frames",
    "title": "Week 1",
    "section": "Pandas Data Frames",
    "text": "Pandas Data Frames\nWe’ll learn how to read in data using pandas shortly, but here we read in a data file to create a data frame to work with.\nThere is a dataset on the light measurement quality of cheese at: https://www4.stat.ncsu.edu/~online/datasets/cheese.csv\n\nimport pandas as pd\nimport numpy as np\ncheese = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/cheese.csv\")\n\nWe can use the .head() and .info() methods to inspect the data.\n\ncheese.head()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\n\n\n1\n26\n2\n51.52\n6.18\n17.09\n\n\n2\n26\n3\n52.69\n6.09\n17.59\n\n\n3\n26\n4\n52.06\n6.36\n17.50\n\n\n4\n26\n5\n51.63\n6.13\n17.19\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ncheese.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 24 entries, 0 to 23\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   syrup   24 non-null     int64  \n 1   rep     24 non-null     int64  \n 2   L       24 non-null     float64\n 3   a       24 non-null     float64\n 4   b       24 non-null     float64\ndtypes: float64(3), int64(2)\nmemory usage: 1.1 KB\n\n\nWe can also easily add a variable by simply creating a new column reference. Here the value specified is replicated the appropriate amount of times to fill the column. We’ll look at this in more detail later.\n\ncheese[\"dummy\"] = \"no\"\n\n\ncheese.head()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nno\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nno\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nno\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nno\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can reorder the rows of the data frame using the .sort_values() method. Simply specify the column name (or names as a list).\n\ncheese.sort_values(\"a\")\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nno\n\n\n5\n26\n6\n52.73\n6.12\n17.50\nno\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nno\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nno\n\n\n10\n42\n5\n48.64\n6.30\n16.21\nno\n\n\n23\n62\n6\n47.88\n6.34\n15.64\nno\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nno\n\n\n7\n42\n2\n48.57\n6.42\n15.91\nno\n\n\n20\n62\n3\n47.35\n6.49\n15.70\nno\n\n\n22\n62\n5\n46.77\n6.66\n15.91\nno\n\n\n19\n62\n2\n46.66\n6.66\n16.30\nno\n\n\n8\n42\n3\n47.57\n6.84\n16.04\nno\n\n\n18\n62\n1\n45.99\n6.84\n15.68\nno\n\n\n11\n42\n6\n47.49\n6.91\n15.91\nno\n\n\n21\n62\n4\n45.83\n6.96\n15.61\nno\n\n\n9\n42\n4\n46.85\n6.97\n15.85\nno\n\n\n6\n42\n1\n47.21\n7.02\n16.00\nno\n\n\n17\n55\n6\n42.65\n7.55\n14.40\nno\n\n\n16\n55\n5\n42.12\n7.56\n14.03\nno\n\n\n13\n55\n2\n42.31\n7.59\n13.98\nno\n\n\n14\n55\n3\n42.31\n7.63\n14.42\nno\n\n\n15\n55\n4\n41.49\n7.66\n13.58\nno\n\n\n12\n55\n1\n41.43\n7.71\n13.74\nno\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ncheese.sort_values([\"syrup\", \"a\"])\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nno\n\n\n5\n26\n6\n52.73\n6.12\n17.50\nno\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nno\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nno\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nno\n\n\n10\n42\n5\n48.64\n6.30\n16.21\nno\n\n\n7\n42\n2\n48.57\n6.42\n15.91\nno\n\n\n8\n42\n3\n47.57\n6.84\n16.04\nno\n\n\n11\n42\n6\n47.49\n6.91\n15.91\nno\n\n\n9\n42\n4\n46.85\n6.97\n15.85\nno\n\n\n6\n42\n1\n47.21\n7.02\n16.00\nno\n\n\n17\n55\n6\n42.65\n7.55\n14.40\nno\n\n\n16\n55\n5\n42.12\n7.56\n14.03\nno\n\n\n13\n55\n2\n42.31\n7.59\n13.98\nno\n\n\n14\n55\n3\n42.31\n7.63\n14.42\nno\n\n\n15\n55\n4\n41.49\n7.66\n13.58\nno\n\n\n12\n55\n1\n41.43\n7.71\n13.74\nno\n\n\n23\n62\n6\n47.88\n6.34\n15.64\nno\n\n\n20\n62\n3\n47.35\n6.49\n15.70\nno\n\n\n19\n62\n2\n46.66\n6.66\n16.30\nno\n\n\n22\n62\n5\n46.77\n6.66\n15.91\nno\n\n\n18\n62\n1\n45.99\n6.84\n15.68\nno\n\n\n21\n62\n4\n45.83\n6.96\n15.61\nno\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can add some NaN or not a number values. These represent missing data values. For a discussion of why this isn’t None, see https://stackoverflow.com/questions/17534106/what-is-the-difference-between-nan-and-none\n\ncheese.loc[0:3, \"dummy\"] = np.nan\ncheese.head()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nNaN\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nNaN\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nNaN\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nNaN\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can then drop any rows that contain a NaN value (in any column) with .dropna(). This doesn’t overwrite the object.\n\ncheese.dropna()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n5\n26\n6\n52.73\n6.12\n17.50\nno\n\n\n6\n42\n1\n47.21\n7.02\n16.00\nno\n\n\n7\n42\n2\n48.57\n6.42\n15.91\nno\n\n\n8\n42\n3\n47.57\n6.84\n16.04\nno\n\n\n9\n42\n4\n46.85\n6.97\n15.85\nno\n\n\n10\n42\n5\n48.64\n6.30\n16.21\nno\n\n\n11\n42\n6\n47.49\n6.91\n15.91\nno\n\n\n12\n55\n1\n41.43\n7.71\n13.74\nno\n\n\n13\n55\n2\n42.31\n7.59\n13.98\nno\n\n\n14\n55\n3\n42.31\n7.63\n14.42\nno\n\n\n15\n55\n4\n41.49\n7.66\n13.58\nno\n\n\n16\n55\n5\n42.12\n7.56\n14.03\nno\n\n\n17\n55\n6\n42.65\n7.55\n14.40\nno\n\n\n18\n62\n1\n45.99\n6.84\n15.68\nno\n\n\n19\n62\n2\n46.66\n6.66\n16.30\nno\n\n\n20\n62\n3\n47.35\n6.49\n15.70\nno\n\n\n21\n62\n4\n45.83\n6.96\n15.61\nno\n\n\n22\n62\n5\n46.77\n6.66\n15.91\nno\n\n\n23\n62\n6\n47.88\n6.34\n15.64\nno\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ncheese.head()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nNaN\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nNaN\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nNaN\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nNaN\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can also fill in any NaN values using .fillna(). This also doesn’t overwrite the object.\n\ncheese.fillna(\"yes\")\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nyes\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nyes\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nyes\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nyes\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n5\n26\n6\n52.73\n6.12\n17.50\nno\n\n\n6\n42\n1\n47.21\n7.02\n16.00\nno\n\n\n7\n42\n2\n48.57\n6.42\n15.91\nno\n\n\n8\n42\n3\n47.57\n6.84\n16.04\nno\n\n\n9\n42\n4\n46.85\n6.97\n15.85\nno\n\n\n10\n42\n5\n48.64\n6.30\n16.21\nno\n\n\n11\n42\n6\n47.49\n6.91\n15.91\nno\n\n\n12\n55\n1\n41.43\n7.71\n13.74\nno\n\n\n13\n55\n2\n42.31\n7.59\n13.98\nno\n\n\n14\n55\n3\n42.31\n7.63\n14.42\nno\n\n\n15\n55\n4\n41.49\n7.66\n13.58\nno\n\n\n16\n55\n5\n42.12\n7.56\n14.03\nno\n\n\n17\n55\n6\n42.65\n7.55\n14.40\nno\n\n\n18\n62\n1\n45.99\n6.84\n15.68\nno\n\n\n19\n62\n2\n46.66\n6.66\n16.30\nno\n\n\n20\n62\n3\n47.35\n6.49\n15.70\nno\n\n\n21\n62\n4\n45.83\n6.96\n15.61\nno\n\n\n22\n62\n5\n46.77\n6.66\n15.91\nno\n\n\n23\n62\n6\n47.88\n6.34\n15.64\nno\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can get a lot of summary statistics easily from a data frame using the .describe() method.\n\ncheese.describe()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\n\n\n\n\ncount\n24.000000\n24.000000\n24.000000\n24.000000\n24.000000\n\n\nmean\n46.250000\n3.500000\n47.151667\n6.800417\n15.800417\n\n\nstd\n14.013193\n1.744557\n3.691321\n0.559895\n1.239907\n\n\nmin\n26.000000\n1.000000\n41.430000\n6.090000\n13.580000\n\n\n25%\n38.000000\n2.000000\n45.035000\n6.330000\n15.312500\n\n\n50%\n48.500000\n3.500000\n47.280000\n6.750000\n15.910000\n\n\n75%\n56.750000\n5.000000\n49.360000\n7.152500\n16.497500\n\n\nmax\n62.000000\n6.000000\n52.730000\n7.710000\n17.590000"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#reading-raw-data-with-pandas",
    "href": "01_Programming_in_python/Learning_Python.html#reading-raw-data-with-pandas",
    "title": "Week 1",
    "section": "Reading Raw Data with Pandas",
    "text": "Reading Raw Data with Pandas\nBelow we’ll check our working directory and then read in a SAS file using read_sas() from pandas.\n\nimport os\nos.getcwd()\n\n'/content'\n\n\n\npd.read_sas(\"air.sas7bdat\")\n\n\n  \n    \n\n\n\n\n\n\nDATE\nAIR\n\n\n\n\n0\n1949-01-01\n112.0\n\n\n1\n1949-02-01\n118.0\n\n\n2\n1949-03-01\n132.0\n\n\n3\n1949-04-01\n129.0\n\n\n4\n1949-05-01\n121.0\n\n\n...\n...\n...\n\n\n139\n1960-08-01\n606.0\n\n\n140\n1960-09-01\n508.0\n\n\n141\n1960-10-01\n461.0\n\n\n142\n1960-11-01\n390.0\n\n\n143\n1960-12-01\n432.0\n\n\n\n\n\n144 rows × 2 columns"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#numerical-summaries",
    "href": "01_Programming_in_python/Learning_Python.html#numerical-summaries",
    "title": "Week 1",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nContingency Tables\nWe’ll import pandas and read in a data set about NFL games. The data has information about games from 2002 to 2014. Each row represents one game and each column a variable measured on that game.\nAvailable here: https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\n\nimport pandas as pd\nscores = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\")\nscores.head()\n\n\n  \n    \n\n\n\n\n\n\nweek\ndate\nday\nseason\nawayTeam\nAQ1\nAQ2\nAQ3\nAQ4\nAOT\n...\nhomeFumLost\nhomeNumPen\nhomePenYds\nhome3rdConv\nhome3rdAtt\nhome4thConv\nhome4thAtt\nhomeTOP\nHminusAScore\nhomeSpread\n\n\n\n\n0\n1\n5-Sep\nThu\n2002\nSan Francisco 49ers\n3\n0\n7\n6\n-1\n...\n0\n10\n80\n4\n8\n0\n1\n32.47\n-3\n-4.0\n\n\n1\n1\n8-Sep\nSun\n2002\nMinnesota Vikings\n3\n17\n0\n3\n-1\n...\n1\n4\n33\n2\n6\n0\n0\n28.48\n4\n4.5\n\n\n2\n1\n8-Sep\nSun\n2002\nNew Orleans Saints\n6\n7\n7\n0\n6\n...\n0\n8\n85\n1\n6\n0\n1\n31.48\n-6\n6.0\n\n\n3\n1\n8-Sep\nSun\n2002\nNew York Jets\n0\n17\n3\n11\n6\n...\n1\n10\n82\n4\n8\n2\n2\n39.13\n-6\n-3.0\n\n\n4\n1\n8-Sep\nSun\n2002\nArizona Cardinals\n10\n3\n3\n7\n-1\n...\n0\n7\n56\n6\n10\n1\n2\n34.40\n8\n6.0\n\n\n\n\n\n5 rows × 82 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can use the pd.cut() function to take a numeric variable and chop it up into groups. Note: generally splitting a numeric variable into categories isn’t a great idea. You lose information!\n\npd.cut(scores.AtotalYds, bins = [0, 150, 250, 400, 700])\n\n0       (250, 400]\n1       (250, 400]\n2       (250, 400]\n3       (250, 400]\n4       (250, 400]\n           ...    \n3466    (250, 400]\n3467    (250, 400]\n3468    (250, 400]\n3469    (150, 250]\n3470    (250, 400]\nName: AtotalYds, Length: 3471, dtype: category\nCategories (4, interval[int64, right]): [(0, 150] &lt; (150, 250] &lt; (250, 400] &lt; (400, 700]]\n\n\nGreat, but we can improve the categories being produced for the category variable. We just need to specify the labels argument and have the same number of labels as the number of categories produced.\n\nscores[\"AtotalYdsCut\"] = pd.cut(scores.AtotalYds, bins = [0, 150, 250, 400, 700], labels = [\"Poor\", \"Ok\", \"Good\", \"Great\"])\n\nNow we can easily create a one-way table to display the number of occurrences of each category using the .value_counts() method.\n\nscores.AtotalYdsCut.value_counts()\n\nGood     2082\nGreat     706\nOk        611\nPoor       72\nName: AtotalYdsCut, dtype: int64\n\n\nLet’s create a similar variable for the away team total yards and create a two-table using the pd.crosstab() function.\n\nscores[\"HtotalYdsCut\"] = pd.cut(scores.HtotalYds, bins = [0, 150, 250, 400, 700], labels = [\"Poor\", \"Ok\", \"Good\", \"Great\"])\npd.crosstab(scores.HtotalYdsCut, scores.AtotalYdsCut)\n\n\n  \n    \n\n\n\n\n\nAtotalYdsCut\nPoor\nOk\nGood\nGreat\n\n\nHtotalYdsCut\n\n\n\n\n\n\n\n\nPoor\n0\n5\n20\n10\n\n\nOk\n4\n56\n282\n101\n\n\nGood\n43\n399\n1354\n396\n\n\nGreat\n25\n151\n426\n199\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nNumeric Variable Summaries with pd.pivot_table()\nIn addition to the methods/functions used in the notes, we can also create summary stats across groups using the pivot_table() function and the .agg() method.\nFirst let’s look at the pivot_table() function. We need to supply a data frame, the columns we want to summarize (as values), the grouping variable(s) (as index), and the aggregation function(s) (as aggfunc).\n\npd.pivot_table(scores, values = \"AtotalYds\", index = \"season\")\n\n\n  \n    \n\n\n\n\n\n\nAtotalYds\n\n\nseason\n\n\n\n\n\n2002\n324.161049\n\n\n2003\n308.247191\n\n\n2004\n321.161049\n\n\n2005\n308.378277\n\n\n2006\n316.449438\n\n\n2007\n321.906367\n\n\n2008\n318.760300\n\n\n2009\n323.977528\n\n\n2010\n329.734082\n\n\n2011\n341.655431\n\n\n2012\n347.089888\n\n\n2013\n340.685393\n\n\n2014\n343.310861\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can give more than one function if we specify it as a list.\n\npd.pivot_table(scores, values = \"AtotalYds\", index = \"season\", aggfunc = [\"mean\", \"median\", \"std\"])\n\n\n  \n    \n\n\n\n\n\n\nmean\nmedian\nstd\n\n\n\nAtotalYds\nAtotalYds\nAtotalYds\n\n\nseason\n\n\n\n\n\n\n\n2002\n324.161049\n327\n90.314948\n\n\n2003\n308.247191\n311\n88.807591\n\n\n2004\n321.161049\n318\n90.912791\n\n\n2005\n308.378277\n304\n82.750663\n\n\n2006\n316.449438\n323\n79.343217\n\n\n2007\n321.906367\n327\n82.208671\n\n\n2008\n318.760300\n317\n86.567603\n\n\n2009\n323.977528\n323\n86.360668\n\n\n2010\n329.734082\n327\n88.964809\n\n\n2011\n341.655431\n336\n86.608624\n\n\n2012\n347.089888\n352\n88.788882\n\n\n2013\n340.685393\n341\n83.003251\n\n\n2014\n343.310861\n343\n83.797760\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can also group by more than one variable by specify the variables as a list.\n\npd.pivot_table(scores, values = \"AtotalYds\", index = [\"season\", \"week\"], aggfunc = [\"mean\", \"median\", \"std\"])\n\n\n  \n    \n\n\n\n\n\n\n\nmean\nmedian\nstd\n\n\n\n\nAtotalYds\nAtotalYds\nAtotalYds\n\n\nseason\nweek\n\n\n\n\n\n\n\n2002\n1\n310.000000\n286.0\n70.386552\n\n\n10\n352.857143\n367.0\n76.175766\n\n\n11\n299.125000\n332.5\n98.892450\n\n\n12\n357.750000\n348.0\n97.722396\n\n\n13\n314.062500\n302.0\n104.683949\n\n\n...\n...\n...\n...\n...\n\n\n2014\n9\n344.769231\n364.0\n96.695531\n\n\nConfChamp\n257.500000\n257.5\n68.589358\n\n\nDivision\n367.250000\n363.0\n46.399533\n\n\nSuperBowl\n396.000000\n396.0\nNaN\n\n\nWildCard\n257.000000\n276.5\n133.434129\n\n\n\n\n\n273 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n.agg() method\nThe .agg() method gives us another way to create summary stats. We can create a variable and pass a tuple containing the column to summarize and the function to use when summarizing.\n\nscores.agg(max_AtotalYds = (\"AtotalYds\", \"max\"))\n\n\n  \n    \n\n\n\n\n\n\nAtotalYds\n\n\n\n\nmax_AtotalYds\n622\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\nWe can create multiple summaries easily by specifying more than one variable. These summaries also respect the groupby() groupings!\n\nscores.groupby(\"season\").agg(max_AtotalYds = (\"AtotalYds\", \"max\"))\n\n\n  \n    \n\n\n\n\n\n\nmax_AtotalYds\n\n\nseason\n\n\n\n\n\n2002\n591\n\n\n2003\n548\n\n\n2004\n605\n\n\n2005\n494\n\n\n2006\n536\n\n\n2007\n531\n\n\n2008\n564\n\n\n2009\n524\n\n\n2010\n592\n\n\n2011\n622\n\n\n2012\n583\n\n\n2013\n542\n\n\n2014\n596\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nscores \\\n.groupby(\"season\") \\\n.agg(max_AtotalYds = (\"AtotalYds\", \"max\"),\n     min_AtotalYds = (\"AtotalYds\", \"min\"))\n\n\n  \n    \n\n\n\n\n\n\nmax_AtotalYds\nmin_AtotalYds\n\n\nseason\n\n\n\n\n\n\n2002\n591\n47\n\n\n2003\n548\n96\n\n\n2004\n605\n26\n\n\n2005\n494\n113\n\n\n2006\n536\n104\n\n\n2007\n531\n104\n\n\n2008\n564\n125\n\n\n2009\n524\n72\n\n\n2010\n592\n67\n\n\n2011\n622\n137\n\n\n2012\n583\n119\n\n\n2013\n542\n103\n\n\n2014\n596\n78"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#more-function-writing",
    "href": "01_Programming_in_python/Learning_Python.html#more-function-writing",
    "title": "Week 1",
    "section": "More Function Writing",
    "text": "More Function Writing\nLet’s look at the use of a lambda function as the key to the sorted() function. We’ll use a toy data set we create with an id column that we want to sort by.\nWe can use list comprehensions to generate ids of the form ‘id#’ and some cost values we just make up. Then we’ll put those into a data frame to inspect.\n\nimport pandas as pd\nnums = [4, 10, 22, 5, 12, 1]\nids = [\"id\" + str(x) for x in nums]\ncost = [21, 32, 12, 0, 23, 43]\nprint(ids, cost)\n\n['id4', 'id10', 'id22', 'id5', 'id12', 'id1'] [21, 32, 12, 0, 23, 43]\n\n\nPut those into a data frame.\n\nmy_df = pd.DataFrame(cost, index = ids, columns = [\"cost\"])\nmy_df\n\n\n  \n    \n\n\n\n\n\n\ncost\n\n\n\n\nid4\n21\n\n\nid10\n32\n\n\nid22\n12\n\n\nid5\n0\n\n\nid12\n23\n\n\nid1\n43\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow let’s see how the sorted function acts on the ids column.\n\nsorted(my_df.index)\n\n['id1', 'id10', 'id12', 'id22', 'id4', 'id5']\n\n\nNot quite what we want! We can sort with a key. A lambda function can be used to make a quick function that will take the id column’s value and look at the 2nd element on, converting that to an int.\nPassing this as the function to use for the key will give us what we want!\n\nsorted(my_df.index, key = lambda x: int(x[2:]))\n\n['id1', 'id4', 'id5', 'id10', 'id12', 'id22']\n\n\nNow we can sort the data frame using this sorting.\n\nmy_df.loc[sorted(my_df.index, key = lambda x: int(x[2:]))]\n\n\n  \n    \n\n\n\n\n\n\ncost\n\n\n\n\nid1\n43\n\n\nid4\n21\n\n\nid5\n0\n\n\nid10\n32\n\n\nid12\n23\n\n\nid22\n12\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nInvestigating the LLN\nLet’s use our find_means() function to see the behavior of the sample mean when the sample size gets large samples.\nIn this case, we are generating data from a standard normal distribution. This is the population distribution. Since we are generating the data ourselves, we know that the actual mean of the population here is 0. In statistics we often try to estimate these population quantities. Our usual estimator of the population mean is the sample mean. Ideally our estimator should be observed really close to the truth (population value) when our sample size grows!\nA statistical theory called the Law of Large Numbers (LLN) says that the behavior of sample average type statistics should get closer to the population value as the sample size grows.\nWe can demonstrate this behavior by using our find_means() function for increasingly large random samples from the standard normal distribution! We know the true value is 0 so we can see how close it gets using our simulations.\nFirst, let’s create our find_means() function and bring in numpy as needed.\n\nimport numpy as np\nfrom numpy.random import default_rng\nrng = default_rng(3)\n\ndef find_means(*args, decimals = 4):\n    \"\"\"\n    Assume that args will be a bunch of numpy arrays (1D) or pandas series\n    \"\"\"\n    means = []\n    for x in args: #iterate over the tuple values\n        means.append(np.mean(x).round(decimals))\n    return means\n\nNow we can use map() to create samples of varying sizes easily!\n\nstandard_normal_data = map(rng.standard_normal, range(1, 51))\n\nNow we can pass that list using * to unpack it. The elements will be read into the unlimited positional argument we defined, args.\n\nfind_means(*standard_normal_data)\n\n[2.0409,\n -1.0688,\n -0.412,\n 0.0515,\n -0.4263,\n 0.1058,\n 0.5156,\n 0.2256,\n -0.5504,\n 0.3226,\n -0.5999,\n 0.0015,\n -0.0263,\n -0.0402,\n 0.1694,\n 0.3517,\n -0.0175,\n -0.0879,\n 0.5763,\n -0.0086,\n -0.3138,\n 0.0313,\n 0.3178,\n 0.1171,\n -0.2855,\n 0.0906,\n 0.0179,\n 0.1475,\n 0.3238,\n -0.0359,\n 0.091,\n 0.241,\n 0.1158,\n 0.0413,\n 0.0656,\n -0.0811,\n 0.2771,\n -0.1547,\n -0.1708,\n -0.151,\n -0.0666,\n 0.0119,\n 0.0339,\n 0.1983,\n -0.1047,\n -0.0413,\n 0.1733,\n 0.0081,\n 0.1325,\n -0.0272]"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#plotting-matplotlib",
    "href": "01_Programming_in_python/Learning_Python.html#plotting-matplotlib",
    "title": "Week 1",
    "section": "Plotting matplotlib",
    "text": "Plotting matplotlib\nLet’s take our function that was able to output many means from normal random samples and use it to make a plot! This will visualize the result we were seeing.\nLet’s first manually create the plot, then put the plot functionality into a function! We really have a sample size associated with each of these means. Let’s first create a data frame that has the sample size and the calculated mean.\n\nstandard_normal_data = map(rng.standard_normal, range(1, 51))\nmy_means = find_means(*standard_normal_data)\nmean_df = pd.DataFrame(zip(my_means, range(1,51)), columns = [\"means\", \"n\"])\nmean_df.head()\n\n\n  \n    \n\n\n\n\n\n\nmeans\nn\n\n\n\n\n0\n-1.4405\n1\n\n\n1\n0.2436\n2\n\n\n2\n0.1613\n3\n\n\n3\n-0.4510\n4\n\n\n4\n-0.3320\n5\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow import matplotlib.pyplot and let’s make a plot!\n\nimport matplotlib.pyplot as plt\nplt.scatter(mean_df.n, mean_df.means)\nplt.axhline(y = 0, color = 'r', linestyle = '-')\n\n&lt;matplotlib.lines.Line2D at 0x78731bb13ac0&gt;\n\n\n\n\n\nGreat, now let’s make a function that does this for us! The function will just take in a sample size and then produce the plot from that. Our find_means() function will be used as a helper function.\n\ndef plot_means(n = 50):\n  standard_normal_data = map(rng.standard_normal, range(1, n+1))\n  my_means = find_means(*standard_normal_data)\n  mean_df = pd.DataFrame(zip(my_means, range(1, n+1)), columns = [\"means\", \"n\"])\n  plt.scatter(mean_df.n, mean_df.means)\n  plt.axhline(y = 0, color = 'r', linestyle = '-')\n\n\nplot_means(10)\n\n\n\n\n\nplot_means(1000)"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#plotting-with-pandas",
    "href": "01_Programming_in_python/Learning_Python.html#plotting-with-pandas",
    "title": "Week 1",
    "section": "Plotting with pandas",
    "text": "Plotting with pandas\nLet’s revisit our NFL data example from previous. We’ll then look at line plots.\n\nimport pandas as pd\nscores = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\")\nscores.head()\n\n\n  \n    \n\n\n\n\n\n\nweek\ndate\nday\nseason\nawayTeam\nAQ1\nAQ2\nAQ3\nAQ4\nAOT\n...\nhomeFumLost\nhomeNumPen\nhomePenYds\nhome3rdConv\nhome3rdAtt\nhome4thConv\nhome4thAtt\nhomeTOP\nHminusAScore\nhomeSpread\n\n\n\n\n0\n1\n5-Sep\nThu\n2002\nSan Francisco 49ers\n3\n0\n7\n6\n-1\n...\n0\n10\n80\n4\n8\n0\n1\n32.47\n-3\n-4.0\n\n\n1\n1\n8-Sep\nSun\n2002\nMinnesota Vikings\n3\n17\n0\n3\n-1\n...\n1\n4\n33\n2\n6\n0\n0\n28.48\n4\n4.5\n\n\n2\n1\n8-Sep\nSun\n2002\nNew Orleans Saints\n6\n7\n7\n0\n6\n...\n0\n8\n85\n1\n6\n0\n1\n31.48\n-6\n6.0\n\n\n3\n1\n8-Sep\nSun\n2002\nNew York Jets\n0\n17\n3\n11\n6\n...\n1\n10\n82\n4\n8\n2\n2\n39.13\n-6\n-3.0\n\n\n4\n1\n8-Sep\nSun\n2002\nArizona Cardinals\n10\n3\n3\n7\n-1\n...\n0\n7\n56\n6\n10\n1\n2\n34.40\n8\n6.0\n\n\n\n\n\n5 rows × 82 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe saw we could get summaries of the data in a number of ways. One was the .agg() method.\n\nscores \\\n.groupby(\"season\") \\\n.agg(max_AtotalYds = (\"AtotalYds\", \"max\"),\n     min_AtotalYds = (\"AtotalYds\", \"min\"))\n\n\n  \n    \n\n\n\n\n\n\nmax_AtotalYds\nmin_AtotalYds\n\n\nseason\n\n\n\n\n\n\n2002\n591\n47\n\n\n2003\n548\n96\n\n\n2004\n605\n26\n\n\n2005\n494\n113\n\n\n2006\n536\n104\n\n\n2007\n531\n104\n\n\n2008\n564\n125\n\n\n2009\n524\n72\n\n\n2010\n592\n67\n\n\n2011\n622\n137\n\n\n2012\n583\n119\n\n\n2013\n542\n103\n\n\n2014\n596\n78\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nLine graphs are especially useful when you have data over time. For instance, the summary above is over season. We can put season on the x-axis and see how our summaries change over time. The plot.line() method is great for creating these types of plots. It prefers data where each column will be plotted and the index represents the time. That is exactly what we have outputted from above!\nLet’s add a mean and median variable for the AtotalYds and then create a line plot for these four variables.\n\nsummaries = scores \\\n.groupby(\"season\") \\\n.agg(max_AtotalYds = (\"AtotalYds\", \"max\"),\n     min_AtotalYds = (\"AtotalYds\", \"min\"),\n     mean_AtotalYds = (\"AtotalYds\", \"mean\"),\n     median_AtotalYds = (\"AtotalYds\", \"median\"))\nsummaries.head()\n\n\n  \n    \n\n\n\n\n\n\nmax_AtotalYds\nmin_AtotalYds\nmean_AtotalYds\nmedian_AtotalYds\n\n\nseason\n\n\n\n\n\n\n\n\n2002\n591\n47\n324.161049\n327.0\n\n\n2003\n548\n96\n308.247191\n311.0\n\n\n2004\n605\n26\n321.161049\n318.0\n\n\n2005\n494\n113\n308.378277\n304.0\n\n\n2006\n536\n104\n316.449438\n323.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nsummaries.plot()\n\n&lt;Axes: xlabel='season'&gt;"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#error-handling",
    "href": "01_Programming_in_python/Learning_Python.html#error-handling",
    "title": "Week 1",
    "section": "Error Handling",
    "text": "Error Handling\nLet’s use our find_means() function and add a try block to keep processing if one of our arguments passed isn’t appropriate.\n\ndef find_means(*args, decimals = 4):\n    \"\"\"\n    Assume that args will be a bunch of numpy arrays (1D) or pandas series\n    \"\"\"\n    means = []\n    for x in args: #iterate over the tuple values\n        means.append(np.mean(x).round(decimals))\n    return means\n\n\nfind_means(np.array([1,2,3,6, \"hi\"]))\n\nUFuncTypeError: ufunc 'add' did not contain a loop with signature matching types (dtype('&lt;U21'), dtype('&lt;U21')) -&gt; None\n\n\n\ndef find_means(*args, decimals = 4):\n    \"\"\"\n    Assume that args will be a bunch of numpy arrays (1D) or pandas series\n    \"\"\"\n    means = []\n    for x in args: #iterate over the tuple values\n        try:\n            means.append(np.mean(x).round(decimals))\n        except TypeError:\n            print(\"It appears that the data type is wrong!\")\n            means.append(np.nan)\n    return means\n\n\nfind_means(np.array([1,2,3,6, \"hi\"]))\n\nIt appears that the data type is wrong!\n\n\n[nan]\n\n\n\nfind_means(np.array([424,13,13]), np.array([1,2,3,6, \"hi\"]), np.array([\"yo\"]))\n\nIt appears that the data type is wrong!\nIt appears that the data type is wrong!\n\n\n[150.0, nan, nan]"
  },
  {
    "objectID": "02_Big_Data_Management/02-Role_of_Statistics_Landing.html",
    "href": "02_Big_Data_Management/02-Role_of_Statistics_Landing.html",
    "title": "The Role of Statistics in Big Data",
    "section": "",
    "text": "The video below describes ways in which statistics is used with big data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "02_Big_Data_Management/02-Role_of_Statistics_Landing.html#notes",
    "href": "02_Big_Data_Management/02-Role_of_Statistics_Landing.html#notes",
    "title": "The Role of Statistics in Big Data",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/04-SQL_Joins_Landing.html",
    "href": "02_Big_Data_Management/04-SQL_Joins_Landing.html",
    "title": "SQL Joins",
    "section": "",
    "text": "The video below discusses SQL joins. These actions are used to combine data across two or more tables in a database. These are extremely important and useful in practice!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe notebook written in the video is available here."
  },
  {
    "objectID": "02_Big_Data_Management/04-SQL_Joins_Landing.html#notes",
    "href": "02_Big_Data_Management/04-SQL_Joins_Landing.html#notes",
    "title": "SQL Joins",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/06-Data_Pipelines_Storage_Landing.html",
    "href": "02_Big_Data_Management/06-Data_Pipelines_Storage_Landing.html",
    "title": "Data Pipelines & Data Storage",
    "section": "",
    "text": "The video below describes common pipelines for ingesting and storing data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer."
  },
  {
    "objectID": "02_Big_Data_Management/06-Data_Pipelines_Storage_Landing.html#notes",
    "href": "02_Big_Data_Management/06-Data_Pipelines_Storage_Landing.html#notes",
    "title": "Data Pipelines & Data Storage",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThat wraps up our week 7 content! Head out our Moodle site and start on homework 5. This homework won’t focus on SQL (homework 6 will though!).\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/08-Connecting_to_JupyterHub.html",
    "href": "02_Big_Data_Management/08-Connecting_to_JupyterHub.html",
    "title": "Connecting to our JupyterHub",
    "section": "",
    "text": "We have a dedicated JupyterHub here in the college of sciences! The JupyterHub has pyspark enabled in it so we don’t have to set things up ourselves!\nTo connect:\n\nConnect to the NC State VPN.\n\nYou want to download Cisco AnyConnect VPN software.\nOnce installed, you need to login. See this site for info. Basically, you connect to vpn.ncsu.edu and login with your username and password. You also need to do a duo login (I usually use a push notification - type ‘push’ in as the second password)\nOnce connect, you log into the JupyterHub here.\nThen you can select a pyspark kernel!\n\n\nThe video below shows a quick walkthrough of this process.\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/10-pyspark_RDD_Landing.html",
    "href": "02_Big_Data_Management/10-pyspark_RDD_Landing.html",
    "title": "pyspark: Resilient Distributed Data Sets",
    "section": "",
    "text": "The video below describes the underlying data structure in spark called the resilient distributed data (RDD) data set. While we rarely utilize these data structures and their functionality exactly, it is useful to have an idea about RDDs and the functionality they have.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nupdate video"
  },
  {
    "objectID": "02_Big_Data_Management/10-pyspark_RDD_Landing.html#notes",
    "href": "02_Big_Data_Management/10-pyspark_RDD_Landing.html#notes",
    "title": "pyspark: Resilient Distributed Data Sets",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version"
  },
  {
    "objectID": "02_Big_Data_Management/10-pyspark_RDD_Landing.html#additional-readings-for-week-9",
    "href": "02_Big_Data_Management/10-pyspark_RDD_Landing.html#additional-readings-for-week-9",
    "title": "pyspark: Resilient Distributed Data Sets",
    "section": "Additional Readings for Week 9",
    "text": "Additional Readings for Week 9\n\npyspark\n\nQuick-start guide\nSQL API\npyspark SQL cheat sheet\npandas-on-Spark user guide\npandas-on-Spark example\n\n\n\nLooking for More?\n\nData Engineering Topics to Learn\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  },
  {
    "objectID": "02_Big_Data_Management/12-pyspark_Spark_SQL_Landing.html",
    "href": "02_Big_Data_Management/12-pyspark_Spark_SQL_Landing.html",
    "title": "pyspark: Spark SQL",
    "section": "",
    "text": "The video below discusses how pyspark can be used using its SQL style data frame and functionality.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nupdate video"
  },
  {
    "objectID": "02_Big_Data_Management/12-pyspark_Spark_SQL_Landing.html#notes",
    "href": "02_Big_Data_Management/12-pyspark_Spark_SQL_Landing.html#notes",
    "title": "pyspark: Spark SQL",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!"
  }
]