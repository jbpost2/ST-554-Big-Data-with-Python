[
  {
    "objectID": "04_Streaming_Data/06-Streaming_Joins_Landing.html",
    "href": "04_Streaming_Data/06-Streaming_Joins_Landing.html",
    "title": "Streaming Joins",
    "section": "",
    "text": "The video below discusses how we can join two streams, join a stream with a static data frame, and the requirements to do so!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe notebook used in the video is available here. You’ll need to download this .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.\nThe impressions and clicks data sets are available at https://www4.stat.ncsu.edu/online/datasets/.",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Streaming Joins"
    ]
  },
  {
    "objectID": "04_Streaming_Data/06-Streaming_Joins_Landing.html#notes",
    "href": "04_Streaming_Data/06-Streaming_Joins_Landing.html#notes",
    "title": "Streaming Joins",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThis ends the course! I hope you enjoyed it :)\nHead back to the Moodle site to work on your final project.",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Streaming Joins"
    ]
  },
  {
    "objectID": "04_Streaming_Data/04-Reading_Writing_Streams_With_Spark_Structured_Streaming_Landing.html",
    "href": "04_Streaming_Data/04-Reading_Writing_Streams_With_Spark_Structured_Streaming_Landing.html",
    "title": "Reading & Writing Streams with Spark Structured Streaming",
    "section": "",
    "text": "The video below goes into the syntax of reading and writing streams with Spark Structured Streaming.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe notebook used in the video is available here. You’ll need to download this .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Reading & Writing Streams with Spark Structured Streaming"
    ]
  },
  {
    "objectID": "04_Streaming_Data/04-Reading_Writing_Streams_With_Spark_Structured_Streaming_Landing.html#notes",
    "href": "04_Streaming_Data/04-Reading_Writing_Streams_With_Spark_Structured_Streaming_Landing.html#notes",
    "title": "Reading & Writing Streams with Spark Structured Streaming",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Reading & Writing Streams with Spark Structured Streaming"
    ]
  },
  {
    "objectID": "04_Streaming_Data/01-Streaming_Data_Concepts_Landing.html",
    "href": "04_Streaming_Data/01-Streaming_Data_Concepts_Landing.html",
    "title": "Streaming Data Concepts",
    "section": "",
    "text": "The video below discusses the broad ideas around handling streaming data. That is, data that comes in over time!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Streaming Data Concepts"
    ]
  },
  {
    "objectID": "04_Streaming_Data/01-Streaming_Data_Concepts_Landing.html#notes",
    "href": "04_Streaming_Data/01-Streaming_Data_Concepts_Landing.html#notes",
    "title": "Streaming Data Concepts",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Streaming Data Concepts"
    ]
  },
  {
    "objectID": "04_Streaming_Data/01-Streaming_Data_Concepts_Landing.html#additional-readings-for-weeks-14-15",
    "href": "04_Streaming_Data/01-Streaming_Data_Concepts_Landing.html#additional-readings-for-weeks-14-15",
    "title": "Streaming Data Concepts",
    "section": "Additional Readings for Weeks 14 & 15",
    "text": "Additional Readings for Weeks 14 & 15\n\nStreaming Data Ideas\n\nNice intro from kafka  about streaming data.\nStreaming data intro with some detail.\nDecent intro to streaming data ideas.  Stop reading at (Examples of modern streaming architectures on AWS).\nBasic overview of streaming data ideas.\n\n\n\nStreaming Data Tasks\n\nAn older article that discusses a list of common tasks with streaming data with examples.\n\n\n\nStreaming Data & Spark\n\nSpark Structured Streaming Overview\nMore on spark and kafka integration\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Streaming Data Concepts"
    ]
  },
  {
    "objectID": "03_Modeling_Data/12-MLflow_Landing.html",
    "href": "03_Modeling_Data/12-MLflow_Landing.html",
    "title": "MLflow",
    "section": "",
    "text": "The video below describes the use of MLflow to improve the model fitting process.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe pyspark code used in the notes is available in this notebook and the data set is also available online. You’ll need to download the .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "MLflow"
    ]
  },
  {
    "objectID": "03_Modeling_Data/12-MLflow_Landing.html#notes",
    "href": "03_Modeling_Data/12-MLflow_Landing.html#notes",
    "title": "MLflow",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "MLflow"
    ]
  },
  {
    "objectID": "03_Modeling_Data/10-MLLibBasics_Landing.html",
    "href": "03_Modeling_Data/10-MLLibBasics_Landing.html",
    "title": "Spark MLlib Basics",
    "section": "",
    "text": "The video below discusses how to use Spark MLlib through pyspark.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe pyspark code used in the notes is available in this notebook and the data set is also available online. You’ll need to download the .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Spark MLlib Basics"
    ]
  },
  {
    "objectID": "03_Modeling_Data/10-MLLibBasics_Landing.html#notes",
    "href": "03_Modeling_Data/10-MLLibBasics_Landing.html#notes",
    "title": "Spark MLlib Basics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Spark MLlib Basics"
    ]
  },
  {
    "objectID": "03_Modeling_Data/10-MLLibBasics_Landing.html#additional-readings",
    "href": "03_Modeling_Data/10-MLLibBasics_Landing.html#additional-readings",
    "title": "Spark MLlib Basics",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nMLlib Guide\n\nAvailable here!\nPython documentation (using the search in the upper left has been pretty useful)\n\n\n\nMLflow & MLOps\n\nhttps://mlflow.org/\nhttps://ml-ops.org/\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Spark MLlib Basics"
    ]
  },
  {
    "objectID": "03_Modeling_Data/08-Bagging_And_Random_Forests_Landing.html",
    "href": "03_Modeling_Data/08-Bagging_And_Random_Forests_Landing.html",
    "title": "Bagging & Random Forests",
    "section": "",
    "text": "The video below looks at two ensemble tree methods called bagging (although bagging is more general) and random forests. We see that bagging is a special case of a random forest.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Bagging & Random Forests"
    ]
  },
  {
    "objectID": "03_Modeling_Data/08-Bagging_And_Random_Forests_Landing.html#notes",
    "href": "03_Modeling_Data/08-Bagging_And_Random_Forests_Landing.html#notes",
    "title": "Bagging & Random Forests",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Bagging & Random Forests"
    ]
  },
  {
    "objectID": "03_Modeling_Data/06-Loss_Functions_and_Model_Metrics_Landing.html",
    "href": "03_Modeling_Data/06-Loss_Functions_and_Model_Metrics_Landing.html",
    "title": "Loss Functions & Model Metrics",
    "section": "",
    "text": "The video below gets further into the differences between loss functions and model metrics.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Loss Functions & Model Metrics"
    ]
  },
  {
    "objectID": "03_Modeling_Data/06-Loss_Functions_and_Model_Metrics_Landing.html#notes",
    "href": "03_Modeling_Data/06-Loss_Functions_and_Model_Metrics_Landing.html#notes",
    "title": "Loss Functions & Model Metrics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Loss Functions & Model Metrics"
    ]
  },
  {
    "objectID": "03_Modeling_Data/06-Loss_Functions_and_Model_Metrics_Landing.html#additional-readings",
    "href": "03_Modeling_Data/06-Loss_Functions_and_Model_Metrics_Landing.html#additional-readings",
    "title": "Loss Functions & Model Metrics",
    "section": "Additional Readings",
    "text": "Additional Readings\n\nModeling Readings\n\nIntrouction to Statistical Learning with Python\n\nChapter 2: section 2\nChapter 8: sections 1 and 2\n\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Loss Functions & Model Metrics"
    ]
  },
  {
    "objectID": "03_Modeling_Data/04-Logistic_Regression_Extensions_Landing.html",
    "href": "03_Modeling_Data/04-Logistic_Regression_Extensions_Landing.html",
    "title": "Logistic Regression Extensions",
    "section": "",
    "text": "The video below looks at how we can take the basic logistic regression model and make it more flexible.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Logistic Regression Extensions"
    ]
  },
  {
    "objectID": "03_Modeling_Data/04-Logistic_Regression_Extensions_Landing.html#notes",
    "href": "03_Modeling_Data/04-Logistic_Regression_Extensions_Landing.html#notes",
    "title": "Logistic Regression Extensions",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Logistic Regression Extensions"
    ]
  },
  {
    "objectID": "03_Modeling_Data/02-Modeling_Example_Landing.html",
    "href": "03_Modeling_Data/02-Modeling_Example_Landing.html",
    "title": "Modeling Example",
    "section": "",
    "text": "The video goes through an example of fitting a model and working with a training and test set to help remind of us those big topics.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe code used in the video is available in this notebook. You’ll need to download this .ipynb file and upload it to your JupyterHub environment (or run this in colab). You can use a python or pyspark kernel.\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Modeling Example"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/12-pyspark_Spark_SQL_Landing.html",
    "href": "02_Big_Data_Management/12-pyspark_Spark_SQL_Landing.html",
    "title": "pyspark: Spark SQL",
    "section": "",
    "text": "The video below discusses how pyspark can be used using its SQL style data frame and functionality.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe pyspark code used in the notes and the example done at the end of the notes is available in this notebook. You’ll need to download this .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.\nIf you are not an NC State student, you can download docker and gain access to Spark with a Jupyter notebook interface reasonably quickly!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`pyspark`: Spark SQL"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/12-pyspark_Spark_SQL_Landing.html#notes",
    "href": "02_Big_Data_Management/12-pyspark_Spark_SQL_Landing.html#notes",
    "title": "pyspark: Spark SQL",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`pyspark`: Spark SQL"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/10-pyspark_RDD_Landing.html",
    "href": "02_Big_Data_Management/10-pyspark_RDD_Landing.html",
    "title": "pyspark: Resilient Distributed Data Sets",
    "section": "",
    "text": "The video below describes the underlying data structure in spark called the resilient distributed data (RDD) data set. While we rarely utilize these data structures and their functionality exactly, it is useful to have an idea about RDDs and the functionality they have.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe pyspark code used in the notes and the example done at the end of the notes is available in this notebook. You’ll need to download this .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.\nIf you are not an NC State student, you can download docker and gain access to Spark with a Jupyter notebook interface reasonably quickly!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`pyspark`: Resilient Distributed Data Sets"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/10-pyspark_RDD_Landing.html#notes",
    "href": "02_Big_Data_Management/10-pyspark_RDD_Landing.html#notes",
    "title": "pyspark: Resilient Distributed Data Sets",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`pyspark`: Resilient Distributed Data Sets"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/10-pyspark_RDD_Landing.html#additional-readings-for-week-9",
    "href": "02_Big_Data_Management/10-pyspark_RDD_Landing.html#additional-readings-for-week-9",
    "title": "pyspark: Resilient Distributed Data Sets",
    "section": "Additional Readings for Week 9",
    "text": "Additional Readings for Week 9\n\npyspark\n\nQuick-start guide\nSQL API\npyspark SQL cheat sheet\npandas-on-Spark user guide\npandas-on-Spark example\n\n\n\nLooking for More?\n\nData Engineering Topics to Learn\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`pyspark`: Resilient Distributed Data Sets"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/08-Connecting_to_JupyterHub.html",
    "href": "02_Big_Data_Management/08-Connecting_to_JupyterHub.html",
    "title": "Connecting to our JupyterHub",
    "section": "",
    "text": "We have a dedicated JupyterHub here in the college of sciences! The JupyterHub has pyspark enabled in it so we don’t have to set things up ourselves!\nTo connect:\n\nConnect to the NC State VPN.\n\nYou want to download Cisco AnyConnect VPN software.\nOnce installed, you need to login. See this site for info. Basically, you connect to vpn.ncsu.edu and login with your username and password. You also need to do a duo login (I usually use a push notification - type ‘push’ in as the second password)\nOnce connect, you log into the JupyterHub here.\nThen you can select a pyspark kernel!\n\n\nThe video below shows a quick walkthrough of this process.\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Connecting to our JupyterHub"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/06-Data_Pipelines_Storage_Landing.html",
    "href": "02_Big_Data_Management/06-Data_Pipelines_Storage_Landing.html",
    "title": "Data Pipelines & Data Storage",
    "section": "",
    "text": "The video below describes common pipelines for ingesting and storing data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Data Pipelines & Data Storage"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/06-Data_Pipelines_Storage_Landing.html#notes",
    "href": "02_Big_Data_Management/06-Data_Pipelines_Storage_Landing.html#notes",
    "title": "Data Pipelines & Data Storage",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThat wraps up our week 7 content! Head out our Moodle site and start on homework 5. This homework won’t focus on SQL (homework 6 will though!).\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Data Pipelines & Data Storage"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/02-Role_of_Statistics_Landing.html",
    "href": "02_Big_Data_Management/02-Role_of_Statistics_Landing.html",
    "title": "The Role of Statistics in Big Data",
    "section": "",
    "text": "The video below describes ways in which statistics is used with big data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "The Role of Statistics in Big Data"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/02-Role_of_Statistics_Landing.html#notes",
    "href": "02_Big_Data_Management/02-Role_of_Statistics_Landing.html#notes",
    "title": "The Role of Statistics in Big Data",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "The Role of Statistics in Big Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/27-LASSO_Landing.html",
    "href": "01_Programming_in_python/27-LASSO_Landing.html",
    "title": "LASSO Models",
    "section": "",
    "text": "The video below describes the Least Angle Subset and Selection Operator (LASSO). This is a method for fitting an MLR type model with a penalty term involved. This clever penalty term shrinks the coefficients estimates towards zero, setting some coefficient estimates exactly to zero (hence doing variable selection). This model also introduces the notion of a tuning parameter. This tuning parameter must be selected and is usually done so using CV.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "LASSO Models"
    ]
  },
  {
    "objectID": "01_Programming_in_python/27-LASSO_Landing.html#notes",
    "href": "01_Programming_in_python/27-LASSO_Landing.html#notes",
    "title": "LASSO Models",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThis wraps up the material for week 5! Your first project (along with the exam this week) is the next big assessment.\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "LASSO Models"
    ]
  },
  {
    "objectID": "01_Programming_in_python/25-Cross_Validation_Landing.html",
    "href": "01_Programming_in_python/25-Cross_Validation_Landing.html",
    "title": "Cross Validation",
    "section": "",
    "text": "The video below talks about the idea of cross validation (CV). This is a method of evaluating a predictive model while not necessarily needing to split the data into a training and test set (although sometimes we will still do this and just apply CV on the training set).\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Cross Validation"
    ]
  },
  {
    "objectID": "01_Programming_in_python/25-Cross_Validation_Landing.html#notes",
    "href": "01_Programming_in_python/25-Cross_Validation_Landing.html#notes",
    "title": "Cross Validation",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Cross Validation"
    ]
  },
  {
    "objectID": "01_Programming_in_python/23-Fitting_Evaluating_SLR_Models_Landing.html",
    "href": "01_Programming_in_python/23-Fitting_Evaluating_SLR_Models_Landing.html",
    "title": "Fitting & Evaluating SLR Models",
    "section": "",
    "text": "The video below introduces the language and ideas around fitting and evaluating predictive models using the very basic simple linear regression (SLR) model as the example. This simple model is ubiquitous and is a decent starting point for modeling a quantitative response.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Fitting & Evaluating SLR Models"
    ]
  },
  {
    "objectID": "01_Programming_in_python/23-Fitting_Evaluating_SLR_Models_Landing.html#notes",
    "href": "01_Programming_in_python/23-Fitting_Evaluating_SLR_Models_Landing.html#notes",
    "title": "Fitting & Evaluating SLR Models",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Fitting & Evaluating SLR Models"
    ]
  },
  {
    "objectID": "01_Programming_in_python/13-EDA_Landing.html",
    "href": "01_Programming_in_python/13-EDA_Landing.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The video below discusses the common steps when investigating data. This is usually referred to as an exploratory data analysis (EDA).\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_Programming_in_python/13-EDA_Landing.html#notes",
    "href": "01_Programming_in_python/13-EDA_Landing.html#notes",
    "title": "Exploratory Data Analysis",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_Programming_in_python/13-EDA_Landing.html#additional-readings-for-week-3",
    "href": "01_Programming_in_python/13-EDA_Landing.html#additional-readings-for-week-3",
    "title": "Exploratory Data Analysis",
    "section": "Additional Readings for Week 3",
    "text": "Additional Readings for Week 3\nExploratory Data Analysis (EDA) is often the first step of dealing with data. However, EDA is somewhat of an art and something that you get better at with experience. Often people new to EDA don’t know what they should be looking for! Here are some articles that discuss different strategies for EDA.\n\nShopify page\nIBM page\nWikipedia page\nNIST page\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "01_Programming_in_python/01-Course_Goals_Landing.html",
    "href": "01_Programming_in_python/01-Course_Goals_Landing.html",
    "title": "Course Goals & Other Resources",
    "section": "",
    "text": "The video below discusses the course goals.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Course Goals & Other Resources"
    ]
  },
  {
    "objectID": "01_Programming_in_python/01-Course_Goals_Landing.html#notes",
    "href": "01_Programming_in_python/01-Course_Goals_Landing.html#notes",
    "title": "Course Goals & Other Resources",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Course Goals & Other Resources"
    ]
  },
  {
    "objectID": "01_Programming_in_python/01-Course_Goals_Landing.html#additional-readings-for-topic-1",
    "href": "01_Programming_in_python/01-Course_Goals_Landing.html#additional-readings-for-topic-1",
    "title": "Course Goals & Other Resources",
    "section": "Additional Readings for Topic 1",
    "text": "Additional Readings for Topic 1\n\nPIP Information (Optional)\nWe use Pip to install our modules when needed (although most of the ones we need are already available to us on Google Colab). Here is more information about pip:\n\npip documentation\nMore help on understanding the basics of pip\nUsing pip with Colab (very short)\n\n\n\nJupyterLab Info (Optional)\nThere are a lot of options for the IDE to use.  We’ll use JupyterLab as our programming environment for the course. We’ll start out in Colab, which is built on JupyterLab. JupyterLab Notebooks are replacing the traditional Jupyter Notebook. They really just give a bit more functionality than the traditional notebook!\n\nJupyterLab getting started guide\nNote that these notebooks can be used with an R kernel (that is, you can use them to run R code!)\nMarkdown guide\n\n\n\nPython Help (Optional)\nWe’ll continue to learn python throughout the first couple of topics. You may want more detail or another explanation of concepts. The best place to learn about particular objects, functions, or methods is in the official documentation but that can be difficult to read when you are first starting out. Below are a number of tutorials that may be helpful!\n\nA nice tutorial from python.org (although sometimes they throw in some things that may be a bit out there for beginners).\nA good reference for python basics from w3schools. Generally w3schools is good to just get an idea about lots of languages! But overall the tutorials aren’t great.\nThis is a nice website but a little hard to navigate. I find that searching in the box for something (say ‘lists’) tends to bring up a pretty good article.\nGeeks for geeks is pretty solid!\nnumpy documentation\npandas documentation\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Course Goals & Other Resources"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/05-SQL_Resources.html",
    "href": "02_Big_Data_Management/05-SQL_Resources.html",
    "title": "SQL Resources",
    "section": "",
    "text": "There is a ton more to learn about SQL if you are interested. We’ll be using it a bit more to interact with Spark. Spark has a pandas-on-Spark API (way of interacting) and a Spark-SQL API. While pandas is generally easier, it isn’t as functional as the SQL interface.\nLuckily, we’ll mostly use SQL to rename variables, select data, filter observations, and maybe create a new observation or two. As such, it is useful to understand a bit more about it than we covered in lecture/readings.",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "SQL Resources"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/05-SQL_Resources.html#variable-types",
    "href": "02_Big_Data_Management/05-SQL_Resources.html#variable-types",
    "title": "SQL Resources",
    "section": "Variable Types",
    "text": "Variable Types\nWe briefly discussed the difference between python using None and SQLite using NULL. Generally, python data types are going to get transformed to SQLite (or, later, Spark) when we move data in and out.\nSQLite and Python types:\n\nSQLite natively supports the following types: \n\nNULL\nINTEGER\nREAL\nTEXT\nBLOB\n\nThe following Python types can be sent to SQLite and will be converted as follows:\n\n\n\n\nPython type\nSQLite type\n\n\n\n\nNone\nNULL\n\n\nint\nINTEGER\n\n\nfloat\nREAL\n\n\nstr (depends on text_factory)\nTEXT\n\n\nbytes\nBLOG",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "SQL Resources"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/05-SQL_Resources.html#sqlite-practice-non-graded",
    "href": "02_Big_Data_Management/05-SQL_Resources.html#sqlite-practice-non-graded",
    "title": "SQL Resources",
    "section": "SQLite Practice (Non-graded)",
    "text": "SQLite Practice (Non-graded)\nYou should\n\nhead to the sqlitetutorial.net site and read through the tutorials based around the SELECT statement (there are 7 or so listed on the page linked).\nthen read through the five ‘SQLite function’ pages (For example: https://www.sqlitetutorial.net/sqlite-avg/)\nLastly, for those that want more practice, head to datalemur.com and try some of the ‘easy’ tasks! This will help prepare you for the logic of our later SQL work in Spark!\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "SQL Resources"
    ]
  },
  {
    "objectID": "04_Streaming_Data/02-Common_Streaming_Tasks.html#common-issues-preprocessing-detecting-trends-counting-and-averages",
    "href": "04_Streaming_Data/02-Common_Streaming_Tasks.html#common-issues-preprocessing-detecting-trends-counting-and-averages",
    "title": "Common Streaming Tasks",
    "section": "Common Issues: Preprocessing, Detecting Trends, Counting, and Averages",
    "text": "Common Issues: Preprocessing, Detecting Trends, Counting, and Averages\nWhen we work with streaming data, the data pretty much always comes with some kind of time stamp or date-type data. That means we should first do a recap of how to deal with time data.\nThen we’ll discuss the basics of processing data and sending an alert or writing to a log.\nAs we often need to combine two streams, we’ll recap the basic ideas of joining two data frames.\nFinally, we take on bascis statistical summaries of streaming data such as - Counting events - Increasing and decreasing trends - Means and standard deviations - Summary stats over time windows",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Common Streaming Tasks"
    ]
  },
  {
    "objectID": "04_Streaming_Data/02-Common_Streaming_Tasks.html#air-quality-data",
    "href": "04_Streaming_Data/02-Common_Streaming_Tasks.html#air-quality-data",
    "title": "Common Streaming Tasks",
    "section": "Air Quality Data",
    "text": "Air Quality Data\nLet’s start with some data we can use to emulate some streaming concepts\nFrom: https://archive.ics.uci.edu/ml/datasets/Air+quality\n\n…dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device\n\nWe’ll read this data in. Note that the data has a Date and a Time column\n\nimport pandas as pd\nair_data = pd.read_csv(\"https://www4.stat.ncsu.edu/online/datasets/AirQualityUCI.csv\", sep = \";\", decimal = \",\")\nair_data\n\n\n  \n    \n\n\n\n\n\n\nDate\nTime\nCO(GT)\nPT08.S1(CO)\nNMHC(GT)\nC6H6(GT)\nPT08.S2(NMHC)\nNOx(GT)\nPT08.S3(NOx)\nNO2(GT)\nPT08.S4(NO2)\nPT08.S5(O3)\nT\nRH\nAH\nUnnamed: 15\nUnnamed: 16\n\n\n\n\n0\n10/03/2004\n18.00.00\n2.6\n1360.0\n150.0\n11.9\n1046.0\n166.0\n1056.0\n113.0\n1692.0\n1268.0\n13.6\n48.9\n0.7578\nNaN\nNaN\n\n\n1\n10/03/2004\n19.00.00\n2.0\n1292.0\n112.0\n9.4\n955.0\n103.0\n1174.0\n92.0\n1559.0\n972.0\n13.3\n47.7\n0.7255\nNaN\nNaN\n\n\n2\n10/03/2004\n20.00.00\n2.2\n1402.0\n88.0\n9.0\n939.0\n131.0\n1140.0\n114.0\n1555.0\n1074.0\n11.9\n54.0\n0.7502\nNaN\nNaN\n\n\n3\n10/03/2004\n21.00.00\n2.2\n1376.0\n80.0\n9.2\n948.0\n172.0\n1092.0\n122.0\n1584.0\n1203.0\n11.0\n60.0\n0.7867\nNaN\nNaN\n\n\n4\n10/03/2004\n22.00.00\n1.6\n1272.0\n51.0\n6.5\n836.0\n131.0\n1205.0\n116.0\n1490.0\n1110.0\n11.2\n59.6\n0.7888\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9466\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9467\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9468\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9469\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9470\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n9471 rows × 17 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nThe Date and Time columns can’t be handled easily. For instance, there is no way to determine the ‘day’ easily.\n\nair_data.Date.day #no attribute for day!\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-4-76fe55ca24a5&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 air_data.Date.day #no attribute for day!\n\n/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py in __getattr__(self, name)\n   6297         ):\n   6298             return self[name]\n-&gt; 6299         return object.__getattribute__(self, name)\n   6300 \n   6301     @final\n\nAttributeError: 'Series' object has no attribute 'day'",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Common Streaming Tasks"
    ]
  },
  {
    "objectID": "04_Streaming_Data/02-Common_Streaming_Tasks.html#dates-and-times-in-python",
    "href": "04_Streaming_Data/02-Common_Streaming_Tasks.html#dates-and-times-in-python",
    "title": "Common Streaming Tasks",
    "section": "Dates and Times in Python",
    "text": "Dates and Times in Python\nMost standard date/time operations can be handled via the datetime module. This module includes the following data types:\n\ndate: attributes of year, month, day\ntime: attributes of hour, minute, second, microsecond, and tzinfo\ndatetime: attributes of both date and time\ntimedelta: difference between two date, time or datetime instances\n\nWith this functionality we can add and subtract dates/times to get meaningful info while keeping the data in a more readable format (rather than say looking at the data as days since Jan 1, 1960)!\n\nDates and Times in pandas\nRather than using the datetime’s functionality, we’ll focus on how pandas handles date data via NumPy’s functionality.\n\npandas uses NumPy’s datetime64 and timedelta64 dtypes\n\nThese have very similar functionality for doing useful things with dates!\nWe saw that our columns that were supposed to be dates were read in as strings. We can coerce a string to a date/time variable.\nDate time variables have information on both the date and the time of day such as \"04-01-2022 10:00\". Of course we need to carefully specify whether this is April or January as different countries do this differently!\n\na = pd.to_datetime([\"04-01-2022 10:00\"], dayfirst=True)\na\n\nDatetimeIndex(['2022-01-04 10:00:00'], dtype='datetime64[ns]', freq=None)\n\n\nAs mentioned, date-time type variables have useful attributes we can pull from them.\n\na.day\n\nIndex([4], dtype='int32')\n\n\n\na.hour\n\nIndex([10], dtype='int32')\n\n\nThere are also useful methods to get descriptive names.\n\na.day_name()\n\nIndex(['Tuesday'], dtype='object')\n\n\n\na.month_name()\n\nIndex(['January'], dtype='object')\n\n\nAs mentioned, now that we have a date we can actually subtract two dates to get useful information (while still seeing the date in a nice format).\n\nb = pd.to_datetime([\"04-01-2022 11:00\"])\na-b\n\nTimedeltaIndex(['-88 days +23:00:00'], dtype='timedelta64[ns]', freq=None)\n\n\nWith our data, we can try to read in the columns appropriately with options on the pd.read_csv() function.\n\nair_data = pd.read_csv(\"https://www4.stat.ncsu.edu/online/datasets/AirQualityUCI.csv\",\n                       sep = \";\",\n                       decimal = \",\",\n                       parse_dates = [[\"Date\", \"Time\"]])\nair_data.info()\n\nFutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n  air_data = pd.read_csv(\"https://www4.stat.ncsu.edu/online/datasets/AirQualityUCI.csv\",\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9471 entries, 0 to 9470\nData columns (total 16 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Date_Time      9471 non-null   object \n 1   CO(GT)         9357 non-null   float64\n 2   PT08.S1(CO)    9357 non-null   float64\n 3   NMHC(GT)       9357 non-null   float64\n 4   C6H6(GT)       9357 non-null   float64\n 5   PT08.S2(NMHC)  9357 non-null   float64\n 6   NOx(GT)        9357 non-null   float64\n 7   PT08.S3(NOx)   9357 non-null   float64\n 8   NO2(GT)        9357 non-null   float64\n 9   PT08.S4(NO2)   9357 non-null   float64\n 10  PT08.S5(O3)    9357 non-null   float64\n 11  T              9357 non-null   float64\n 12  RH             9357 non-null   float64\n 13  AH             9357 non-null   float64\n 14  Unnamed: 15    0 non-null      float64\n 15  Unnamed: 16    0 non-null      float64\ndtypes: float64(15), object(1)\nmemory usage: 1.2+ MB\n\n\nUserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  air_data = pd.read_csv(\"https://www4.stat.ncsu.edu/online/datasets/AirQualityUCI.csv\",\n\n\nThis read those two columns in as a single column of the appropriate type! We now have all the nice functionality to deal with dates.\nLastly, let’s rename the CO(GT) column since we’ll use that one in the upcoming tasks and remove the bunch of missing rows at the end of the data frame.\n\nair_data = air_data.rename(columns = {'CO(GT)': 'co_gt'})\nair_data.dropna(subset=['co_gt'], inplace = True)",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Common Streaming Tasks"
    ]
  },
  {
    "objectID": "04_Streaming_Data/02-Common_Streaming_Tasks.html#preprocessing-sending-alerts",
    "href": "04_Streaming_Data/02-Common_Streaming_Tasks.html#preprocessing-sending-alerts",
    "title": "Common Streaming Tasks",
    "section": "Preprocessing & Sending alerts",
    "text": "Preprocessing & Sending alerts\nAs data comes in we often need to:\n\nCheck if the data is missing\nCheck if data is in an appropriate range\netc.\n\nIf whatever condition is not met we then want to print an alert, write the event to a file, send an email, etc.\nLater we’ll jump to pyspark and talk about how to handle actual streaming data. For now, let’s emulate having data streaming in using a simple for loop. We can think of each iteration of the loop as the next data value coming in.\nUsing our air_data object, let’s focus on the co_gt variable (true hourly averaged CO concentration (mg/m^3))\n\n‘Take data in over time’ (via a loop over the rows)\nIf the data exceeds 8 we print a message\n\n\nfor i in range(air_data.shape[0]):\n    if air_data.iloc[i].co_gt &gt; 8:\n        print(\"High CO Concentration at \" + str(air_data.Date_Time[i]) + \", loop\", str(i))\n\nHigh CO Concentration at 15/03/2004 09.00.00, loop 111\nHigh CO Concentration at 22/10/2004 18.00.00, loop 5424\nHigh CO Concentration at 25/10/2004 18.00.00, loop 5496\nHigh CO Concentration at 26/10/2004 17.00.00, loop 5519\nHigh CO Concentration at 26/10/2004 18.00.00, loop 5520\nHigh CO Concentration at 02/11/2004 20.00.00, loop 5690\nHigh CO Concentration at 04/11/2004 18.00.00, loop 5736\nHigh CO Concentration at 05/11/2004 17.00.00, loop 5759\nHigh CO Concentration at 17/11/2004 18.00.00, loop 6048\nHigh CO Concentration at 19/11/2004 19.00.00, loop 6097\nHigh CO Concentration at 19/11/2004 20.00.00, loop 6098\nHigh CO Concentration at 23/11/2004 18.00.00, loop 6192\nHigh CO Concentration at 23/11/2004 19.00.00, loop 6193\nHigh CO Concentration at 23/11/2004 20.00.00, loop 6194\nHigh CO Concentration at 23/11/2004 21.00.00, loop 6195\nHigh CO Concentration at 24/11/2004 20.00.00, loop 6218\nHigh CO Concentration at 26/11/2004 18.00.00, loop 6264\nHigh CO Concentration at 26/11/2004 21.00.00, loop 6267\nHigh CO Concentration at 02/12/2004 19.00.00, loop 6409\nHigh CO Concentration at 13/12/2004 18.00.00, loop 6672\nHigh CO Concentration at 14/12/2004 18.00.00, loop 6696\nHigh CO Concentration at 16/12/2004 19.00.00, loop 6745\nHigh CO Concentration at 16/12/2004 20.00.00, loop 6746\nHigh CO Concentration at 16/12/2004 21.00.00, loop 6747\nHigh CO Concentration at 23/12/2004 18.00.00, loop 6912\nHigh CO Concentration at 23/12/2004 19.00.00, loop 6913\nHigh CO Concentration at 23/12/2004 20.00.00, loop 6914\nHigh CO Concentration at 17/01/2005 18.00.00, loop 7512\nHigh CO Concentration at 17/01/2005 19.00.00, loop 7513\nHigh CO Concentration at 10/02/2005 20.00.00, loop 8090\n\n\nReasonably straight forward! Of course we might have some other tasks that we’d check for. We can use if/else logic for that.\n\n‘Take data in over time’ (via a loop over the rows)\nIf the data exceeds 8 we print a message\nIf the data is less than 0 we print a message (-200 represents missing here)\n\nWrite either occurrence to a log file (or perhaps a database) rather than printing to the console. Using the menus on the left, create a directory called logs to run the code below.\n\nfor i in range(air_data.shape[0]):\n    temp = air_data.iloc[i]\n    dt = temp.Date_Time\n    value = temp.co_gt\n    if value &gt; 8:\n        with open('logs/COHigh.txt', 'a') as f:\n            f.write(str(dt) + \", \" + str(value) + \"\\n\")\n    elif value &lt; 0:\n        with open('logs/COInvalid.txt', 'a') as f:\n            f.write(str(dt) + \", \" + str(value) + \"\\n\")\n\nWe may also want to check if something is ‘down’. This can mean a lot of things but if something is down, we might want to be notified. We can use python to send an email!\n\nUnfortunately, it seems that doing this with gmail isn’t doable at this point.\nI’ll follow this article!\n\nFirst we need to have the dotenv package. I’ll install this vai pip.\n\n!pip install dotenv\n\nCollecting dotenv\n  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\nCollecting python-dotenv (from dotenv)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, dotenv\nSuccessfully installed dotenv-0.9.9 python-dotenv-1.1.0\n\n\nNow let’s read in some module things.\n\nimport os\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.base import MIMEBase\nfrom email import encoders\nfrom dotenv import load_dotenv\n\nLet’s set up things to send the email. I’ve deleted the password here so you’ll need to create your own account and what-not to run this code!\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up email details from environment variables\nsender_email = \"MS_uW2WFe@trial-eqvygm0z9k8l0p7w.mlsender.net\"\npassword = \"removed\"\nsmtp_server = \"smtp.mailersend.net\"\nsmtp_port = \"587\"\n\n# Define the receiver email\nreceiver_email = \"st554testemail@gmail.com\"\n\nsubject = \"Sensor down!\"\n#we'll set up the body of the email in the loop\n\nNow we’ll simulate data coming in via our loop. I’m going to restrict the number of rows to cycle through to 1000 as I only want a couple of emails sent (there is a limited number of free ones on the account).\nIf we observe six missing observations in a row then we’ll send an email as a warning that something seems to be down. To do so:\n\nWe’ll initiate a missing variable at 0\nIf the value is -200 then we’ll add one to missing\nIf the value is not -200 then we reset missing to 0\nIf the value of missing becomes 6 then we send an email\n\n\n#initate the value\nmissing = 0\n\n#loop through the first 1000 ros\nfor i in range(1000):\n    #grab the row of interest\n    temp = air_data.iloc[i]\n    #get the date time and co values\n    dt = temp.Date_Time\n    value = temp.co_gt\n\n    #Check if the value is large, add to log, reset missing\n    if value &gt; 8:\n        with open('logs/COHigh.txt', 'a') as f:\n            f.write(str(dt) + \", \" + str(value) + \"\\n\")\n        missing = 0\n    #else, check if it is missing (&lt;0), add to log, increase missing\n    elif value &lt; 0:\n        with open('logs/COInvalid.txt', 'a') as f:\n            f.write(str(dt) + \", \" + str(value) + \"\\n\")\n        if value == -200:\n            missing += 1\n        #if missing is 6, send the email!\n        if missing == 6:\n            body = \"We have an issue at \" + str(dt) + \" to resolve!\"\n            # Create the email message\n            message = MIMEMultipart()\n            message[\"From\"] = sender_email\n            message[\"To\"] = receiver_email\n            message[\"Subject\"] = subject\n            # Attach the email body to the message\n            message.attach(MIMEText(body, \"plain\"))\n            # Establish a connection to the SMTP server and send the email\n            try:\n                # Connect to the SMTP server\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()  # Secure the connection\n                    server.login(sender_email, password)  # Log in to the SMTP server\n                    server.sendmail(\n                        sender_email, receiver_email, message.as_string()\n                    )  # Send the email\n                print(\"Email sent successfully\")\n            except Exception as e:\n                print(f\"Error: {e}\")\n    #if a reasonable value set missing to 0\n    else:\n        missing = 0\n\nEmail sent successfully\nEmail sent successfully\nEmail sent successfully\n\n\nChecking my email - it worked!",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Common Streaming Tasks"
    ]
  },
  {
    "objectID": "04_Streaming_Data/02-Common_Streaming_Tasks.html#combining-data-streams",
    "href": "04_Streaming_Data/02-Common_Streaming_Tasks.html#combining-data-streams",
    "title": "Common Streaming Tasks",
    "section": "Combining Data Streams",
    "text": "Combining Data Streams\n\nOften have multiple data streams that need to be combined\n\nUsually combined via a shared key or time stamps\n\nOnce combined we can then preprocess/summarize/etc.\n\nExample of streams using google ads type data.\n\nTo understand the important things for this example we need to define two terms:\n\nImpression - ad seen by a user\nClicks - ad was clicked on by user\n\nWe may get data each time we get an impression and each time we get a click. There will be more impressions than clicks of course.\nUsually there is a userID of some kind that we would want to join this data on. But essentially, once we have a unique ‘key’ or at least criteria for combining the streams the ideas are the same as SQL type joins.\nJust as an example, let’s simulate some fake impressions and clicks data with unique userIDs (there would likely be other information in each data stream that would come along as well).\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed(10)\nimpressions = pd.DataFrame({\n  'userId': range(500),\n  'impressionTime': (pd.to_datetime('2022-01-01') + pd.to_timedelta(np.random.rand(500), unit = \"D\")).sort_values()\n})\nimpressions.head()\n\n\n  \n    \n\n\n\n\n\n\nuserId\nimpressionTime\n\n\n\n\n0\n0\n2022-01-01 00:02:32.033682620\n\n\n1\n1\n2022-01-01 00:05:41.130210730\n\n\n2\n2\n2022-01-01 00:12:28.161050454\n\n\n3\n3\n2022-01-01 00:13:16.703184279\n\n\n4\n4\n2022-01-01 00:14:04.126023142\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nclicks = impressions.iloc[np.random.randint(size = 30, low = 0, high = 499)].sort_index()\n#add a slightly positive time delta to get the clicktime\nclicks['clickTime'] = clicks.impressionTime + pd.to_timedelta(np.random.rand(30)/100, unit = \"D\")\nclicks.drop(columns = 'impressionTime', axis = 1, inplace = True)\nclicks.head()\n\n\n  \n    \n\n\n\n\n\n\nuserId\nclickTime\n\n\n\n\n35\n35\n2022-01-01 01:30:34.800044013\n\n\n38\n38\n2022-01-01 01:37:35.836843295\n\n\n58\n58\n2022-01-01 02:42:47.536707642\n\n\n90\n90\n2022-01-01 04:10:54.616200590\n\n\n94\n94\n2022-01-01 04:13:45.397812575\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWe can “join” these too with the usual SQL join idea. Here we just use pd.merge(). We’ll use clicks as our right table and do a ‘right’ join to only return rows that match the clicks rows.\n\ncombined = pd.merge(left = impressions, right = clicks, on = \"userId\", how = 'right')\ncombined.head()\n\n\n  \n    \n\n\n\n\n\n\nuserId\nimpressionTime\nclickTime\n\n\n\n\n0\n35\n2022-01-01 01:24:48.442588485\n2022-01-01 01:30:34.800044013\n\n\n1\n38\n2022-01-01 01:33:27.963279930\n2022-01-01 01:37:35.836843295\n\n\n2\n58\n2022-01-01 02:30:39.145926778\n2022-01-01 02:42:47.536707642\n\n\n3\n90\n2022-01-01 04:02:01.489706816\n2022-01-01 04:10:54.616200590\n\n\n4\n94\n2022-01-01 04:11:43.920512629\n2022-01-01 04:13:45.397812575\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow we could find the time it took someone to click (of those that clicked).\n\n(combined.clickTime-combined.impressionTime).head()\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n0 days 00:05:46.357455528\n\n\n1\n0 days 00:04:07.873563365\n\n\n2\n0 days 00:12:08.390780864\n\n\n3\n0 days 00:08:53.126493774\n\n\n4\n0 days 00:02:01.477299946\n\n\n\n\ndtype: timedelta64[ns]\n\n\nIdea is easy :) Will be harder with actual data streams!",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Common Streaming Tasks"
    ]
  },
  {
    "objectID": "04_Streaming_Data/02-Common_Streaming_Tasks.html#finding-averages-standard-deviations",
    "href": "04_Streaming_Data/02-Common_Streaming_Tasks.html#finding-averages-standard-deviations",
    "title": "Common Streaming Tasks",
    "section": "Finding Averages & Standard Deviations",
    "text": "Finding Averages & Standard Deviations\nWe all know how to find averages and standard deviations. Given data \\(y_1,...,y_n\\)\n\\[\\bar{y} = \\frac{\\sum_{i=1}^{n}y_i}{n}\\] \\[s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(y_i-\\bar{y})^2} = \\sqrt{\\frac{1}{n-1}\\left(\\sum_{i=1}^{n}y_i^2-\\frac{(\\sum_{i=1}^{n}y_i)^2}{n}\\right)}\\]\nWhen dealing with streaming data we don’t have all the data to start with!\n\nFor the mean, we could just store the sum and the count\nUpdate each as new data comes in to find the new mean\nDon’t want to store sums as they can get very large over time!\n\n\nUpdating an Average\nWe can modify the formula for the sample mean to update it for a new observation:\n\\[\\bar{y}_n = \\bar{y}_{n-1} + \\frac{y_n-\\bar{y}_{n-1}}{n}\\]\n\nHere \\(\\bar{y}_{n-1}\\) is the sample mean we have with our first \\(n-1\\) observations\n\nLet’s write a quick function to do this in python!\n\ndef update_mean(new_data, old_mean, count):\n    return old_mean + (1/count)*(new_data-old_mean)\n\nLet’s apply it to our air_data.\n\nFirst we’ll remove missing values.\nThen we’ll initiate a mean value of 0.\nNow we’ll loop through the rows, again acting as though they are streaming in.\n\n\nair_data = air_data.loc[air_data.co_gt != -200].reset_index()\n\n#initialize the mean\no_mean = 0\nn = 1\nmeans = []\nfor i in range(air_data.shape[0]):\n    means.append(update_mean(air_data.co_gt[i], old_mean = o_mean, count = n))\n    o_mean = means[i]\n    n += 1\n\npd.DataFrame(zip(air_data.co_gt, means), columns = [\"Data\", \"Means\"])\n\n\n  \n    \n\n\n\n\n\n\nData\nMeans\n\n\n\n\n0\n2.6\n2.600000\n\n\n1\n2.0\n2.300000\n\n\n2\n2.2\n2.266667\n\n\n3\n2.2\n2.250000\n\n\n4\n1.6\n2.120000\n\n\n...\n...\n...\n\n\n7669\n3.1\n2.152686\n\n\n7670\n2.4\n2.152718\n\n\n7671\n2.4\n2.152750\n\n\n7672\n2.1\n2.152743\n\n\n7673\n2.2\n2.152750\n\n\n\n\n7674 rows × 2 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nUpdating a Standard Deviations\nUpdating the standard deviation is a bit more complicated but can be done!\n\\[s^2_n = \\frac{n-2}{n-1}s^2_{n-1}+\\frac{(y_n-\\bar{y}_{n-1})^2}{n}, n &gt; 1\\]\n\nHere we deal with the sample variance as we can just take the square root to then find the sample standard deviation\n\\(s^2_{n-1}\\) represents the sample variance using just the first \\(n-1\\) observations.\n\nLet’s create a function in python to update our sample variance.\n\ndef update_var(new_data, old_var, old_mean, count):\n    return ((count-2)/(count-1))*old_var + (new_data - old_mean)**2/count\n\nWe can’t start looping through the data at \\(n=1\\) without some special cases. Here we’ll initialize some things and start the loop after a few observations.\n\n#set up things to start the loop\nn = 3\n#lists to capture the updated means and variances\nmeans = [air_data.co_gt[0], air_data.co_gt[0:2].mean()]\nvariances = [np.nan, air_data.co_gt[0:2].var()]\n\no_mean = means[1]\no_var = variances[1]\n\n#loop through the observations\nfor i in range(2, air_data.shape[0]):\n    means.append(update_mean(air_data.co_gt[i], old_mean = o_mean, count = n))\n    variances.append(update_var(air_data.co_gt[i], old_var = o_var, old_mean = o_mean, count = n))\n    o_mean = means[i]\n    o_var = variances[i]\n    n += 1\n\nNow we’ll find the standard deviation as well and zip() it together with the rolling summary stats!\n\npd.DataFrame(zip(air_data.co_gt, means, np.sqrt(np.array(variances)), np.array(variances)),\n  columns = [\"Data\", \"Means\", \"SDs\", \"Vars\"])\n\n\n  \n    \n\n\n\n\n\n\nData\nMeans\nSDs\nVars\n\n\n\n\n0\n2.6\n2.600000\nNaN\nNaN\n\n\n1\n2.0\n2.300000\n0.424264\n0.180000\n\n\n2\n2.2\n2.266667\n0.305505\n0.093333\n\n\n3\n2.2\n2.250000\n0.251661\n0.063333\n\n\n4\n1.6\n2.120000\n0.363318\n0.132000\n\n\n...\n...\n...\n...\n...\n\n\n7669\n3.1\n2.152686\n1.453625\n2.113026\n\n\n7670\n2.4\n2.152718\n1.453533\n2.112759\n\n\n7671\n2.4\n2.152750\n1.453441\n2.112491\n\n\n7672\n2.1\n2.152743\n1.453347\n2.112216\n\n\n7673\n2.2\n2.152750\n1.453252\n2.111941\n\n\n\n\n7674 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nFinding Averages & Standard Deviations in Selected Windows\nLuckily, if we want to find sample means and standard deviations in windows things aren’t too complicated.\nLet’s just worry about the sample mean.\n\nWe need to choose our time window, say the most recent 10 values\nStore the most recent 3 values and find the average\nWhen new data comes in, pop out the first value (oldest) and add the newest value\nRepeat!\n\n\nmy_values = []\nrolling_means = [np.nan, np.nan]\nwindow = 3\nfor i in range(air_data.shape[0]):\n    my_values.append(air_data.co_gt[i])\n    if i &lt; window-1:\n        continue\n    rolling_means.append(np.mean(my_values))\n    my_values = my_values[1:]\n\nWe can compare what we calculated to the pandas functionality:\n\npd.DataFrame(zip(air_data.rolling(3).co_gt.mean(), rolling_means), columns = [\"pandas\", \"us\"]).head()\n\n\n  \n    \n\n\n\n\n\n\npandas\nus\n\n\n\n\n0\nNaN\nNaN\n\n\n1\nNaN\nNaN\n\n\n2\n2.266667\n2.266667\n\n\n3\n2.133333\n2.133333\n\n\n4\n2.000000\n2.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nSimilar functionality exists for rolling standard deviations.\n\nair_data.rolling(3).co_gt.std().head()\n\n\n\n\n\n\n\n\nco_gt\n\n\n\n\n0\nNaN\n\n\n1\nNaN\n\n\n2\n0.305505\n\n\n3\n0.115470\n\n\n4\n0.346410\n\n\n\n\ndtype: float64",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Common Streaming Tasks"
    ]
  },
  {
    "objectID": "03_Modeling_Data/MLLib_MLflow.html",
    "href": "03_Modeling_Data/MLLib_MLflow.html",
    "title": "Using MLlib from pyspark to Fit Machine Learning Models",
    "section": "",
    "text": "Start our spark session.\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/03/08 17:02:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\nNow read in a data set using pandas and convert it to a spark SQL style data frame.\n\nbike_data = pd.read_csv(\"bikeDetails.csv\")\nbike_data.head()\n\n\n\n\n\n\n\n\nname\nselling_price\nyear\nseller_type\nowner\nkm_driven\nex_showroom_price\n\n\n\n\n0\nRoyal Enfield Classic 350\n175000\n2019\nIndividual\n1st owner\n350\nNaN\n\n\n1\nHonda Dio\n45000\n2017\nIndividual\n1st owner\n5650\nNaN\n\n\n2\nRoyal Enfield Classic Gunmetal Grey\n150000\n2018\nIndividual\n1st owner\n12000\n148114.0\n\n\n3\nYamaha Fazer FI V 2.0 [2016-2018]\n65000\n2015\nIndividual\n1st owner\n23000\n89643.0\n\n\n4\nYamaha SZ [2013-2014]\n20000\n2011\nIndividual\n2nd owner\n21000\nNaN\n\n\n\n\n\n\n\nConvert to a spark SQL data frame.\n\nbike = spark.createDataFrame(bike_data)\nbike.show(5)\n\n                                                                                \n\n\n+--------------------+-------------+----+-----------+---------+---------+-----------------+\n|                name|selling_price|year|seller_type|    owner|km_driven|ex_showroom_price|\n+--------------------+-------------+----+-----------+---------+---------+-----------------+\n|Royal Enfield Cla...|       175000|2019| Individual|1st owner|      350|              NaN|\n|           Honda Dio|        45000|2017| Individual|1st owner|     5650|              NaN|\n|Royal Enfield Cla...|       150000|2018| Individual|1st owner|    12000|         148114.0|\n|Yamaha Fazer FI V...|        65000|2015| Individual|1st owner|    23000|          89643.0|\n|Yamaha SZ [2013-2...|        20000|2011| Individual|2nd owner|    21000|              NaN|\n+--------------------+-------------+----+-----------+---------+---------+-----------------+\nonly showing top 5 rows\n\n\n\nWe’ll fit a linear regression model using log selling price as our response and log km driven, year, and a 1st owner indicator variable as our predictors. This means we need to create some new columns and drop a bunch of others. - We also need to rename the response as ‘label’.\n- These first steps can easily be done using the SQLTransformer(), StringIndexer(), and Binarizer() functions from pyspark.ml.feature - Using the MLlib functions will allow us to place these transformations into a pipeline (which we’ll do shortly).\n\nfrom pyspark.ml.feature import SQLTransformer, StringIndexer, Binarizer, VectorAssembler\n\nFirst let’s try to create our dummy variable. This isn’t as easy as you’d like (nothing in here really is!). We can first take the string to a numeric index. Then we can binary-ize that!\nWe’ll use StringIndexer() first. An example is given here. This is an estimator that we can use the .fit() method on. Then we can use the .transform() method on that.\n\nindexer = StringIndexer(inputCols = [\"owner\"], outputCols = [\"owner_numeric\"])\nindexerTrans = indexer.fit(bike) #now indexerTrans will have a .transform() method\nindexerTrans.transform(bike).show(30)\n\n                                                                                \n\n\n+--------------------+-------------+----+-----------+---------+---------+-----------------+-------------+\n|                name|selling_price|year|seller_type|    owner|km_driven|ex_showroom_price|owner_numeric|\n+--------------------+-------------+----+-----------+---------+---------+-----------------+-------------+\n|Royal Enfield Cla...|       175000|2019| Individual|1st owner|      350|              NaN|          0.0|\n|           Honda Dio|        45000|2017| Individual|1st owner|     5650|              NaN|          0.0|\n|Royal Enfield Cla...|       150000|2018| Individual|1st owner|    12000|         148114.0|          0.0|\n|Yamaha Fazer FI V...|        65000|2015| Individual|1st owner|    23000|          89643.0|          0.0|\n|Yamaha SZ [2013-2...|        20000|2011| Individual|2nd owner|    21000|              NaN|          1.0|\n|    Honda CB Twister|        18000|2010| Individual|1st owner|    60000|          53857.0|          0.0|\n|Honda CB Hornet 160R|        78500|2018| Individual|1st owner|    17000|          87719.0|          0.0|\n|Royal Enfield Bul...|       180000|2008| Individual|2nd owner|    39000|              NaN|          1.0|\n|Hero Honda CBZ ex...|        30000|2010| Individual|1st owner|    32000|              NaN|          0.0|\n|  Bajaj Discover 125|        50000|2016| Individual|1st owner|    42000|          60122.0|          0.0|\n|         Yamaha FZ16|        35000|2015| Individual|1st owner|    32000|          78712.0|          0.0|\n|          Honda Navi|        28000|2016| Individual|2nd owner|    10000|          47255.0|          1.0|\n|Bajaj Avenger Str...|        80000|2018| Individual|1st owner|    21178|          95955.0|          0.0|\n|       Yamaha YZF R3|       365000|2019| Individual|1st owner|     1127|         351680.0|          0.0|\n|             Jawa 42|       185000|2020| Individual|1st owner|     1700|              NaN|          0.0|\n|Suzuki Access 125...|        25000|2012| Individual|1st owner|    55000|          58314.0|          0.0|\n|  Hero Honda Glamour|        25000|2006| Individual|1st owner|    27000|              NaN|          0.0|\n|    Yamaha YZF R15 S|        40000|2010| Individual|2nd owner|    45000|         117926.0|          1.0|\n|Royal Enfield Cla...|       150000|2018| Individual|1st owner|    23000|         148114.0|          0.0|\n|         Yamaha FZ25|       120000|2018| Individual|1st owner|    39000|         132680.0|          0.0|\n|Hero Passion Pro 110|        15000|2008| Individual|1st owner|    60000|              NaN|          0.0|\n|Honda Navi [2016-...|        26000|2016| Individual|1st owner|    17450|          44389.0|          0.0|\n|      Honda Activa i|        32000|2013| Individual|2nd owner|    20696|          53900.0|          1.0|\n|       Jawa Standard|       180000|2019| Individual|1st owner|     2000|              NaN|          0.0|\n|Royal Enfield Thu...|       110000|2016| Individual|1st owner|    20000|              NaN|          0.0|\n|    Honda Dream Yuga|        25000|2012| Individual|1st owner|    35000|          56147.0|          0.0|\n|TVS Apache RTR 16...|        80000|2018| Individual|1st owner|    15210|              NaN|          0.0|\n|Honda Navi [2016-...|        42000|2017| Individual|1st owner|    24000|          44389.0|          0.0|\n|Yamaha Fazer [200...|        40000|2013| Individual|3rd owner|    35000|          84751.0|          2.0|\n|Hero Honda Splend...|        21000|2009| Individual|1st owner|    10000|              NaN|          0.0|\n+--------------------+-------------+----+-----------+---------+---------+-----------------+-------------+\nonly showing top 30 rows\n\n\n\nAlright, now let’s conver that to a 0/1 indicator with Binarizer().\n\nbinaryTrans = Binarizer(threshold = 0.5, inputCol = \"owner_numeric\", outputCol = \"owner_indicator\")\nbinaryTrans.transform(\n    indexerTrans.transform(bike)\n).show()\n\n+--------------------+-------------+----+-----------+---------+---------+-----------------+-------------+---------------+\n|                name|selling_price|year|seller_type|    owner|km_driven|ex_showroom_price|owner_numeric|owner_indicator|\n+--------------------+-------------+----+-----------+---------+---------+-----------------+-------------+---------------+\n|Royal Enfield Cla...|       175000|2019| Individual|1st owner|      350|              NaN|          0.0|            0.0|\n|           Honda Dio|        45000|2017| Individual|1st owner|     5650|              NaN|          0.0|            0.0|\n|Royal Enfield Cla...|       150000|2018| Individual|1st owner|    12000|         148114.0|          0.0|            0.0|\n|Yamaha Fazer FI V...|        65000|2015| Individual|1st owner|    23000|          89643.0|          0.0|            0.0|\n|Yamaha SZ [2013-2...|        20000|2011| Individual|2nd owner|    21000|              NaN|          1.0|            1.0|\n|    Honda CB Twister|        18000|2010| Individual|1st owner|    60000|          53857.0|          0.0|            0.0|\n|Honda CB Hornet 160R|        78500|2018| Individual|1st owner|    17000|          87719.0|          0.0|            0.0|\n|Royal Enfield Bul...|       180000|2008| Individual|2nd owner|    39000|              NaN|          1.0|            1.0|\n|Hero Honda CBZ ex...|        30000|2010| Individual|1st owner|    32000|              NaN|          0.0|            0.0|\n|  Bajaj Discover 125|        50000|2016| Individual|1st owner|    42000|          60122.0|          0.0|            0.0|\n|         Yamaha FZ16|        35000|2015| Individual|1st owner|    32000|          78712.0|          0.0|            0.0|\n|          Honda Navi|        28000|2016| Individual|2nd owner|    10000|          47255.0|          1.0|            1.0|\n|Bajaj Avenger Str...|        80000|2018| Individual|1st owner|    21178|          95955.0|          0.0|            0.0|\n|       Yamaha YZF R3|       365000|2019| Individual|1st owner|     1127|         351680.0|          0.0|            0.0|\n|             Jawa 42|       185000|2020| Individual|1st owner|     1700|              NaN|          0.0|            0.0|\n|Suzuki Access 125...|        25000|2012| Individual|1st owner|    55000|          58314.0|          0.0|            0.0|\n|  Hero Honda Glamour|        25000|2006| Individual|1st owner|    27000|              NaN|          0.0|            0.0|\n|    Yamaha YZF R15 S|        40000|2010| Individual|2nd owner|    45000|         117926.0|          1.0|            1.0|\n|Royal Enfield Cla...|       150000|2018| Individual|1st owner|    23000|         148114.0|          0.0|            0.0|\n|         Yamaha FZ25|       120000|2018| Individual|1st owner|    39000|         132680.0|          0.0|            0.0|\n+--------------------+-------------+----+-----------+---------+---------+-----------------+-------------+---------------+\nonly showing top 20 rows\n\n\n\nGreat! Now the easy part. We just want to create some log variables and select only certain columns. The SQLTransformer() allows for (only) basic SQL commands. Note we want our response to be called label so we’ll rename it here.\n\nsqlTrans = SQLTransformer(\n    statement = \"\"\"\n                SELECT owner_indicator, year, log(km_driven) as log_km_driven, log(selling_price) as label FROM __THIS__\n                \"\"\"\n)\n\n\nsqlTrans.transform(\n    binaryTrans.transform(\n        indexerTrans.transform(bike)\n    )\n).show(30)\n\n+---------------+----+------------------+------------------+\n|owner_indicator|year|     log_km_driven|             label|\n+---------------+----+------------------+------------------+\n|            0.0|2019| 5.857933154483459|12.072541252905651|\n|            0.0|2017| 8.639410824140487|10.714417768752456|\n|            0.0|2018| 9.392661928770137|11.918390573078392|\n|            0.0|2015|10.043249494911286|11.082142548877775|\n|            1.0|2011|  9.95227771670556| 9.903487552536127|\n|            0.0|2010|11.002099841204238| 9.798127036878302|\n|            0.0|2018| 9.740968623038354|  11.2708539037705|\n|            1.0|2008|10.571316925111784|12.100712129872347|\n|            0.0|2010|10.373491181781864|10.308952660644293|\n|            0.0|2016|10.645424897265505|10.819778284410283|\n|            0.0|2015|10.373491181781864| 10.46310334047155|\n|            1.0|2016| 9.210340371976184|10.239959789157341|\n|            0.0|2018|   9.9607181859904|11.289781913656018|\n|            0.0|2019| 7.027314514039777| 12.80765263256463|\n|            0.0|2020| 7.438383530044307|12.128111104060462|\n|            0.0|2012|10.915088464214607|10.126631103850338|\n|            0.0|2006|10.203592144986466|10.126631103850338|\n|            1.0|2010|10.714417768752456|10.596634733096073|\n|            0.0|2018|10.043249494911286|11.918390573078392|\n|            0.0|2018|10.571316925111784|11.695247021764184|\n|            0.0|2008|11.002099841204238| 9.615805480084347|\n|            0.0|2016| 9.767094927630573|10.165851817003619|\n|            1.0|2013| 9.937695723865865|10.373491181781864|\n|            0.0|2019| 7.600902459542082|12.100712129872347|\n|            0.0|2016| 9.903487552536127|11.608235644774552|\n|            0.0|2012| 10.46310334047155|10.126631103850338|\n|            0.0|2018|  9.62970838525334|11.289781913656018|\n|            0.0|2017|10.085809109330082|10.645424897265505|\n|            1.0|2013| 10.46310334047155|10.596634733096073|\n|            0.0|2009| 9.210340371976184|  9.95227771670556|\n+---------------+----+------------------+------------------+\nonly showing top 30 rows\n\n\n\nWe also need to put the predictors into a single column called ‘features’.\nPlacing multiple columns into one can be done via VectorAssembler() from pyspark.ml.feature.\n\nassembler = VectorAssembler(inputCols = [\"year\", \"log_km_driven\", \"owner_indicator\"], outputCol = \"features\", handleInvalid = 'keep')\n\nNotice that we are passing what would be the result columns from the previous SQL transform we did. The VectorAssembler() also has a .transform() method. Let’s see how these would be used together to produce a new data set.\n\nassembler.transform(\n    sqlTrans.transform(\n        binaryTrans.transform(\n            indexerTrans.transform(bike)\n        )\n    )\n).show(10)\n\n+---------------+----+------------------+------------------+--------------------+\n|owner_indicator|year|     log_km_driven|             label|            features|\n+---------------+----+------------------+------------------+--------------------+\n|            0.0|2019| 5.857933154483459|12.072541252905651|[2019.0,5.8579331...|\n|            0.0|2017| 8.639410824140487|10.714417768752456|[2017.0,8.6394108...|\n|            0.0|2018| 9.392661928770137|11.918390573078392|[2018.0,9.3926619...|\n|            0.0|2015|10.043249494911286|11.082142548877775|[2015.0,10.043249...|\n|            1.0|2011|  9.95227771670556| 9.903487552536127|[2011.0,9.9522777...|\n|            0.0|2010|11.002099841204238| 9.798127036878302|[2010.0,11.002099...|\n|            0.0|2018| 9.740968623038354|  11.2708539037705|[2018.0,9.7409686...|\n|            1.0|2008|10.571316925111784|12.100712129872347|[2008.0,10.571316...|\n|            0.0|2010|10.373491181781864|10.308952660644293|[2010.0,10.373491...|\n|            0.0|2016|10.645424897265505|10.819778284410283|[2016.0,10.645424...|\n+---------------+----+------------------+------------------+--------------------+\nonly showing top 10 rows\n\n\n\nAwesome! Our data is now transformed to have the new variables we want and the data is in the format needed to fit model using MLlib. Specifically: - A column named label that is the response variable - A column named features that is a column containing all the predictor values together\nNext step, we’ll fit a basic multiple linear regression model using the LinearRegression() function from pyspark.ml.regression. This function does regularized regression (Elastic net, which includes LASSO and Ridge Regression as special cases) so there are a few set up parameters we can modify to just get the usual MLR fit.\n\nfrom pyspark.ml.regression import LinearRegression\n\n\nlr = LinearRegression(regParam = 0, elasticNetParam = 0)\n\nNow we can use the .fit() method on the transformed data we made above.\n\nlr_data = assembler.transform(\n                sqlTrans.transform(\n                    binaryTrans.transform(\n                        indexerTrans.transform(bike)\n                    )\n                )\n)\nlr_data\n\nDataFrame[owner_indicator: double, year: bigint, log_km_driven: double, label: double, features: vector]\n\n\n\nlrModel = lr.fit(lr_data)\n\n24/03/08 17:03:10 WARN Instrumentation: [1d9ded2d] regParam is zero, which might cause numerical instability and overfitting.\n                                                                                \n\n\nWe can inspect the model fit using attributes of our lrModel object.\n\nprint(\"Intercept: %s\" % str(lrModel.intercept), \"Coefficients: %s\" % str(lrModel.coefficients))\n\nIntercept: -151.65184885852284 Coefficients: [0.08175654813171404,-0.22825871252866586,0.10021253855848164]\n\n\nLet’s inspect the training set RMSE and other model fit metrics.\n\ntrainingSummary = lrModel.summary\ntrainingSummary.residuals.show(5)\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"r2: %f\" % trainingSummary.r2)\n\n+--------------------+\n|           residuals|\n+--------------------+\n|-0.00495628658079...|\n| -0.5646701626673813|\n|  0.7294822208803797|\n| 0.28700612130943703|\n| -0.6856003220335012|\n+--------------------+\nonly showing top 5 rows\n\nRMSE: 0.509892\nr2: 0.485191\n\n\nOf course we will often want to do predictions as well. This is easy to do! The model itself actually becomes a transformer once it is fit! We just use the .transform() method and pass it data similar in format to that on which the model was trained. Here we’ll just look at the prediction from the data used to fit the model.\n\npreds = lrModel.transform(lr_data)\n\n\npreds.show(5)\n\n+---------------+----+------------------+------------------+--------------------+------------------+\n|owner_indicator|year|     log_km_driven|             label|            features|        prediction|\n+---------------+----+------------------+------------------+--------------------+------------------+\n|            0.0|2019| 5.857933154483459|12.072541252905651|[2019.0,5.8579331...|12.077497539486444|\n|            0.0|2017| 8.639410824140487|10.714417768752456|[2017.0,8.6394108...|11.279087931419838|\n|            0.0|2018| 9.392661928770137|11.918390573078392|[2018.0,9.3926619...|11.188908352198013|\n|            0.0|2015|10.043249494911286|11.082142548877775|[2015.0,10.043249...|10.795136427568337|\n|            1.0|2011|  9.95227771670556| 9.903487552536127|[2011.0,9.9522777...|10.589087874569628|\n+---------------+----+------------------+------------------+--------------------+------------------+\nonly showing top 5 rows\n\n\n\n\npreds.select(\"label\", \"prediction\").show(5)\n\n+------------------+------------------+\n|             label|        prediction|\n+------------------+------------------+\n|12.072541252905651|12.077497539486444|\n|10.714417768752456|11.279087931419838|\n|11.918390573078392|11.188908352198013|\n|11.082142548877775|10.795136427568337|\n| 9.903487552536127|10.589087874569628|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nJust a sanity check to show we get the same RMSE if we do it with these values:\n\nmy_preds = preds.select(\"label\", \"prediction\").toPandas()\nimport numpy as np\nnp.sqrt(np.mean((my_preds[\"label\"]-my_preds[\"prediction\"])**2))\n\n0.5098915575316225\n\n\nOne more sanity check. Compare with sklearn.\nNote: I had an error with the scipy module not being recognized. I opened a new terminal window (file –&gt; New Launcher, choose Terminal at the bottom) and had to submit the code: pip install scipy --force\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nbike_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/bikeDetails.csv\")\nbike_data['log_selling_price'] = np.log(bike_data['selling_price'])\nbike_data['log_km_driven'] = np.log(bike_data['km_driven'])\nbike_data['one_owner'] = pd.get_dummies(data = bike_data['owner'])['1st owner']\nmlr_fit = linear_model.LinearRegression() #Create a reg object\nmlr_fit.fit(bike_data[['year','log_km_driven','one_owner']], bike_data['log_selling_price'].values)\nprint(mlr_fit.intercept_, mlr_fit.coef_)\nprint(mean_squared_error(bike_data['log_selling_price'], mlr_fit.predict(bike_data[[\"year\", \"log_km_driven\", \"one_owner\"]]), squared = False))\n\n-151.55163631488233 [ 0.08175655 -0.22825871 -0.10021254]\n0.5098915575316225\n\n\n/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n  warnings.warn(\n\n\nCool! We’ve fit an MLR model using the pyspark MLlib library!\n\nUsing Cross-Validation to Select Our Model\nOf course we know that what we care about is the quality of the prediction our model makes on data it wasn’t trained on. Generally, this means we need to either - split our data into a training and test (sometimes called validation) set - use cross-validation\nAnd of course, we’ve talked about the need for both when selecting from many types of models.\nLet’s focus on doing k-fold CV using pyspark. We need to set up our grid of tuning parameters (if applicable) and then run our CV algorithm. This can be done using the ParamGridBuilder() and CrossValidator() functions from pyspark.ml.tuning, respectively.\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nThe only tuning parameter we’ll worry about is the regParam. which is \\(\\lambda\\) in the penalty part of the loss function: \\[\\alpha(\\lambda||w||_1)+(1-\\alpha)(\\lambda/2||w||_2^2)\\] If we set the elasticNetParam to 1 (\\(\\alpha\\) above) then we are doing LASSO regression.\nWe can see some details for each algorithm via the help files. We can look at how well the model does when we use different amounts of LASSO regularization. We use ParamGridBuilder() and .addGrid() to specify the tuning parameter values. Then finally we use the .build() method to instruct it to build the grid.\n\nlr = LinearRegression()\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0, 0.05, 0.1, 0.15]) \\\n    .addGrid(lr.elasticNetParam, [1]) \\\n    .build()\n\nNext up, we can use the CrossValidator() function to run k-fold CV over the grid of tuning parameters we just set up.\nWe need to tell pyspark what loss function to use when evaluating. This is done via the RegressionEvaluator() function from pyspark.ml.evaluation. To override the default metric used we can specify it explicitly using the metricName= argument when calling the function. rmse is the default here.\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n\ncrossval = CrossValidator(estimator = lr,\n                          estimatorParamMaps = paramGrid,\n                          evaluator = RegressionEvaluator(metricName='rmse'),\n                          numFolds=5)\n\nWe’ve now set up the crossval object. Just like with sklearn we now use the .fit() method to actually fit the models.\n\ncvModel = crossval.fit(lr_data)\n\n24/03/08 17:06:44 WARN Instrumentation: [e4f7ec65] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:06:49 WARN Instrumentation: [96732fed] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:06:52 WARN Instrumentation: [ccc91038] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:06:54 WARN Instrumentation: [04867e84] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:06:56 WARN Instrumentation: [e89224fc] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:06:58 WARN Instrumentation: [ca268c57] regParam is zero, which might cause numerical instability and overfitting.\n\n\nBy default, the only model returned is the best model as measured by our Loss function.\nTo determine which model was returned we can look at the .avgMetrics attribute along with the paramGrid object we used to fit the models.\n\nlist(zip([0, 0.05, 0.1, 0.15], cvModel.avgMetrics))\n\n[(0, 0.5112406194486974),\n (0.05, 0.5136269855905575),\n (0.1, 0.5235001947739562),\n (0.15, 0.5395464815213404)]\n\n\nWe can see the best model’s parameter estimates as well.\n\nprint(cvModel.bestModel._java_obj.intercept(), cvModel.bestModel._java_obj.coefficients())\n\n-151.6518488577565 [0.08175654813133452,-0.22825871252886246,0.10021253855757116]\n\n\nAs with the single linear model fit we did above, this new object, cvModel, is now a transformation as well. This allows us to get prediction using the best fit model. If we had a test set we could use it to get the test set prediction easily. As we didn’t split into a test set, let’s just see how to use it to predict on the training data (i.e. return the fitted values for the model).\n\npredsCV = cvModel.transform(lr_data)\n\n\npredsCV.select(\"label\", \"prediction\").show(5)\n\n+------------------+------------------+\n|             label|        prediction|\n+------------------+------------------+\n|12.072541252905651|12.077497539485364|\n|10.714417768752456|11.279087931418985|\n|11.918390573078392|11.188908352196648|\n|11.082142548877775|10.795136427567968|\n| 9.903487552536127|10.589087874569884|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nAs our model chosen was not regularized, we get the same predictions as previous!\n\npreds.select(\"label\", \"prediction\").show(5)\n\n+------------------+------------------+\n|             label|        prediction|\n+------------------+------------------+\n|12.072541252905651|12.077497539486444|\n|10.714417768752456|11.279087931419838|\n|11.918390573078392|11.188908352198013|\n|11.082142548877775|10.795136427568337|\n| 9.903487552536127|10.589087874569628|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nWe can find the training RMSE by passing these predictions to the RegressionEvaluator().evaluate() method.\n\nRegressionEvaluator().evaluate(cvModel.transform(lr_data))\n\n0.509891557531623\n\n\n\n\nPipelines and the Training/Test Split\nOf course we often want to have a training and test set so we can do all of our model fitting and tuning on the training set and then see how different model types compare on the test set. We should always split our data into training and test sets first, before doing transformations. Reason being: - If we do transformations on the training set, we want to use the exact same transformations on the test set - For instance, if we center some predictors (subtract the mean) in our training set, we want to subtract the mean of the training set to standardize the test set values prior to doing test set predictions! - By splitting the data first, we can make sure we aren’t using any test data (and the knowledge that comes with it) when training our models\nBy setting up a pipeline in MLlib we can easily (ha, that’s kind of a joke) create the sequence of transformations/model fits and apply those same transformations on our test set!\nLet’s start with the training/test split. This can be done using the .randomSplit() method on a spark SQL style data frame.\n\ntrain, test = bike.randomSplit([0.8,0.2], seed = 1)\nprint(train.count(), test.count())\n\n840 221\n\n\nNow that we’ve split our data we can set up the transformations to do. Just like with other spark stuff, things are set up as a DAG and done only at run time.\nWe created the transformation plans previously so we’ll just pull them down here for clarity.\n\n#Sequence of transformations\n#indexerTrans\n#binaryTrans\n#sqlTrans\n#assembler\n#(linear model or other model here!)\n\nWe are then ready to create our pipeline that includes these transformations and the model that we want to fit. We use the Pipeline() function from the pyspark.ml module to set up our sequence of transformations/estimators.\n\nfrom pyspark.ml import Pipeline\n\n\nlr = LinearRegression()\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0, 0.01, 0.04, 0.06, 0.1]) \\\n    .addGrid(lr.elasticNetParam, [0, 0.5, 0.8, 0.9, 1]) \\\n    .build()\npipeline = Pipeline(stages = [indexerTrans, binaryTrans, sqlTrans, assembler, lr])\n\nOur DAG is now set up and we can use this pipeline within our CV calculation (or basic model fitting). What’s nice is that since it contains all the information about the transformations done, we can easily apply this to a test set and not have to worry about how to do the transformations/prepping of the data on that set.\nInstead of using the model type as the estimator we’ll pass the pipeline we’ve set up.\n\ncrossval = CrossValidator(estimator = pipeline,\n                          estimatorParamMaps = paramGrid,\n                          evaluator = RegressionEvaluator(),\n                          numFolds=5)\n\nWith everything set up, we can now fit our models!\n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(train)\n\n24/03/08 17:08:42 WARN Instrumentation: [45408c1a] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:43 WARN Instrumentation: [bdf949e2] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:43 WARN Instrumentation: [c9e03b2c] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:44 WARN Instrumentation: [cc03885d] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:45 WARN Instrumentation: [8f5c1468] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:56 WARN Instrumentation: [d5eaed65] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:57 WARN Instrumentation: [e1d0afb2] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:58 WARN Instrumentation: [1e88fc13] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:58 WARN Instrumentation: [769528f1] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:08:59 WARN Instrumentation: [a95045a7] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:10 WARN Instrumentation: [9e6d1f94] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:11 WARN Instrumentation: [778a593a] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:11 WARN Instrumentation: [7a4bdc3a] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:12 WARN Instrumentation: [ffefcfcf] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:12 WARN Instrumentation: [333c0cdd] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:23 WARN Instrumentation: [0c04f6cd] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:24 WARN Instrumentation: [a9177fea] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:24 WARN Instrumentation: [efc198f1] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:25 WARN Instrumentation: [4c91d04f] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:25 WARN Instrumentation: [0d67662a] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:35 WARN Instrumentation: [1f57078a] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:36 WARN Instrumentation: [5973b334] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:36 WARN Instrumentation: [f17e57e2] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:37 WARN Instrumentation: [d043d7d8] regParam is zero, which might cause numerical instability and overfitting.\n24/03/08 17:09:37 WARN Instrumentation: [750b1ce7] regParam is zero, which might cause numerical instability and overfitting.\n\n\nCheck which model is chosen as the best:\n\nmy_list = []\nfor i in range(len(paramGrid)):\n    my_list.append([cvModel.avgMetrics[i], paramGrid[i].values()])\nmy_list\n\n[[0.5124021026719168, dict_values([0.0, 0.0])],\n [0.5124021026718817, dict_values([0.0, 0.5])],\n [0.5124021026718577, dict_values([0.0, 0.8])],\n [0.5124021026718556, dict_values([0.0, 0.9])],\n [0.5124021026719084, dict_values([0.0, 1.0])],\n [0.5123205996805408, dict_values([0.01, 0.0])],\n [0.5123483858580572, dict_values([0.01, 0.5])],\n [0.5122975622688871, dict_values([0.01, 0.8])],\n [0.5122867285770079, dict_values([0.01, 0.9])],\n [0.5122589945611346, dict_values([0.01, 1.0])],\n [0.5123909761343102, dict_values([0.04, 0.0])],\n [0.5122102563576977, dict_values([0.04, 0.5])],\n [0.5120946331251414, dict_values([0.04, 0.8])],\n [0.5121238218400215, dict_values([0.04, 0.9])],\n [0.51224440770079, dict_values([0.04, 1.0])],\n [0.512663579012802, dict_values([0.06, 0.0])],\n [0.5126627597862455, dict_values([0.06, 0.5])],\n [0.5137013318120469, dict_values([0.06, 0.8])],\n [0.5142717497055167, dict_values([0.06, 0.9])],\n [0.5148905572778366, dict_values([0.06, 1.0])],\n [0.5136433512562688, dict_values([0.1, 0.0])],\n [0.5159728675900753, dict_values([0.1, 0.5])],\n [0.5199269354732108, dict_values([0.1, 0.8])],\n [0.5215278837702534, dict_values([0.1, 0.9])],\n [0.523281713834437, dict_values([0.1, 1.0])]]\n\n\nUse that best model to get test error on the test set.\n\ncvModel.transform(test).show(5)\n\n+---------------+----+------------------+------------------+--------------------+------------------+\n|owner_indicator|year|     log_km_driven|             label|            features|        prediction|\n+---------------+----+------------------+------------------+--------------------+------------------+\n|            0.0|2019| 5.857933154483459|12.072541252905651|[2019.0,5.8579331...|11.953749958528732|\n|            0.0|2020| 7.438383530044307|12.128111104060462|[2020.0,7.4383835...|11.705822488439821|\n|            1.0|2013| 9.937695723865865|10.373491181781864|[2013.0,9.9376957...|10.679205674574945|\n|            0.0|2018|10.571316925111784|11.695247021764184|[2018.0,10.571316...|10.919885599143953|\n|            0.0|2017| 7.824046010856292|10.915088464214607|[2017.0,7.8240460...|11.405445806006924|\n+---------------+----+------------------+------------------+--------------------+------------------+\nonly showing top 5 rows\n\n\n\n\ntest_error = RegressionEvaluator().evaluate(cvModel.transform(test))\nprint(test_error)\n\n0.5114772577953631\n\n\nFantastic! We can now set up a pipeline and use CV to fit our model. Then we can take the best model and find test set predictions!\nRemember, once we have our final model we fit that model on the entire data set. Then we use it for future predictions and what-not.\n\n\nMLflow Basics\nNote: To run this code in our jupyterhub I had to do a few things first. I needed to force install a few modules. Open a new terminal window and do the following:\npip install requests --force\npip install pyyaml --force\npip install entrypoints --force\nThis section is modified from the tutorial given here. Although this uses sklearn, it can be modified to work with MLlib!\nFirst, they create a function to train an elastic net model specifically on the wine data we’ve used before. This function takes in the \\(\\alpha\\) and \\(L_1\\) ratio values from the sklearn ElasticNet() function. This function can then be passed different values of these and the given metrics will be found and logged.\n\n# Wine Quality Sample\ndef train(in_alpha, in_l1_ratio):\n    import os\n    import warnings\n    import sys\n\n    import pandas as pd\n    import numpy as np\n    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import ElasticNet\n\n    import mlflow\n    import mlflow.sklearn\n\n    import logging\n\n    logging.basicConfig(level=logging.WARN)\n    logger = logging.getLogger(__name__)\n\n    def eval_metrics(actual, pred):\n        rmse = np.sqrt(mean_squared_error(actual, pred))\n        mae = mean_absolute_error(actual, pred)\n        r2 = r2_score(actual, pred)\n        return rmse, mae, r2\n\n    warnings.filterwarnings(\"ignore\")\n    np.random.seed(40)\n\n    # Read the wine-quality csv file from the URL\n    csv_url = (\n        \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n    )\n    try:\n        data = pd.read_csv(csv_url, sep=\";\")\n    except Exception as e:\n        logger.exception(\n            \"Unable to download training & test CSV, check your internet connection. Error: %s\", e\n        )\n\n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is \"quality\" which is a scalar from [3, 9]\n    train_x = train.drop([\"quality\"], axis=1)\n    test_x = test.drop([\"quality\"], axis=1)\n    train_y = train[[\"quality\"]]\n    test_y = test[[\"quality\"]]\n\n    # Set default values if no alpha is provided\n    if float(in_alpha) is None:\n        alpha = 0.5\n    else:\n        alpha = float(in_alpha)\n\n    # Set default values if no l1_ratio is provided\n    if float(in_l1_ratio) is None:\n        l1_ratio = 0.5\n    else:\n        l1_ratio = float(in_l1_ratio)\n\n    # Useful for multiple runs (only doing one run in this sample notebook)\n    with mlflow.start_run():\n        # Execute ElasticNet\n        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n        lr.fit(train_x, train_y)\n\n        # Evaluate Metrics\n        predicted_qualities = lr.predict(test_x)\n        (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n        # Print out metrics\n        print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n        print(\"  RMSE: %s\" % rmse)\n        print(\"  MAE: %s\" % mae)\n        print(\"  R2: %s\" % r2)\n\n        # Log parameter, metrics, and model to MLflow\n        mlflow.log_param(\"alpha\", alpha)\n        mlflow.log_param(\"l1_ratio\", l1_ratio)\n        mlflow.log_metric(\"rmse\", rmse)\n        mlflow.log_metric(\"r2\", r2)\n        mlflow.log_metric(\"mae\", mae)\n\n        mlflow.sklearn.log_model(lr, \"model\")\n\nWith this function set up, we now just call it with different values and it creates logs for them.\n\ntrain(0.5, 0.5)\n\n2024/03/29 13:51:46 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: No module named 'git'\n\n\nElasticnet model (alpha=0.500000, l1_ratio=0.500000):\n  RMSE: 0.7931640229276851\n  MAE: 0.6271946374319586\n  R2: 0.10862644997792614\n\n\n\ntrain(0.5, 1)\n\nElasticnet model (alpha=0.500000, l1_ratio=1.000000):\n  RMSE: 0.832819092896359\n  MAE: 0.6681279771237894\n  R2: 0.017268050734704055\n\n\n\ntrain(0.1, 0.5)\n\nElasticnet model (alpha=0.100000, l1_ratio=0.500000):\n  RMSE: 0.7308996187375898\n  MAE: 0.5615486628017713\n  R2: 0.2430813606733676\n\n\n\ntrain(1, 0)\n\nElasticnet model (alpha=1.000000, l1_ratio=0.000000):\n  RMSE: 0.7508731220796289\n  MAE: 0.5811664801219333\n  R2: 0.2011470433755671\n\n\nIf not in a docker container we would now be able to call the mlflow UI and look through things. I couldn’t get that to work and gave up! Instead, we can just read in the data it uses in the UI and look at it within python.\n\nimport mlflow\n#throws an issue but it returns the data appropriately\nruns = mlflow.search_runs(experiment_ids=[\"0\"])\nruns.head()\n\n\n\n\n\n\n\n\nrun_id\nexperiment_id\nstatus\nartifact_uri\nstart_time\nend_time\nmetrics.r2\nmetrics.mae\nmetrics.rmse\nparams.l1_ratio\nparams.alpha\ntags.mlflow.log-model.history\ntags.mlflow.source.type\ntags.mlflow.user\ntags.mlflow.runName\ntags.mlflow.source.name\n\n\n\n\n0\n65d548898f1e40558fef1624d3d78cc6\n0\nFINISHED\nfile:///home/jupyter-jbpost2%40ncsu.edu/mlruns...\n2024-03-08 22:15:28.734000+00:00\n2024-03-08 22:15:29.939000+00:00\n0.201147\n0.581166\n0.750873\n0.0\n1.0\n[{\"run_id\": \"65d548898f1e40558fef1624d3d78cc6\"...\nLOCAL\njupyter-jbpost2@ncsu.edu\nbedecked-dolphin-527\n/opt/tljh/user/envs/pySpark/lib/python3.9/site...\n\n\n1\n6f421ddea3b5467889fcbc030ff9c9df\n0\nFINISHED\nfile:///home/jupyter-jbpost2%40ncsu.edu/mlruns...\n2024-03-08 22:15:26.864000+00:00\n2024-03-08 22:15:28.101000+00:00\n0.243081\n0.561549\n0.730900\n0.5\n0.1\n[{\"run_id\": \"6f421ddea3b5467889fcbc030ff9c9df\"...\nLOCAL\njupyter-jbpost2@ncsu.edu\nenthused-ape-110\n/opt/tljh/user/envs/pySpark/lib/python3.9/site...\n\n\n2\ne79aa57b53084ea79b24950ecbb0284f\n0\nFINISHED\nfile:///home/jupyter-jbpost2%40ncsu.edu/mlruns...\n2024-03-08 22:15:23.857000+00:00\n2024-03-08 22:15:25.117000+00:00\n0.017268\n0.668128\n0.832819\n1.0\n0.5\n[{\"run_id\": \"e79aa57b53084ea79b24950ecbb0284f\"...\nLOCAL\njupyter-jbpost2@ncsu.edu\nwise-worm-607\n/opt/tljh/user/envs/pySpark/lib/python3.9/site...\n\n\n3\nc40bba6d7d8d43dc8cbce75cfb448b93\n0\nFINISHED\nfile:///home/jupyter-jbpost2%40ncsu.edu/mlruns...\n2024-03-08 22:15:16.728000+00:00\n2024-03-08 22:15:18.462000+00:00\n0.108626\n0.627195\n0.793164\n0.5\n0.5\n[{\"run_id\": \"c40bba6d7d8d43dc8cbce75cfb448b93\"...\nLOCAL\njupyter-jbpost2@ncsu.edu\nwelcoming-pug-473\n/opt/tljh/user/envs/pySpark/lib/python3.9/site...\n\n\n\n\n\n\n\n\nruns.columns\n\nIndex(['run_id', 'experiment_id', 'status', 'artifact_uri', 'start_time',\n       'end_time', 'metrics.r2', 'metrics.mae', 'metrics.rmse',\n       'params.l1_ratio', 'params.alpha', 'tags.mlflow.log-model.history',\n       'tags.mlflow.source.type', 'tags.mlflow.user', 'tags.mlflow.runName',\n       'tags.mlflow.source.name'],\n      dtype='object')\n\n\n\nruns[[\"metrics.rmse\", \"metrics.r2\", \"metrics.mae\", \"params.l1_ratio\", \"params.alpha\"]].sort_values(\"metrics.rmse\")\n\n\n\n\n\n\n\n\nmetrics.rmse\nmetrics.r2\nmetrics.mae\nparams.l1_ratio\nparams.alpha\n\n\n\n\n1\n0.730900\n0.243081\n0.561549\n0.5\n0.1\n\n\n0\n0.750873\n0.201147\n0.581166\n0.0\n1.0\n\n\n3\n0.793164\n0.108626\n0.627195\n0.5\n0.5\n\n\n2\n0.832819\n0.017268\n0.668128\n1.0\n0.5"
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#common-database-software",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#common-database-software",
    "title": "Data Storage and Basic SQL",
    "section": "Common Database Software",
    "text": "Common Database Software\nMany common types of relational databases management systems (RDBMS) exist. Some are free and some are not. A few common ones are:\n\nOracle\nMySQL\nSQL Server\nPostgreSQL\nSQLite\nAzure SQL\n\nMost RDBMS have their own Structured Query Language (SQL), however the basic functionality is similar across them!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Data Storage and Basic SQL"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#actions-on-databases",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#actions-on-databases",
    "title": "Data Storage and Basic SQL",
    "section": "Actions on Databases",
    "text": "Actions on Databases\nThere are a few common actions we often want to perform on a database. The acronym CRUD is used to describe four actions:\n\nCreate data\nRead data\nUpdate data\nDelete data\n\nWe will write SQL code to do these actions!\nYou could imagine that, with multiple users possibly accessing a database, we need to be very careful in how we do these actions. There are four properties that relational database transactions must have:\n\nAtomicity defines all the elements that make up a complete database transaction.\nConsistency defines the rules for maintaining data points in a correct state after a transaction.\nIsolation keeps the effect of a transaction invisible to others until it is committed, to avoid confusion.\nDurability ensures that data changes become permanent once the transaction is committed.\n\nThis is commonly referred to as ACID properties.\nLet’s explore using SQLite with python!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Data Storage and Basic SQL"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#accessing-an-sqlite-database-in-python",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#accessing-an-sqlite-database-in-python",
    "title": "Data Storage and Basic SQL",
    "section": "Accessing an SQLite Database in python",
    "text": "Accessing an SQLite Database in python\nThe sqlite3 module provides the ability to connect and perform actions on an SQLite database.\n\nWe need to import this module\nThen we just need a path to our database file (if we already have one, otherwise one will be created at the path - if possible)\nWe’ll look at the commonly used chinook database the diagram above described.\nAs we need to have the ability to write to the database from our notebook, we can’t share the same database. That means you’ll need to download the chinook.db file and upload it to your folder area on the left!\nIf you want the changes to remain after you are done, you’d need to download that file before closing the session!\n\n\n#bring in the module\nimport sqlite3\n#make the connection to the .db file. My file is in the main folder on the left\ncon = sqlite3.connect(\"chinook.db\")\n\n\nAccessing the Table Schema\nEvery SQLite database contains a “schema table” with information about that database. This describes the - tables - indices (special lookup tables to improve efficiency of queries) - triggers (named database object that is executed automatically when an INSERT, UPDATE or DELETE statement is issued against the associated table) - views (read-only tables, combinations of tables, etc.)\nA schema file contains one row for each table, index, view, and trigger in the schema.\nWe can get the schema by issuing an SQL command! A common SQL command for querying a database looks like this: - SELECT column1, column2 FROM table WHERE logical_of_some_kind;\nFor SQLite from python, usually we’ll follow this structure:\n\nCreate a cursor object using cursor = con.cursor()\nWrite our SQL command as a string\nExecute the SQL code using the .execute() method (cursor.execute()) on our cursor object\nUse the .fetchall() method (cursor.fetchall()) to actually return the data requested\nClose the conneciton made by the cursor (cursor.close())\n\n\nBelow we use the multiline comment (three quotation marks to start and end) in order to write more legible SQL code\n\n\n#create a 'cursor' object from our connection\ncursor = con.cursor()\n\n#SQL query to return all table names in the data base\n#The * indicates we want to select everything\nget_schema = '''\n        SELECT *\n        FROM sqlite_schema\n        WHERE type = \"table\";\n        '''\n\n#execute the SQL query on the database!\ncursor.execute(get_schema)\n\n#The information for the query is stored in memory. We use the fetchall() method to actually return the information\nresult = cursor.fetchall()\n\n#finall we close the connection the cursor made with that query\ncursor.close()\n\nWe can now look at what is stored in result. Here it is actually a list that we can cycle through and print information from.\n\nprint(type(result))\nfor i in result:\n  print(i)\n\n&lt;class 'list'&gt;\n('table', 'albums', 'albums', 2, 'CREATE TABLE \"albums\"\\r\\n(\\r\\n    [AlbumId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Title] NVARCHAR(160)  NOT NULL,\\r\\n    [ArtistId] INTEGER  NOT NULL,\\r\\n    FOREIGN KEY ([ArtistId]) REFERENCES \"artists\" ([ArtistId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'sqlite_sequence', 'sqlite_sequence', 3, 'CREATE TABLE sqlite_sequence(name,seq)')\n('table', 'artists', 'artists', 4, 'CREATE TABLE \"artists\"\\r\\n(\\r\\n    [ArtistId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(120)\\r\\n)')\n('table', 'customers', 'customers', 5, 'CREATE TABLE \"customers\"\\r\\n(\\r\\n    [CustomerId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [FirstName] NVARCHAR(40)  NOT NULL,\\r\\n    [LastName] NVARCHAR(20)  NOT NULL,\\r\\n    [Company] NVARCHAR(80),\\r\\n    [Address] NVARCHAR(70),\\r\\n    [City] NVARCHAR(40),\\r\\n    [State] NVARCHAR(40),\\r\\n    [Country] NVARCHAR(40),\\r\\n    [PostalCode] NVARCHAR(10),\\r\\n    [Phone] NVARCHAR(24),\\r\\n    [Fax] NVARCHAR(24),\\r\\n    [Email] NVARCHAR(60)  NOT NULL,\\r\\n    [SupportRepId] INTEGER,\\r\\n    FOREIGN KEY ([SupportRepId]) REFERENCES \"employees\" ([EmployeeId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'employees', 'employees', 8, 'CREATE TABLE \"employees\"\\r\\n(\\r\\n    [EmployeeId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [LastName] NVARCHAR(20)  NOT NULL,\\r\\n    [FirstName] NVARCHAR(20)  NOT NULL,\\r\\n    [Title] NVARCHAR(30),\\r\\n    [ReportsTo] INTEGER,\\r\\n    [BirthDate] DATETIME,\\r\\n    [HireDate] DATETIME,\\r\\n    [Address] NVARCHAR(70),\\r\\n    [City] NVARCHAR(40),\\r\\n    [State] NVARCHAR(40),\\r\\n    [Country] NVARCHAR(40),\\r\\n    [PostalCode] NVARCHAR(10),\\r\\n    [Phone] NVARCHAR(24),\\r\\n    [Fax] NVARCHAR(24),\\r\\n    [Email] NVARCHAR(60),\\r\\n    FOREIGN KEY ([ReportsTo]) REFERENCES \"employees\" ([EmployeeId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'genres', 'genres', 10, 'CREATE TABLE \"genres\"\\r\\n(\\r\\n    [GenreId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(120)\\r\\n)')\n('table', 'invoices', 'invoices', 11, 'CREATE TABLE \"invoices\"\\r\\n(\\r\\n    [InvoiceId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [CustomerId] INTEGER  NOT NULL,\\r\\n    [InvoiceDate] DATETIME  NOT NULL,\\r\\n    [BillingAddress] NVARCHAR(70),\\r\\n    [BillingCity] NVARCHAR(40),\\r\\n    [BillingState] NVARCHAR(40),\\r\\n    [BillingCountry] NVARCHAR(40),\\r\\n    [BillingPostalCode] NVARCHAR(10),\\r\\n    [Total] NUMERIC(10,2)  NOT NULL,\\r\\n    FOREIGN KEY ([CustomerId]) REFERENCES \"customers\" ([CustomerId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'invoice_items', 'invoice_items', 13, 'CREATE TABLE \"invoice_items\"\\r\\n(\\r\\n    [InvoiceLineId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [InvoiceId] INTEGER  NOT NULL,\\r\\n    [TrackId] INTEGER  NOT NULL,\\r\\n    [UnitPrice] NUMERIC(10,2)  NOT NULL,\\r\\n    [Quantity] INTEGER  NOT NULL,\\r\\n    FOREIGN KEY ([InvoiceId]) REFERENCES \"invoices\" ([InvoiceId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION,\\r\\n    FOREIGN KEY ([TrackId]) REFERENCES \"tracks\" ([TrackId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'media_types', 'media_types', 15, 'CREATE TABLE \"media_types\"\\r\\n(\\r\\n    [MediaTypeId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(120)\\r\\n)')\n('table', 'playlists', 'playlists', 16, 'CREATE TABLE \"playlists\"\\r\\n(\\r\\n    [PlaylistId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(120)\\r\\n)')\n('table', 'playlist_track', 'playlist_track', 17, 'CREATE TABLE \"playlist_track\"\\r\\n(\\r\\n    [PlaylistId] INTEGER  NOT NULL,\\r\\n    [TrackId] INTEGER  NOT NULL,\\r\\n    CONSTRAINT [PK_PlaylistTrack] PRIMARY KEY  ([PlaylistId], [TrackId]),\\r\\n    FOREIGN KEY ([PlaylistId]) REFERENCES \"playlists\" ([PlaylistId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION,\\r\\n    FOREIGN KEY ([TrackId]) REFERENCES \"tracks\" ([TrackId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'tracks', 'tracks', 20, 'CREATE TABLE \"tracks\"\\r\\n(\\r\\n    [TrackId] INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\\r\\n    [Name] NVARCHAR(200)  NOT NULL,\\r\\n    [AlbumId] INTEGER,\\r\\n    [MediaTypeId] INTEGER  NOT NULL,\\r\\n    [GenreId] INTEGER,\\r\\n    [Composer] NVARCHAR(220),\\r\\n    [Milliseconds] INTEGER  NOT NULL,\\r\\n    [Bytes] INTEGER,\\r\\n    [UnitPrice] NUMERIC(10,2)  NOT NULL,\\r\\n    FOREIGN KEY ([AlbumId]) REFERENCES \"albums\" ([AlbumId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION,\\r\\n    FOREIGN KEY ([GenreId]) REFERENCES \"genres\" ([GenreId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION,\\r\\n    FOREIGN KEY ([MediaTypeId]) REFERENCES \"media_types\" ([MediaTypeId]) \\r\\n\\t\\tON DELETE NO ACTION ON UPDATE NO ACTION\\r\\n)')\n('table', 'sqlite_stat1', 'sqlite_stat1', 864, 'CREATE TABLE sqlite_stat1(tbl,idx,stat)')\n\n\nOften we want to put this information into our common pandas data frame format!\n\nimport pandas as pd\n\n\n#create a data frame\nschema_df = pd.DataFrame(result)\nschema_df\n\n\n  \n    \n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nPretty good, just missing column names. For the schema table, those are standardized.\n\n#current column names\nschema_df.columns\n\nRangeIndex(start=0, stop=5, step=1)\n\n\n\nschema_df.rename(columns={0: 'type', 1: 'name', 2: 'tbl_name', 3: 'rootpage', 4: 'sql'}, inplace = True)\nschema_df\n\n\n  \n    \n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nAlternatively, we can use the function read_sql() from pandas to automatically put the result into a data frame!\n\nHere we don’t need a cursor object, we just pass the SQL string and the connection object.\n\n\nschema_df2 = pd.read_sql(get_schema, con)\nschema_df2\n\n\n  \n    \n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nAhh, that’s better!\n\n\nQuerying a Table\nNow that we know which tables exist, we can query them! Let’s return all the albums in the albums table.\nWe go through the same process as before (as we closed our cursor object we need to create a new one):\n\nCreate our cursor using con.cursor()\nWrite our SQL command as a string\nExecute the SQL code using the .execute() method (cursor.execute()) on our cursor object\nUse the .fetchall() method (cursor.fetchall()) to actually return the data requested\nClose the conneciton made by the cursor (cursor.close())\n\nThe FROM function allows us to choose which table to query. LIMIT 20 says to limit what is returned to 20 rows.\n\n#create the cursor instance\ncursor = con.cursor()\n#create the SQL string\nget_albums = '''\n        SELECT *\n        FROM albums\n        LIMIT 20;\n        '''\n#execute the query\nalbums = cursor.execute(get_albums)\n#grab the results\nalbum_results = albums.fetchall()\n#close the cursor\ncursor.close()\n#check the results\nalbum_results\n\n[(1, 'For Those About To Rock We Salute You', 1),\n (2, 'Balls to the Wall', 2),\n (3, 'Restless and Wild', 2),\n (4, 'Let There Be Rock', 1),\n (5, 'Big Ones', 3),\n (6, 'Jagged Little Pill', 4),\n (7, 'Facelift', 5),\n (8, 'Warner 25 Anos', 6),\n (9, 'Plays Metallica By Four Cellos', 7),\n (10, 'Audioslave', 8),\n (11, 'Out Of Exile', 8),\n (12, 'BackBeat Soundtrack', 9),\n (13, 'The Best Of Billy Cobham', 10),\n (14, 'Alcohol Fueled Brewtality Live! [Disc 1]', 11),\n (15, 'Alcohol Fueled Brewtality Live! [Disc 2]', 11),\n (16, 'Black Sabbath', 12),\n (17, 'Black Sabbath Vol. 4 (Remaster)', 12),\n (18, 'Body Count', 13),\n (19, 'Chemical Wedding', 14),\n (20, 'The Best Of Buddy Guy - The Millenium Collection', 15)]\n\n\nOk, when just reading a table, let’s go with pd.read_sql() instead since it returns things in a nicer format!\n\nalbum_results = pd.read_sql(get_albums, con)\nalbum_results\n\n\n  \n    \n\n\n\n\n\n\nAlbumId\nTitle\nArtistId\n\n\n\n\n0\n1\nFor Those About To Rock We Salute You\n1\n\n\n1\n2\nBalls to the Wall\n2\n\n\n2\n3\nRestless and Wild\n2\n\n\n3\n4\nLet There Be Rock\n1\n\n\n4\n5\nBig Ones\n3\n\n\n5\n6\nJagged Little Pill\n4\n\n\n6\n7\nFacelift\n5\n\n\n7\n8\nWarner 25 Anos\n6\n\n\n8\n9\nPlays Metallica By Four Cellos\n7\n\n\n9\n10\nAudioslave\n8\n\n\n10\n11\nOut Of Exile\n8\n\n\n11\n12\nBackBeat Soundtrack\n9\n\n\n12\n13\nThe Best Of Billy Cobham\n10\n\n\n13\n14\nAlcohol Fueled Brewtality Live! [Disc 1]\n11\n\n\n14\n15\nAlcohol Fueled Brewtality Live! [Disc 2]\n11\n\n\n15\n16\nBlack Sabbath\n12\n\n\n16\n17\nBlack Sabbath Vol. 4 (Remaster)\n12\n\n\n17\n18\nBody Count\n13\n\n\n18\n19\nChemical Wedding\n14\n\n\n19\n20\nThe Best Of Buddy Guy - The Millenium Collection\n15",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Data Storage and Basic SQL"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#process-for-doing-a-crud-activity",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#process-for-doing-a-crud-activity",
    "title": "Data Storage and Basic SQL",
    "section": "Process for doing a CRUD activity",
    "text": "Process for doing a CRUD activity\n(Create, Read, Update, Delete)\nIf we wanted to do something other than just read a table, we need to follow our general structure from above:\n\nCreate a connection using sqlite3.connect(path)\nCreate a cursor object associated with the connection\nWrite an SQL query as a string\nUse cursor.execute() (or cursor.executemany()) to execute the SQL\nClose the cursor object",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Data Storage and Basic SQL"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#common-sql-commands",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#common-sql-commands",
    "title": "Data Storage and Basic SQL",
    "section": "Common SQL commands",
    "text": "Common SQL commands\n\nCREATE TABLE - creates a new table\nINSERT INTO - adds records to a table\nUPDATE - modify existing records\nDELETE FROM - deletes data\nDROP TABLE - removes a table\nSELECT - reads data (use fetchone(), fetchall(), or the returned value as an iterator but we’ll use pd.read_sql() for simple read commands)\n\nAlso many important joins we’ll cover shortly.\nLet’s go through a few of these actions to show how we can write our SQL code and execute it! Note that SQL code is not case sensitive.\n\nCreate a Table\nLet’s start by creating a table. We write the SQL code and specify the name of the table and and variables we want to create.\n\nWe specify the type of data the variable will hold after naming the variable\nCheck this reference for possible data types!\n\n\n#create the cursor instance\ncursor = con.cursor()\n#write our SQL to create a table\n#here we also specify two variables: album and artist along with the type of data they'll hold\nct = \"\"\"\n    CREATE TABLE IF NOT EXISTS justin_music (\n        album TEXT,\n        artist TEXT);\n\"\"\"\n#execute the SQL code\ncursor.execute(ct)\n\n&lt;sqlite3.Cursor at 0x7a23d8f825c0&gt;\n\n\nLet’s check that it worked before we close the connection!\n\npd.read_sql('''\n        SELECT *\n        FROM justin_music;\n        ''', con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\nCool, now let’s close the connection.\n\ncursor = con.cursor()\n\nYou can see we’ll do this process a lot. It may be best to create a function to help us out. (Modified from https://realpython.com/python-sql-libraries/)\n\ndef execute_query(connection, query):\n    cursor = connection.cursor()\n    try:\n        cursor.execute(query)\n        print(\"Query executed successfully\")\n    except Error as e:\n        print(f\"The error '{e}' occurred\")\n    cursor.close()\n\nThe function will execute the query for us and close the cursor.\n\nexecute_query(con, ct)\n\nQuery executed successfully\n\n\n\n\nINSERT INTO\nNow lets add some data to our table using INSERT INTO. We need to pass the table name, optionally the columns we’ll specify, and the values to fill with.\n\ncreate_rows = \"\"\"\n       INSERT INTO\n           justin_music (album, artist)\n       VALUES\n           (\"Sixteen Stone\", \"Bush\"),\n           (\"Listener Supported\", \"Dave Matthews Band\"),\n           (\"Chris Stapleton\", \"Traveler\"),\n           (\"1989\", \"Taylor Swift\");\n\"\"\"\nexecute_query(con, create_rows)\n\nQuery executed successfully\n\n\nLet’s check if it worked!\n\npd.read_sql(\"SELECT * FROM justin_music\", con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n0\nSixteen Stone\nBush\n\n\n1\nListener Supported\nDave Matthews Band\n\n\n2\nChris Stapleton\nTraveler\n\n\n3\n1989\nTaylor Swift\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow let’s add a row with only an artist.\n\ncreate_row = \"\"\"\n       INSERT INTO\n           justin_music (artist)\n       VALUES\n           (\"Taylor Swift\");\n\"\"\"\n\nexecute_query(con, create_row)\n\nQuery executed successfully\n\n\nThis inserts a None into the data frame but note that the value in the original table is actually a NULL.\n\npd.read_sql(\"SELECT * FROM justin_music\", con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n0\nSixteen Stone\nBush\n\n\n1\nListener Supported\nDave Matthews Band\n\n\n2\nChris Stapleton\nTraveler\n\n\n3\n1989\nTaylor Swift\n\n\n4\nNone\nTaylor Swift\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nUPDATE\nWe can use update to modify an existing row of data. We use\n\nUPDATE to specify the table\nSET to specify the values of the columns\nWHERE to describe which records (rows) to modify\n\nWe can use AND, OR, IN, and other logical operators in our SQL code! (See Operators and Parse-Affecting Attributes)\n\nmod_row = \"\"\"\n    UPDATE justin_music\n    SET album = \"Red (Taylor's Version)\", artist = \"Taylor Swift\"\n    WHERE (album is null) AND (artist = \"Taylor Swift\");\n\"\"\"\n\nexecute_query(con, mod_row)\n\nQuery executed successfully\n\n\n\npd.read_sql(\"SELECT * FROM justin_music\", con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n0\nSixteen Stone\nBush\n\n\n1\nListener Supported\nDave Matthews Band\n\n\n2\nChris Stapleton\nTraveler\n\n\n3\n1989\nTaylor Swift\n\n\n4\nRed (Taylor's Version)\nTaylor Swift\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nDELETE FROM\nWe can also delete certain rows. We use\n\nDELETE FROM to specify the table\na WHERE condition to determine what is deleted\n\n\ndel_row = \"\"\"\n    DELETE FROM justin_music\n    WHERE artist = \"Taylor Swift\";\n    \"\"\"\n\nexecute_query(con, del_row)\n\nQuery executed successfully\n\n\n\npd.read_sql(\"SELECT * FROM justin_music\", con)\n\n\n  \n    \n\n\n\n\n\n\nalbum\nartist\n\n\n\n\n0\nSixteen Stone\nBush\n\n\n1\nListener Supported\nDave Matthews Band\n\n\n2\nChris Stapleton\nTraveler\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nDROP TABLE\nAnd of course we can remove an entire table using DROP TABLE. Just specify the table to remove!\n\nexecute_query(con, \"DROP TABLE justin_music\")\n\nQuery executed successfully\n\n\n\npd.read_sql(get_schema, con)\n\n\n  \n    \n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nMy table is gone :(\n\n\nSELECT\nAs a statistician, you’d spend most of your time pulling data of interest and then summarizing it, running models, etc.\nSELECT is the workhorse for that task! As we are just reading in data from the database, we can just use pd.read_sql().\nWith SELECT we\n\nSELECT the column(s) we want\nFROM the table of interest\nSpecifing the records (rows) of interest with a WHERE\n\nFirst, let’s see a way to look at all columns in a table.\n\npd.read_sql('SELECT * FROM albums', con)\n\n\n  \n    \n\n\n\n\n\n\nAlbumId\nTitle\nArtistId\n\n\n\n\n0\n1\nFor Those About To Rock We Salute You\n1\n\n\n1\n2\nBalls to the Wall\n2\n\n\n2\n3\nRestless and Wild\n2\n\n\n3\n4\nLet There Be Rock\n1\n\n\n4\n5\nBig Ones\n3\n\n\n...\n...\n...\n...\n\n\n342\n343\nRespighi:Pines of Rome\n226\n\n\n343\n344\nSchubert: The Late String Quartets & String Qu...\n272\n\n\n344\n345\nMonteverdi: L'Orfeo\n273\n\n\n345\n346\nMozart: Chamber Music\n274\n\n\n346\n347\nKoyaanisqatsi (Soundtrack from the Motion Pict...\n275\n\n\n\n\n347 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow let’s select the Title and ArtistID columns only. LIMIT is a good way to only return (up to) a certain number of results.\n\npd.read_sql(\"SELECT Title, artistID FROM albums LIMIT 10;\", con)\n\n\n  \n    \n\n\n\n\n\n\nTitle\nArtistId\n\n\n\n\n0\nFor Those About To Rock We Salute You\n1\n\n\n1\nBalls to the Wall\n2\n\n\n2\nRestless and Wild\n2\n\n\n3\nLet There Be Rock\n1\n\n\n4\nBig Ones\n3\n\n\n5\nJagged Little Pill\n4\n\n\n6\nFacelift\n5\n\n\n7\nWarner 25 Anos\n6\n\n\n8\nPlays Metallica By Four Cellos\n7\n\n\n9\nAudioslave\n8\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nWHERE allows us to specify conditions on rows.\n\npd.read_sql(\"SELECT * FROM albums WHERE artistID = 2 LIMIT 10;\", con)\n\n\n  \n    \n\n\n\n\n\n\nAlbumId\nTitle\nArtistId\n\n\n\n\n0\n2\nBalls to the Wall\n2\n\n\n1\n3\nRestless and Wild\n2",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Data Storage and Basic SQL"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/03-Databases_SQL.html#clean-up",
    "href": "02_Big_Data_Management/03-Databases_SQL.html#clean-up",
    "title": "Data Storage and Basic SQL",
    "section": "Clean Up",
    "text": "Clean Up\nIf we want to save any changes made to our database, we need to do a commit() to the connection via con.commit(). This saves the changes made and releases any locks on the data. Other connections to the database can only see changes you’ve made if you commit them.\nWhen you are done working you should also close your connection. This is done with the close() method.\n\ncon.close()\n\nWe skimmed over some of the syntax that SQL follows. You should read over the arithmetic operators, comparison operators, and the other operators linked on the left of that page.",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Data Storage and Basic SQL"
    ]
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html#errors-when-programming",
    "href": "01_Programming_in_python/21-Error_Handling.html#errors-when-programming",
    "title": "Error Handling",
    "section": "Errors When Programming",
    "text": "Errors When Programming\nCommonly you’ll have syntax errors and exceptions\n\nA Syntax error is when you typed something in wrong (and it can’t be parsed by python)\n\n\nfor x in range(0,10) #missing colon\n    print(x)\n\n\n  File \"&lt;ipython-input-1-e60b965da02e&gt;\", line 1\n    for x in range(0,10) #missing colon\n                         ^\nSyntaxError: expected ':'\n\n\n\n\nSyntaxError: invalid syntax (&lt;string&gt;, line 1)\n\nThese aren’t things we can fix on the backend. Code needs to be typed in correctly!\nAn exception occurs when python can’t execute your code (during the execution something bad happens)\n\n\nprint(\"the number is \" + 10)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-2-8cddb0f2de90&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 print(\"the number is \" + 10)\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nTypeError: can only concatenate str (not \"int\") to str",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Error Handling"
    ]
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html#dealing-with-exceptions",
    "href": "01_Programming_in_python/21-Error_Handling.html#dealing-with-exceptions",
    "title": "Error Handling",
    "section": "Dealing with Exceptions",
    "text": "Dealing with Exceptions\nExceptions can be dealt with to some degree! Consider this function to print out strings passed by the user.\n\ndef print_strings(*x): #should all be strings!\n    c = 0\n    for i in x:\n        print(\"The value in position \" + str(c) + \" is: \" + i)\n        c += 1\n\nprint_strings(\"cat\", \"dog\", \"bird\")\n\nThe value in position 0 is: cat\nThe value in position 1 is: dog\nThe value in position 2 is: bird\n\n\n\nIf we pass a non-string, this will throw an exception.\n\n\ndef print_strings(*x): #should all be strings!\n    c = 0\n    for i in x:\n        print(\"The value in position \" + str(c) + \" is: \" + i)\n        c += 1\n\nprint_strings(\"cat\", 1, \"bird\")\n\nThe value in position 0 is: cat\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-4-15f25c060c0e&gt; in &lt;cell line: 7&gt;()\n      5         c += 1\n      6 \n----&gt; 7 print_strings(\"cat\", 1, \"bird\")\n\n&lt;ipython-input-4-15f25c060c0e&gt; in print_strings(*x)\n      2     c = 0\n      3     for i in x:\n----&gt; 4         print(\"The value in position \" + str(c) + \" is: \" + i)\n      5         c += 1\n      6 \n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\n\nNote that it says TypeError at the beginning of that big error message. What we can do is run a try block and set up what to do when we get certain exceptions via an except block.\nThe syntax for these is simlar to if and else blocks\n\n\ndef print_strings(*x):\n    c = 0\n    for i in x:\n        try: #try the code that is indented below here\n            print(\"The value in position \" + str(c) + \" is: \" + i)\n        except TypeError: #if we get a TypeError in the previous try block, do this and then continue execution!\n            print(\"Oh no! Not a string\")\n        c += 1\nprint_strings(\"cat\", \"dog\", 1, \"bird\")\n\nThe value in position 0 is: cat\nThe value in position 1 is: dog\nOh no! Not a string\nThe value in position 3 is: bird\n\n\n\nCan have multiple except statements and an else block to account for many situations\nLet’s create a quick function to printout information about a person given as key/value pairs\n\n\ndef print_stuff(**x): #now taking key value pairs (a dictionary within the function)\n    print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\") #we must have a Name argument\n    for key in x: #now run through the other info given and print it out\n        print(\"\\t\", key, \" : \", str(x[key])) #\\t is a tab\n\nprint_stuff(Name = \"Jack London\", Age = 41, Job = \"Writer\")\n\nPay special attention to Jack London\nHis attributes are:\n     Age  :  41\n     Job  :  Writer\n\n\nIf we don’t have a Name argument passed, we’d have an error!\n\nprint_stuff(Person = \"Jack London\", Age = 41, Job = \"Writer\")\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-16-66ebf0cac27f&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 print_stuff(Person = \"Jack London\", Age = 41, Job = \"Writer\")\n\n&lt;ipython-input-15-a21ff30dfb20&gt; in print_stuff(**x)\n      1 def print_stuff(**x): #now taking key value pairs (a dictionary within the function)\n----&gt; 2     print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\") #we must have a Name argument\n      3     for key in x: #now run through the other info given and print it out\n      4         print(\"\\t\", key, \" : \", str(x[key])) #\\t is a tab\n      5 \n\nKeyError: 'Name'\n\n\n\nIn this case we get a KeyError (this is when we look for a certain key in a dictionary (or other similar object) and can’t find it) - We can look for this kind of error - Also look for other errors that might come up\n\ndef print_stuff(**x): #now taking key value pairs\n    try:\n        print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\")\n    except TypeError: #If x.pop(\"Name\") doesn't give a string we'll get a TypeError\n        print(\"Oh no! 'Name' is not a string\")\n    except KeyError: #If the user didn't give a \"Name\" argument we'll get a KeyError\n        print(\"You didn't supply a 'Name'!\")\n    for key in x: #Now print out the rest of the info about the person\n        print(\"\\t\", key, \" : \", str(x[key]))\nprint_stuff(Name = \"Jack London\", Age = 41, Job = \"Writer\")\n\nPay special attention to Jack London\nHis attributes are:\n     Age  :  41\n     Job  :  Writer\n\n\n\nprint_stuff(Name = 11, Age = 41, Job = \"Writer\")\n\nOh no! 'Name' is not a string\n     Age  :  41\n     Job  :  Writer\n\n\n\nprint_stuff(Person = \"Jack London\", Age = 41, Job = \"Writer\")\n\nYou didn't supply a 'Name'!\n     Person  :  Jack London\n     Age  :  41\n     Job  :  Writer\n\n\nAn else block can be given that is similar to what we use with if, elif logic - This specifies what to do if things aren’t accounted for above\n\ndef print_stuff(**x): #now taking key value pairs\n    try:\n        print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\")\n    except TypeError:\n        print(\"Oh no! 'Name' is not a string\")\n    except KeyError:\n        print(\"You didn't supply a 'Name'!\")\n    else: #if no errors occurred\n        print(\"(Valid name by the way - you rule)\")\n    for key in x:\n        print(\"\\t\", key, \" : \", str(x[key]))\n\nprint_stuff(Name = \"Jack London\", Age = 41, Job = \"Writer\")\n\nPay special attention to Jack London\nHis attributes are:\n(Valid name by the way - you rule)\n     Age  :  41\n     Job  :  Writer\n\n\n\nA finally clause can be given to always execute at end regardless\nLike an else block but it always runs\n\n\ndef print_stuff(**x): #now taking key value pairs\n    print(x)\n    try:\n        print(\"Pay special attention to \" + x.pop(\"Name\") + \"\\nHis attributes are:\")\n    except TypeError:\n        print(\"Oh no! 'Name' is not a string\")\n    except KeyError:\n        print(\"You didn't supply a 'Name'!\")\n    else:\n        print(\"(Valid name by the way)\")\n    finally:\n        print(\"This string prints no matter what\")\n    for key in x:\n        print(\"\\t\", key, \" : \", str(x[key]))\n\nprint_stuff(name = \"Jack London\", Age = 41, Job = \"Writer\")\n\n{'name': 'Jack London', 'Age': 41, 'Job': 'Writer'}\nYou didn't supply a 'Name'!\nThis string prints no matter what\n     name  :  Jack London\n     Age  :  41\n     Job  :  Writer",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Error Handling"
    ]
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html#raising-an-exception-yourself",
    "href": "01_Programming_in_python/21-Error_Handling.html#raising-an-exception-yourself",
    "title": "Error Handling",
    "section": "Raising an Exception Yourself",
    "text": "Raising an Exception Yourself\n\nYou can define your own exceptions to be more descriptive!\nIf we try to divide by 0 we get a ZeroDivisionError exception\n\n\n3/0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n&lt;ipython-input-25-f6cc6d14333b&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 3/0\n\nZeroDivisionError: division by zero\n\n\n\n\nInstead we can raise our own exception\n\n\ndef my_divide(x, y):\n    if y == 0:\n        raise Exception(\"Can't divide by 0...\")\n    return x/y\n\n\nmy_divide(3, 9) #works\n\n0.3333333333333333\n\n\n\nmy_divide(3,0) #raises our custom exception and provides that note!\n\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n&lt;ipython-input-28-c7c11902dc96&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 my_divide(3,0) #raises our custom exception and provides that note!\n\n&lt;ipython-input-26-418f7e821d7e&gt; in my_divide(x, y)\n      1 def my_divide(x, y):\n      2     if y == 0:\n----&gt; 3         raise Exception(\"Can't divide by 0...\")\n      4     return x/y\n\nException: Can't divide by 0...",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Error Handling"
    ]
  },
  {
    "objectID": "01_Programming_in_python/21-Error_Handling.html#quick-video",
    "href": "01_Programming_in_python/21-Error_Handling.html#quick-video",
    "title": "Error Handling",
    "section": "Quick Video",
    "text": "Quick Video\nThis video shows an example of using error control! Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=cd799e52-f4f1-4531-8174-b10301708125&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Error Handling"
    ]
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html#plotting-systems-in-python",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html#plotting-systems-in-python",
    "title": "Plotting with matplotlib",
    "section": "Plotting Systems in python",
    "text": "Plotting Systems in python\n\nmatplotlib: based on matlab plotting. Similar to base R plotting\nseaborn: an abstraction of matplotlib but still growing\nBokeh: for interactive visuals via HTML\nplotly: general plotting system that has a python module\nplotnine: a ggplot port",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Plotting with `matplotlib`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html#plotting-with-matplotlib-1",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html#plotting-with-matplotlib-1",
    "title": "Plotting with matplotlib",
    "section": "Plotting with matplotlib",
    "text": "Plotting with matplotlib\n\nTwo APIs (or ways to interact with matplotlib)\n\nExplicit axes interface (object oriented api)\nImplicit pyplot interface (what we’ll cover)\n\nWhen using the implicit API we use functions like\n\nplt.figure(), plt.plot(...), plt.scatter(), plt.bar(), or plt.hist()\n\nWe then determine axes and artist elements\n\nWe add labels, legends, and annotations\nFinally we produce the plot (and would then usually close the plot to denote that we are done working on it - not usually needed when programming in jupyter notebooks)\n\nplt.show() then plt.close()\n\n\n\nReading in Data to Plot\n\nConsider data on titanic passengers in titanic.csv\nThis is a really common dataset to play around with\nLet’s start with a focus on plotting categorical data\n\nWe start by importing matplotlib.pyplot as plt. This is a common reference. The pyplot module has the functions we’ll use to do our plotting such as pyplot.hist() or pyplot.plot().\n\nimport matplotlib.pyplot as plt\n\nNow we’ll read in the titanic dataset using pandas. This dataset is available at: https://www4.stat.ncsu.edu/~online/datasets/titanic.csv\n\nimport pandas as pd\n#readin data\ntitanic_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/titanic.csv\")\n\nSome of the variables in the data have a lot of missing values. Let’s drop those. We also saw that converting some variables to category type variables was useful for having better labeling. Let’s do that as well.\n\n#remove some columns and a bad row\nsub_titanic_data = titanic_data.drop(columns = [\"body\", \"cabin\", \"boat\"], axis = 1) \\\n                               .iloc[:(titanic_data.shape[0]-1)]\n\n#create category versions of the variables\n#embarked variable\nsub_titanic_data[\"embarkedC\"] = sub_titanic_data.embarked.astype(\"category\")\nsub_titanic_data.embarkedC = sub_titanic_data.embarkedC.cat.rename_categories(\n                                    [\"Cherbourg\", \"Queenstown\", \"Southampton\"])\n#sex variable\nsub_titanic_data[\"sexC\"] = sub_titanic_data.sex.astype(\"category\")\nsub_titanic_data.sexC = sub_titanic_data.sexC.cat.rename_categories([\"Female\", \"Male\"])\n#survived variable\nsub_titanic_data[\"survivedC\"] = sub_titanic_data.survived.astype(\"category\")\nsub_titanic_data.survivedC = sub_titanic_data.survivedC.cat.rename_categories([\"Died\", \"Survived\"])\n\n\n\n\nBarplots\nCategorical variable - entries are a label or attribute\nOur goal is to describe the distribution of these variables. We do this by creating summary counts or frequncy counts\n\nBarplots give a visual of those counts!\n\nUse plt.bar()\n\nx represents the categories\nheight the corresponding heights\n\n\n\nWe have three categorical variables we’ll investigate. Let’s start with the embarkedC variable.\nWe know the x values (the category labels). We just need the heights to plot. We can find the heights by creating a one-way contingency table!\n\ntable = sub_titanic_data.embarkedC.value_counts()\ntable\n\n\n\n\n\n\n\n\ncount\n\n\nembarkedC\n\n\n\n\n\nSouthampton\n914\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\n\n\ndtype: int64\n\n\nNow we’ll use plt.bar() and map the categories (via calling the .categories attribute from our column of data) to x and the contingency table counts to height.\nThe x and height values just need to be paired up.\n\n#get the categories\nprint(sub_titanic_data.embarkedC.cat.categories)\n#note that the ordering does not line up with the counts\nprint(table)\n\nIndex(['Cherbourg', 'Queenstown', 'Southampton'], dtype='object')\nembarkedC\nSouthampton    914\nCherbourg      270\nQueenstown     123\nName: count, dtype: int64\n\n\nAs the ordering isn’t the same, we’ll have to be careful to make sure things are paired up appropriately!\n\nindex = [1, 2, 0]\ntable[index]\n\nFutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  table[index]\n\n\n\n\n\n\n\n\n\ncount\n\n\nembarkedC\n\n\n\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\nSouthampton\n914\n\n\n\n\ndtype: int64\n\n\n\nplt.bar(x = sub_titanic_data.embarkedC.cat.categories,  height = table[index])\n#plt.show() would be needed if we weren't in a notebook\n\n\n\n\n\n\n\n\nNice! A good looking barplot. This is our first step. Next we want to make it look a bit nicer by adding labels, legends, and annotations.\nIn this case, we can add a better xlabel, ylabel, and title.\n\nplt.bar(x = sub_titanic_data.embarkedC.cat.categories,  height =  table[index])\n#as these are all being run in the same cell, these get added to the plot created\nplt.xlabel(\"Port Embarked\")\nplt.ylabel(\"Number of People\")\nplt.title(\"Most Embarked in the Southampton Port\")\n\nText(0.5, 1.0, 'Most Embarked in the Southampton Port')\n\n\n\n\n\n\n\n\n\n\nA common way to resize the plot is to first call plt.subplots() and specify the figsize argument. We give this a tuple of the width and height we want.\n\n\nplt.subplots(figsize = (12, 5))\nplt.bar(x = sub_titanic_data.embarkedC.cat.categories,  height =  table[index])\nplt.xlabel(\"Port Embarked\")\nplt.ylabel(\"Number of People\")\nplt.title(\"Most Embarked in the Southampton Port\")\n\nText(0.5, 1.0, 'Most Embarked in the Southampton Port')\n\n\n\n\n\n\n\n\n\n\n\nStacked Barplot with matplotlib\nIf we want to include a second categorical variable in our plot we can do so in a few ways. The first is to color the bars by the values of the other variable. In this way we can see how that variable distributes across the categories of our current variable!\n\nThe first step is to create the table of counts for our two variables\nWe’ll do this via the pd.crosstab() function\n\n\nstack_table = pd.crosstab(sub_titanic_data.embarkedC, sub_titanic_data.survivedC)\nstack_table\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\n\n\nembarkedC\n\n\n\n\n\n\nCherbourg\n120\n150\n\n\nQueenstown\n79\n44\n\n\nSouthampton\n610\n304\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nTo manually build this bar plot with plt.bar() we can do the following: - Put our embarked variable labels on the x-axis - Create bars with heights corresponding to the counts for Died. - Create a second set of bars that sit directly on top of those bars with heights corresponding to the Survived counts - These bars should be different colors to denote the Died vs Survived counts!\nRemember that as we work on a plot within a cell, we continue to add to it unless we use plt.show(). This means we can use two calls to plt.bar() within the same cell and it will keep drawing on the same plot.\n\n#we want to get the heights for the Died bars\nstack_table.loc[:, \"Died\"]\n\n\n\n\n\n\n\n\nDied\n\n\nembarkedC\n\n\n\n\n\nCherbourg\n120\n\n\nQueenstown\n79\n\n\nSouthampton\n610\n\n\n\n\ndtype: int64\n\n\n\n#notice that these now line up with our category order so we don't need to change that\nsub_titanic_data.embarkedC.cat.categories\n\nIndex(['Cherbourg', 'Queenstown', 'Southampton'], dtype='object')\n\n\nOur first step is to plot these on a bar plot. We’ll add a label argument to plt.bar() which will make it easy to add a legend at the end.\n\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Died\"],\n  label = \"Died\")\n\n\n\n\n\n\n\n\nNow we want to find the Survived counts and put those on top of these bars.\n\nstack_table.loc[:, \"Survived\"]\n\n\n\n\n\n\n\n\nSurvived\n\n\nembarkedC\n\n\n\n\n\nCherbourg\n150\n\n\nQueenstown\n44\n\n\nSouthampton\n304\n\n\n\n\ndtype: int64\n\n\nWe can add a bottom = argument to our second plt.bar() call to specify where the bottom of the bars should start (the counts of the Died!)\n\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Died\"],\n  label = \"Died\")\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Survived\"],\n  bottom = stack_table.loc[:, \"Died\"],\n  label = \"Survived\"\n)\n\n\n\n\n\n\n\n\nNice! Colors are automatically created for us. Now we just need to add some nice labels to help us understand the plot.\nWe’ll use plt.legend() to produce a legend based off the labels used.\n\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Died\"],\n  label = \"Died\")\nplt.bar(\n  x = sub_titanic_data.embarkedC.cat.categories,\n  height = stack_table.loc[:, \"Survived\"],\n  bottom = stack_table.loc[:, \"Died\"],\n  label = \"Survived\"\n  )\nplt.xlabel(\"Port Embarked\")\nplt.ylabel(\"Number of People\")\nplt.title(\"Most Embarked in the Southampton Port \\n A higher proportion survived from Cherbourg\")\nplt.legend(loc = 0)\n\n\n\n\n\n\n\n\n\n\n\nSide-by-Side Barplot with matplotlib\nAlternative to the stacked bar plot is the side-by-side bar plot. This is the same idea but we put the bars for the categories next to each other instead of on top of eachother.\nThis is similar to our first bar plot but we need to have different x locations for each bar!\nLet’s take this bar plot of just the Died with port of embarkment.\nWe can change the x values from the categories to numbers.\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  label = \"Died\")\n\n\n\n\n\n\n\n\nNow we can specify the widths of the bars via the width argument. By default they are almost 1 here. Let’s make them smaller.\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  width = 0.4,\n  label = \"Died\")\n\n\n\n\n\n\n\n\nOk, now let’s just fix the x-axis labels! This can be done by using plt.xticks(). Here we specify the x values where we want our axis values to go along with corresponding labels.\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  width = 0.4,\n  label = \"Died\")\nplt.xticks([1, 2, 3], sub_titanic_data.embarkedC.cat.categories)\n\n([&lt;matplotlib.axis.XTick at 0x796e52385c30&gt;,\n  &lt;matplotlib.axis.XTick at 0x796e52385c00&gt;,\n  &lt;matplotlib.axis.XTick at 0x796e52385450&gt;],\n [Text(1, 0, 'Cherbourg'),\n  Text(2, 0, 'Queenstown'),\n  Text(3, 0, 'Southampton')])\n\n\n\n\n\n\n\n\n\nSweet! Now we just add the bars for the Survived group next to these!\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  width = 0.4,\n  label = \"Died\")\nplt.bar(\n  x = [1.4, 2.4, 3.4],\n  height = stack_table.loc[:, \"Survived\"],\n  width = 0.4,\n  label = \"Survived\")\nplt.xticks([1.2, 2.2, 3.2], sub_titanic_data.embarkedC.cat.categories)\n\n([&lt;matplotlib.axis.XTick at 0x796e523f05e0&gt;,\n  &lt;matplotlib.axis.XTick at 0x796e523f05b0&gt;,\n  &lt;matplotlib.axis.XTick at 0x796e523a5ff0&gt;],\n [Text(1.2, 0, 'Cherbourg'),\n  Text(2.2, 0, 'Queenstown'),\n  Text(3.2, 0, 'Southampton')])\n\n\n\n\n\n\n\n\n\nNow we’ll fancy it up with some labels and titles.\n\nplt.bar(\n  x = [1, 2, 3],\n  height = stack_table.loc[:, \"Died\"],\n  width = 0.4,\n  label = \"Died\")\nplt.bar(\n  x = [1.4, 2.4, 3.4],\n  height = stack_table.loc[:, \"Survived\"],\n  width = 0.4,\n  label = \"Survived\")\nplt.xticks([1.2, 2.2, 3.2], sub_titanic_data.embarkedC.cat.categories)\nplt.xlabel(\"Port Embarked\")\nplt.ylabel(\"Number of People\")\nplt.legend(loc = 0)\nplt.title(\"Most Embarked in the Southampton Port \\n A higher proportion survived from Cherbourg\")\n\nText(0.5, 1.0, 'Most Embarked in the Southampton Port \\n A higher proportion survived from Cherbourg')\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Numeric Variables\nWhoa - that was way too much work to create side-by-side bar plots… We could go through similar processes to create histograms, scatterplots, etc…\nFunctions like plt.scatter() aren’t bad to work with:\n\nplt.scatter(sub_titanic_data.age, sub_titanic_data.fare)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Fare\")\n\nText(0, 0.5, 'Fare')\n\n\n\n\n\n\n\n\n\nBut customizing the plots is a good bit of work. pandas has functionality to do plotting on data frames that will save us time!\nHowever, it is really useful to know the basics of matplotlib as many of the plotting systems are built on it!",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Plotting with `matplotlib`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html#quick-video",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html#quick-video",
    "title": "Plotting with matplotlib",
    "section": "Quick Video",
    "text": "Quick Video\nThis video shows an example of using matplotlib plotting! Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=380014c4-f479-4ab1-b0d6-b1030168e8d1&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Plotting with `matplotlib`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/19-Plotting_matplotlib.html#recap",
    "href": "01_Programming_in_python/19-Plotting_matplotlib.html#recap",
    "title": "Plotting with matplotlib",
    "section": "Recap",
    "text": "Recap\n\nMust understand the type of data you have to visualize it\nGoal: Describe the distribution\nmatplotlib can create custom plots\n\nLots of work to specify everything yourself\n\nMany other plotting paradigms to consider!\n\npandas and seaborn next\n\n\nIf you are on the course website, use the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!\nIf you are on Google Colab, head back to our course website for our next lesson!",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Plotting with `matplotlib`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#understand-how-data-is-stored",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#understand-how-data-is-stored",
    "title": "Numerical Summaries",
    "section": "Understand How Data is Stored",
    "text": "Understand How Data is Stored\nFirst, let’s read in some data. Recall, for .csv files (comma separated value files) we can read them in using pandas read_csv() function.\nWe’ll read in the classic titanic data set.\n\nimport pandas as pd\ntitanic_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/titanic.csv\")\n\n\nThe .info() method allows us to see how our variables are stored (among other things)\nColumn data types should make sense for what you expect!\n\n\ntitanic_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1310 entries, 0 to 1309\nData columns (total 14 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   pclass     1309 non-null   float64\n 1   survived   1309 non-null   float64\n 2   name       1309 non-null   object \n 3   sex        1309 non-null   object \n 4   age        1046 non-null   float64\n 5   sibsp      1309 non-null   float64\n 6   parch      1309 non-null   float64\n 7   ticket     1309 non-null   object \n 8   fare       1308 non-null   float64\n 9   cabin      295 non-null    object \n 10  embarked   1307 non-null   object \n 11  boat       486 non-null    object \n 12  body       121 non-null    float64\n 13  home.dest  745 non-null    object \ndtypes: float64(7), object(7)\nmemory usage: 143.4+ KB\n\n\n\n.head() and .tail() help to see what we have as well\n\n\ntitanic_data.head() #clearly some missing values with NaNs\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n0\n1.0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nB5\nS\n2\nNaN\nSt Louis, MO\n\n\n1\n1.0\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\n11\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\n0.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\n0.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\n135.0\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\n0.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ntitanic_data.tail() #note the last row of NaN (not a number)\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n1305\n3.0\n0.0\nZabour, Miss. Thamine\nfemale\nNaN\n1.0\n0.0\n2665\n14.4542\nNaN\nC\nNaN\nNaN\nNaN\n\n\n1306\n3.0\n0.0\nZakarian, Mr. Mapriededer\nmale\n26.5\n0.0\n0.0\n2656\n7.2250\nNaN\nC\nNaN\n304.0\nNaN\n\n\n1307\n3.0\n0.0\nZakarian, Mr. Ortin\nmale\n27.0\n0.0\n0.0\n2670\n7.2250\nNaN\nC\nNaN\nNaN\nNaN\n\n\n1308\n3.0\n0.0\nZimmerman, Mr. Leo\nmale\n29.0\n0.0\n0.0\n315082\n7.8750\nNaN\nS\nNaN\nNaN\nNaN\n\n\n1309\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Numerical Summaries"
    ]
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#do-basic-data-validation",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#do-basic-data-validation",
    "title": "Numerical Summaries",
    "section": "Do Basic Data Validation",
    "text": "Do Basic Data Validation\n\nUse the describe() method on a data frame\nCheck that the min’s, max’s, etc. all make sense!\n\n\ntitanic_data.describe()\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nage\nsibsp\nparch\nfare\nbody\n\n\n\n\ncount\n1309.000000\n1309.000000\n1046.000000\n1309.000000\n1309.000000\n1308.000000\n121.000000\n\n\nmean\n2.294882\n0.381971\n29.881135\n0.498854\n0.385027\n33.295479\n160.809917\n\n\nstd\n0.837836\n0.486055\n14.413500\n1.041658\n0.865560\n51.758668\n97.696922\n\n\nmin\n1.000000\n0.000000\n0.166700\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n25%\n2.000000\n0.000000\n21.000000\n0.000000\n0.000000\n7.895800\n72.000000\n\n\n50%\n3.000000\n0.000000\n28.000000\n0.000000\n0.000000\n14.454200\n155.000000\n\n\n75%\n3.000000\n1.000000\n39.000000\n1.000000\n0.000000\n31.275000\n256.000000\n\n\nmax\n3.000000\n1.000000\n80.000000\n8.000000\n9.000000\n512.329200\n328.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nRecall we can subset our columns with []\n\n\ntitanic_data.columns\n\nIndex(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n      dtype='object')\n\n\n\nWe can determine which percentiles of selected columns to return by combining the column subsetting via selection brackets [] and the .describe() method\n\n\ntitanic_data[[\"age\", \"sibsp\", \"parch\", \"fare\"]].describe(percentiles = [0.05, 0.25, 0.99])\n\n\n  \n    \n\n\n\n\n\n\nage\nsibsp\nparch\nfare\n\n\n\n\ncount\n1046.000000\n1309.000000\n1309.000000\n1308.000000\n\n\nmean\n29.881135\n0.498854\n0.385027\n33.295479\n\n\nstd\n14.413500\n1.041658\n0.865560\n51.758668\n\n\nmin\n0.166700\n0.000000\n0.000000\n0.000000\n\n\n5%\n5.000000\n0.000000\n0.000000\n7.225000\n\n\n25%\n21.000000\n0.000000\n0.000000\n7.895800\n\n\n50%\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n99%\n65.000000\n5.000000\n4.000000\n262.375000\n\n\nmax\n80.000000\n8.000000\n9.000000\n512.329200",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Numerical Summaries"
    ]
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#determine-rate-of-missing-values",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#determine-rate-of-missing-values",
    "title": "Numerical Summaries",
    "section": "Determine Rate of Missing Values",
    "text": "Determine Rate of Missing Values\n\nUse is.null() method to determine the missing values\n\n\ntitanic_data.isnull()\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1305\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n1306\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n1307\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n1308\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\n1309\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n1310 rows × 14 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nYikes! Can’t make heads or tails of that.\nThis is a DataFrame of booleans!\nUse the .sum() method to see how many null values we have for each column\n\n\ntitanic_data.isnull().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\npclass\n1\n\n\nsurvived\n1\n\n\nname\n1\n\n\nsex\n1\n\n\nage\n264\n\n\nsibsp\n1\n\n\nparch\n1\n\n\nticket\n1\n\n\nfare\n2\n\n\ncabin\n1015\n\n\nembarked\n3\n\n\nboat\n824\n\n\nbody\n1189\n\n\nhome.dest\n565\n\n\n\n\ndtype: int64\n\n\n\nThis type of multiple method use gives us a good chance to use our \\ operator to create more readable code by making it multi-line\n\n\ntitanic_data.isnull() \\\n  .sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\npclass\n1\n\n\nsurvived\n1\n\n\nname\n1\n\n\nsex\n1\n\n\nage\n264\n\n\nsibsp\n1\n\n\nparch\n1\n\n\nticket\n1\n\n\nfare\n2\n\n\ncabin\n1015\n\n\nembarked\n3\n\n\nboat\n824\n\n\nbody\n1189\n\n\nhome.dest\n565\n\n\n\n\ndtype: int64",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Numerical Summaries"
    ]
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#clean-up-data-as-needed",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#clean-up-data-as-needed",
    "title": "Numerical Summaries",
    "section": "Clean Up Data As Needed",
    "text": "Clean Up Data As Needed\n\nWe can remove rows with missing using .dropna() method\nFirst, remove the cabin, boat, and body variables since they have so many missing values\n\nIf we want to just remove some columns, can use the .drop() method\n\n\n\nsub_titanic_data = titanic_data.drop(columns = [\"body\", \"cabin\", \"boat\"])\nsub_titanic_data.shape\n\n(1310, 11)\n\n\n\nCheck on the missingness now\n\n\nsub_titanic_data.isnull().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\npclass\n1\n\n\nsurvived\n1\n\n\nname\n1\n\n\nsex\n1\n\n\nage\n264\n\n\nsibsp\n1\n\n\nparch\n1\n\n\nticket\n1\n\n\nfare\n2\n\n\nembarked\n3\n\n\nhome.dest\n565\n\n\n\n\ndtype: int64\n\n\n\nNow we are ready to use the .dropna() method to remove any rows with missing data\n\n\ntemp = sub_titanic_data.dropna()\ntemp.shape #notice the reduction in rows\n\n(684, 11)\n\n\n\ntemp.isnull().sum() #no more missing values\n\n\n\n\n\n\n\n\n0\n\n\n\n\npclass\n0\n\n\nsurvived\n0\n\n\nname\n0\n\n\nsex\n0\n\n\nage\n0\n\n\nsibsp\n0\n\n\nparch\n0\n\n\nticket\n0\n\n\nfare\n0\n\n\nembarked\n0\n\n\nhome.dest\n0\n\n\n\n\ndtype: int64\n\n\n\nUsually, you don’t want to drop all the rows with any missing data as you are throwing out useful info.\nOne option is to impute the missing values… this can be dangerous but can be done with .fillna() method\n\n\nsub_titanic_data.fillna(value = 0) #note, for instance, some values of age are 0 now and the last row is all 0 values\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\nembarked\nhome.dest\n\n\n\n\n0\n1.0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nS\nSt Louis, MO\n\n\n1\n1.0\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\n0.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\n0.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\n0.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1305\n3.0\n0.0\nZabour, Miss. Thamine\nfemale\n0.0000\n1.0\n0.0\n2665\n14.4542\nC\n0\n\n\n1306\n3.0\n0.0\nZakarian, Mr. Mapriededer\nmale\n26.5000\n0.0\n0.0\n2656\n7.2250\nC\n0\n\n\n1307\n3.0\n0.0\nZakarian, Mr. Ortin\nmale\n27.0000\n0.0\n0.0\n2670\n7.2250\nC\n0\n\n\n1308\n3.0\n0.0\nZimmerman, Mr. Leo\nmale\n29.0000\n0.0\n0.0\n315082\n7.8750\nS\n0\n\n\n1309\n0.0\n0.0\n0\n0\n0.0000\n0.0\n0.0\n0\n0.0000\n0\n0\n\n\n\n\n1310 rows × 11 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nCan set the value you want to impute by passing a dictionary of key/value pairs\n\n\nsub_titanic_data.fillna(value = {\"home.dest\": \"Unknown\", \"age\": 200})\n\n\n  \n    \n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\nembarked\nhome.dest\n\n\n\n\n0\n1.0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nS\nSt Louis, MO\n\n\n1\n1.0\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\n0.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\n0.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\n0.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nS\nMontreal, PQ / Chesterville, ON\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1305\n3.0\n0.0\nZabour, Miss. Thamine\nfemale\n200.0000\n1.0\n0.0\n2665\n14.4542\nC\nUnknown\n\n\n1306\n3.0\n0.0\nZakarian, Mr. Mapriededer\nmale\n26.5000\n0.0\n0.0\n2656\n7.2250\nC\nUnknown\n\n\n1307\n3.0\n0.0\nZakarian, Mr. Ortin\nmale\n27.0000\n0.0\n0.0\n2670\n7.2250\nC\nUnknown\n\n\n1308\n3.0\n0.0\nZimmerman, Mr. Leo\nmale\n29.0000\n0.0\n0.0\n315082\n7.8750\nS\nUnknown\n\n\n1309\nNaN\nNaN\nNaN\nNaN\n200.0000\nNaN\nNaN\nNaN\nNaN\nNaN\nUnknown\n\n\n\n\n1310 rows × 11 columns",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Numerical Summaries"
    ]
  },
  {
    "objectID": "01_Programming_in_python/17-Numerical_Summaries.html#investigate-distributions",
    "href": "01_Programming_in_python/17-Numerical_Summaries.html#investigate-distributions",
    "title": "Numerical Summaries",
    "section": "Investigate distributions",
    "text": "Investigate distributions\n\nHow to summarize data depends on the type of data\n\nCategorical (Qualitative) variable - entries are a label or attribute\n\nNumeric (Quantitative) variable - entries are a numerical value where math can be performed\n\nNumerical summaries (across subgroups)\n\nContingency Tables (for categorical data)\nMean/Median\n\nStandard Deviation/Variance/IQR\nQuantiles/Percentiles\n\nGraphical summaries (across subgroups)\n\nBar plots (for categorical data)\nHistograms\n\nBox plots\n\nScatter plots\n\n\n\n\nCategorical Data\nGoal: Describe the distribution of the variable\n\nDistribution = pattern and frequency with which you observe a variable\n\nCategorical variable - entries are a label or attribute\n\nDescribe the relative frequency (or count) for each category\nUsing pandas .value_counts() method and crosstab() function\n\n\nVariables of interest for this section: + embarked (where journey started)\n\nsub_titanic_data.embarked[0:2]\n\n\n\n\n\n\n\n\nembarked\n\n\n\n\n0\nS\n\n\n1\nS\n\n\n\n\ndtype: object\n\n\n\ntype(sub_titanic_data.embarked[0])\n\nstr\n\n\nThe str type isn’t ideal for summarizaitons. A different data type is better!\n\nCategory Type Variables\nA category type variable is really useful for categorical variables.\n\nAkin to a factor variable in R (if you know those)\nCan have more descriptive labels, ordering of categories, etc.\n\nLet’s give the embarked variable more descriptive values and by converting it to a category type and manipulating it that way.\n\nsub_titanic_data[\"embarkedC\"] = sub_titanic_data.embarked.astype(\"category\")\nsub_titanic_data.embarkedC[0:2]\n\n\n\n\n\n\n\n\nembarkedC\n\n\n\n\n0\nS\n\n\n1\nS\n\n\n\n\ndtype: category\n\n\n\nNow we can use the .cat.rename_categories() method on this category variable\n\n\nsub_titanic_data.embarkedC = sub_titanic_data.embarkedC.cat.rename_categories([\"Cherbourg\", \"Queenstown\", \"Southampton\"])\nsub_titanic_data.embarkedC[0:2]\n\n\n\n\n\n\n\n\nembarkedC\n\n\n\n\n0\nSouthampton\n\n\n1\nSouthampton\n\n\n\n\ndtype: category\n\n\nWay better! Now let’s grab two more categorical variables and do similar things:\n\nsex (Male or Female)\n\nsurvived (survived or died)\n\n\n#convert sec variable\nsub_titanic_data[\"sexC\"] = sub_titanic_data.sex.astype(\"category\")\nsub_titanic_data.sexC = sub_titanic_data.sexC.cat.rename_categories([\"Female\", \"Male\"])\n#convert survived variable\nsub_titanic_data[\"survivedC\"] = sub_titanic_data.survived.astype(\"category\")\nsub_titanic_data.survivedC = sub_titanic_data.survivedC.cat.rename_categories([\"Died\", \"Survived\"])\n\n\n\n\nContingency tables\n\nTables of counts are the main numerical summary for categorical data\nCreate one-way contingency tables (.value_counts() method) (one-way because we are looking at one variable at a time)\n\n\nsub_titanic_data.embarkedC.value_counts(dropna = False)\n\n\n\n\n\n\n\n\ncount\n\n\nembarkedC\n\n\n\n\n\nSouthampton\n914\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\nNaN\n3\n\n\n\n\ndtype: int64\n\n\n\nsub_titanic_data.survivedC.value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nsurvivedC\n\n\n\n\n\nDied\n809\n\n\nSurvived\n500\n\n\n\n\ndtype: int64\n\n\n\nsub_titanic_data.sexC.value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nsexC\n\n\n\n\n\nMale\n843\n\n\nFemale\n466\n\n\n\n\ndtype: int64\n\n\n\nAlternatively, we can find a one-way table using the pd.cross_tab() function\n\nThis function is meant to take two columns (or more) and return tabulations between those two variables\nWe can define a dummy variable to cross with\nindex argument is the row variable and columns argument is the column variable\n\n\n\nsub_titanic_data[\"dummy\"] = 0\npd.crosstab(index = sub_titanic_data.embarkedC, columns = sub_titanic_data.dummy)\n\n\n  \n    \n\n\n\n\n\ndummy\n0\n\n\nembarkedC\n\n\n\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\nSouthampton\n914\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\npd.crosstab(index = sub_titanic_data.sexC, columns = sub_titanic_data.dummy)\n\n\n  \n    \n\n\n\n\n\ndummy\n0\n\n\nsexC\n\n\n\n\n\nFemale\n466\n\n\nMale\n843\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nTo summarize two categorical variables together, we use a two-way contingency table\nNow the cross_tab() function can be used more naturally\n\n\npd.crosstab(\n  sub_titanic_data.embarkedC, #index variable\n  sub_titanic_data.survivedC) #column variable\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\n\n\nembarkedC\n\n\n\n\n\n\nCherbourg\n120\n150\n\n\nQueenstown\n79\n44\n\n\nSouthampton\n610\n304\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\npd.crosstab(\n  sub_titanic_data.sexC,\n  sub_titanic_data.survivedC)\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\n\n\nsexC\n\n\n\n\n\n\nFemale\n127\n339\n\n\nMale\n682\n161\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAdd marginal totals with margins = True argument\n\n\npd.crosstab(\n  sub_titanic_data.embarkedC,\n  sub_titanic_data.survivedC,\n  margins = True)\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\nAll\n\n\nembarkedC\n\n\n\n\n\n\n\nCherbourg\n120\n150\n270\n\n\nQueenstown\n79\n44\n123\n\n\nSouthampton\n610\n304\n914\n\n\nAll\n809\n498\n1307\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAdd row and columns names for clarity\n\nUse rownames and colnames arguments\n\n\n\npd.crosstab(\n  sub_titanic_data.embarkedC,\n  sub_titanic_data.survivedC,\n  margins = True,\n  rownames = [\"Embarked Port\"],\n  colnames = [\"Survival Status\"]\n  )\n\n\n  \n    \n\n\n\n\n\nSurvival Status\nDied\nSurvived\nAll\n\n\nEmbarked Port\n\n\n\n\n\n\n\nCherbourg\n120\n150\n270\n\n\nQueenstown\n79\n44\n123\n\n\nSouthampton\n610\n304\n914\n\n\nAll\n809\n498\n1307\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThat looks great!\nFor more than two variables we can create tables but they get harder to read. For instance, we can look at a three-way contingency table:\n\npd.crosstab(\n  [sub_titanic_data.embarkedC, sub_titanic_data.survivedC], #pass a list of columns for the rows\n  sub_titanic_data.sexC,\n  margins = True)\n\n\n  \n    \n\n\n\n\n\n\nsexC\nFemale\nMale\nAll\n\n\nembarkedC\nsurvivedC\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nSurvived\n102\n48\n150\n\n\nQueenstown\nDied\n23\n56\n79\n\n\nSurvived\n37\n7\n44\n\n\nSouthampton\nDied\n93\n517\n610\n\n\nSurvived\n198\n106\n304\n\n\nAll\n\n464\n843\n1307\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nWe can add in names for more clarity\n\n\nmy_tab = pd.crosstab(\n  [sub_titanic_data.embarkedC, sub_titanic_data.survivedC],\n  sub_titanic_data.sexC,\n  margins = True,\n  rownames = ['Embarked Port', 'Survival Status'], #a list similar to how the rows were passed\n  colnames = ['Sex'])\nmy_tab\n\n\n  \n    \n\n\n\n\n\n\nSex\nFemale\nMale\nAll\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nSurvived\n102\n48\n150\n\n\nQueenstown\nDied\n23\n56\n79\n\n\nSurvived\n37\n7\n44\n\n\nSouthampton\nDied\n93\n517\n610\n\n\nSurvived\n198\n106\n304\n\n\nAll\n\n464\n843\n1307\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nWe might want to subset the returned table to get certain values…\n\nNote that the crosstab() function returns a data frame!\n\n\ntype(my_tab)\n\n\n    pandas.core.frame.DataFramedef __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/frame.pyTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index. This alignment also\n    occurs if data is a Series or a DataFrame itself. Alignment is done on\n    Series/DataFrame inputs.\n\n    If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n&gt;&gt;&gt; d = {'col1': [1, 2], 'col2': [3, 4]}\n&gt;&gt;&gt; df = pd.DataFrame(data=d)\n&gt;&gt;&gt; df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n&gt;&gt;&gt; df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n&gt;&gt;&gt; df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n&gt;&gt;&gt; d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n&gt;&gt;&gt; df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n&gt;&gt;&gt; df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n&gt;&gt;&gt; from dataclasses import make_dataclass\n&gt;&gt;&gt; Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3\n\nConstructing DataFrame from Series/DataFrame:\n\n&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df\n   0\na  1\nc  3\n\n&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[\"a\", \"b\", \"c\"], columns=[\"x\"])\n&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[\"a\", \"c\"])\n&gt;&gt;&gt; df2\n   x\na  1\nc  3\n      \n      \n\n\n\nmy_tab.columns # columns of the data frame\n\nIndex(['Female', 'Male', 'All'], dtype='object', name='Sex')\n\n\n\nmy_tab.index #rows of the data frame, these are tuples!\n\nMultiIndex([(  'Cherbourg',     'Died'),\n            (  'Cherbourg', 'Survived'),\n            ( 'Queenstown',     'Died'),\n            ( 'Queenstown', 'Survived'),\n            ('Southampton',     'Died'),\n            ('Southampton', 'Survived'),\n            (        'All',         '')],\n           names=['Embarked Port', 'Survival Status'])\n\n\n\nCan obtain conditional bivariate info via subsetting!\nThe MultiIndex can be tough but let’s look at some examples\nBelow returns the embarked vs survived table for females\n\n\nmy_tab[\"Female\"]\n\n\n\n\n\n\n\n\n\nFemale\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\nCherbourg\nDied\n11\n\n\nSurvived\n102\n\n\nQueenstown\nDied\n23\n\n\nSurvived\n37\n\n\nSouthampton\nDied\n93\n\n\nSurvived\n198\n\n\nAll\n\n464\n\n\n\n\ndtype: int64\n\n\n\nmy_tab.loc[:, \"Female\"] #.loc way of doing this, : gives all of that index\n\n\n\n\n\n\n\n\n\nFemale\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\nCherbourg\nDied\n11\n\n\nSurvived\n102\n\n\nQueenstown\nDied\n23\n\n\nSurvived\n37\n\n\nSouthampton\nDied\n93\n\n\nSurvived\n198\n\n\nAll\n\n464\n\n\n\n\ndtype: int64\n\n\n\nBelow returns the sex vs embarked table for those that died\n\n\nmy_tab.iloc[0:5:2, :] #0:5:2 gives a shorthand for a sequence with steps of 2s\n\n\n  \n    \n\n\n\n\n\n\nSex\nFemale\nMale\nAll\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nQueenstown\nDied\n23\n56\n79\n\n\nSouthampton\nDied\n93\n517\n610\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nUsing .loc[] is better\nMust understand our MultiIndex\n\n\nmy_tab.index\n\nMultiIndex([(  'Cherbourg',     'Died'),\n            (  'Cherbourg', 'Survived'),\n            ( 'Queenstown',     'Died'),\n            ( 'Queenstown', 'Survived'),\n            ('Southampton',     'Died'),\n            ('Southampton', 'Survived'),\n            (        'All',         '')],\n           names=['Embarked Port', 'Survival Status'])\n\n\n\nBelow uses this index to return the sex vs embarked table for those that died\n\n\nmy_tab.loc[((\"Cherbourg\", \"Queenstown\", \"Southampton\"), \"Died\"), :]\n\n\n  \n    \n\n\n\n\n\n\nSex\nFemale\nMale\nAll\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nQueenstown\nDied\n23\n56\n79\n\n\nSouthampton\nDied\n93\n517\n610\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nBelow returns the sex vs survived table for embarked of Cherbourg\n\n\nmy_tab.loc[('Cherbourg', (\"Died\", \"Survived\")), :]\n\n\n  \n    \n\n\n\n\n\n\nSex\nFemale\nMale\nAll\n\n\nEmbarked Port\nSurvival Status\n\n\n\n\n\n\n\nCherbourg\nDied\n11\n109\n120\n\n\nSurvived\n102\n48\n150\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nReturn the sex table for those that died and embarked at Cherbourg\n\nFirst with .iloc[] then with .loc[]\n\n\n\nmy_tab.iloc[0, :]\n\n\n\n\n\n\n\n\nCherbourg\n\n\n\nDied\n\n\nSex\n\n\n\n\n\nFemale\n11\n\n\nMale\n109\n\n\nAll\n120\n\n\n\n\ndtype: int64\n\n\n\nmy_tab.loc[('Cherbourg', 'Died')]\n\n\n\n\n\n\n\n\nCherbourg\n\n\n\nDied\n\n\nSex\n\n\n\n\n\nFemale\n11\n\n\nMale\n109\n\n\nAll\n120\n\n\n\n\ndtype: int64\n\n\n\n\n\n\nNumeric Data\nGoal: Describe the distribution of the variable\n\nDistribution = pattern and frequency with which you observe a variable\n\nNumeric variable - entries are a numerical value where math can be performed\n\nFor a single numeric variable, describe the distribution via\n\nShape: Histogram, Density plot, … (covered later)\nMeasures of center: Mean, Median, …\nMeasures of spread: Variance, Standard Deviation, Quartiles, IQR, …\n\nFor two numeric variables, describe the distribution via\n\nShape: Scatter plot, …\nMeasures of linear relationship: Covariance, Correlation, …\n\n\n\nMeasures of Center\n\nFind mean and median with methods on a Series\n\n\ntype(sub_titanic_data['fare'])\n\n\n    pandas.core.series.Seriesdef __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/series.pyOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n&gt;&gt;&gt; ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n&gt;&gt;&gt; ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n&gt;&gt;&gt; r = [1, 2]\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\n[1, 2]\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n&gt;&gt;&gt; r = np.array([1, 2])\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\narray([999,   2])\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.\n      \n      \n\n\n\nCorresponding methods exist for the common numerical summaries\n\n\nsub_titanic_data['fare'].mean()\n\n33.29547928134557\n\n\n\nsub_titanic_data['fare'].median()\n\n14.4542\n\n\n\nsub_titanic_data.age.mean() #same thing with a different way to get a column\n\n29.8811345124283\n\n\n\nsub_titanic_data.age.median()\n\n28.0\n\n\n\n\n\nMeasures of Spread\n\nStandard Deviation, Quartiles, & IQR found with Series methods as well\n\n\nsub_titanic_data.age.std()\n\n14.413499699923594\n\n\n\nsub_titanic_data.age.quantile(q = [0.2, 0.25, 0.5, 0.95])\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0.20\n19.0\n\n\n0.25\n21.0\n\n\n0.50\n28.0\n\n\n0.95\n57.0\n\n\n\n\ndtype: float64\n\n\n\nq1 = sub_titanic_data.age.quantile(q = [0.25])\nq1\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0.25\n21.0\n\n\n\n\ndtype: float64\n\n\n\nq3 = sub_titanic_data.age.quantile(q = [0.75])\nq3\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0.75\n39.0\n\n\n\n\ndtype: float64\n\n\n\ntype(q1)\n\n\n    pandas.core.series.Seriesdef __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/series.pyOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n&gt;&gt;&gt; ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n&gt;&gt;&gt; ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n&gt;&gt;&gt; r = [1, 2]\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\n[1, 2]\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n&gt;&gt;&gt; r = np.array([1, 2])\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\narray([999,   2])\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.\n      \n      \n\n\n\nAs both q1 and q3 are Series, they have indices\nThis makes them a little more difficult than you might like to subtract (to find the IRQ)\n\n\nq3-q1 #doesn't work due to the differing index names\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0.25\nNaN\n\n\n0.75\nNaN\n\n\n\n\ndtype: float64\n\n\n\nq3[0.75] - q1[0.25] #grab the values by index names and subtract those\n\n18.0\n\n\n\nAlternatively, remember that returning the .values attribute returns a numpy array. We can subtract these.\n\n\nq3.values - q1.values\n\narray([18.])\n\n\n\n\n\nMeasures of Linear Relationship\n\nCorrelation via the .corr() method on a data frame\nThis gives the correlation with any numerically (stored) variables that are passed\n\nJust because it is stored numerically doesn’t mean we should treat it numerically!\n\n\n\nsub_titanic_data[[\"age\", \"fare\", \"sibsp\", \"parch\"]].corr()\n\n\n  \n    \n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\n\n\nage\n1.000000\n0.178739\n-0.243699\n-0.150917\n\n\nfare\n0.178739\n1.000000\n0.160238\n0.221539\n\n\nsibsp\n-0.243699\n0.160238\n1.000000\n0.373587\n\n\nparch\n-0.150917\n0.221539\n0.373587\n1.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n\n\nSummaries Across Groups\nUsually want summaries for different subgroups of data\nTwo approaches we’ll cover: - Use .groupby() method and then use a summarization method - Use pd.crosstab() function with aggfunc argument\n\n.groupby() Examples\nExample: Get similar fare summaries for each survival status\n\nsub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean()\n\n\n\n  \n    \n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\n\n\n\n\n\n\n\n\nDied\n30.545369\n23.353831\n0.521632\n0.328801\n\n\nSurvived\n28.918228\n49.361184\n0.462000\n0.476000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nsub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].std()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].std()\n\n\n\n  \n    \n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\n\n\n\n\n\n\n\n\nDied\n13.922539\n34.145096\n1.210449\n0.912332\n\n\nSurvived\n15.061481\n68.648795\n0.685197\n0.776292\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n.unstack() method on the result can sometimes make the output clearer\n\n\nsub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean().unstack()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby(\"survivedC\")[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean().unstack()\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nsurvivedC\n\n\n\n\n\nage\nDied\n30.545369\n\n\nSurvived\n28.918228\n\n\nfare\nDied\n23.353831\n\n\nSurvived\n49.361184\n\n\nsibsp\nDied\n0.521632\n\n\nSurvived\n0.462000\n\n\nparch\nDied\n0.328801\n\n\nSurvived\n0.476000\n\n\n\n\ndtype: float64\n\n\n\nMultiple grouping variables can be given as a list\n\nExample: Get summary for numeric type variables for each survival status and embarked port\n\n\n\nsub_titanic_data.groupby([\"survivedC\", \"embarkedC\"])[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby([\"survivedC\", \"embarkedC\"])[[\"age\", \"fare\", \"sibsp\", \"parch\"]].mean()\n\n\n\n  \n    \n\n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\nembarkedC\n\n\n\n\n\n\n\n\nDied\nCherbourg\n34.468750\n40.255592\n0.316667\n0.225000\n\n\nQueenstown\n30.202703\n11.615349\n0.379747\n0.177215\n\n\nSouthampton\n29.945385\n21.546160\n0.580328\n0.368852\n\n\nSurvived\nCherbourg\n31.037248\n80.000807\n0.466667\n0.486667\n\n\nQueenstown\n24.153846\n13.833998\n0.272727\n0.000000\n\n\nSouthampton\n27.989881\n39.183470\n0.490132\n0.542763\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nsub_titanic_data.groupby([\"survivedC\", \"embarkedC\"])[[\"age\", \"fare\", \"sibsp\", \"parch\"]].std()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.groupby([\"survivedC\", \"embarkedC\"])[[\"age\", \"fare\", \"sibsp\", \"parch\"]].std()\n\n\n\n  \n    \n\n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\nembarkedC\n\n\n\n\n\n\n\n\nDied\nCherbourg\n14.655181\n56.553704\n0.518293\n0.557040\n\n\nQueenstown\n16.785187\n10.922240\n1.016578\n0.655538\n\n\nSouthampton\n13.496871\n28.786020\n1.320897\n0.990934\n\n\nSurvived\nCherbourg\n15.523752\n97.642219\n0.575410\n0.730327\n\n\nQueenstown\n7.057457\n17.503850\n0.585230\n0.000000\n\n\nSouthampton\n14.926867\n47.656409\n0.744552\n0.831405\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAs our code gets longer, this is a good place to use \\ to extend our code down a line\n\n\nsub_titanic_data \\\n  .groupby([\"survivedC\", \"embarkedC\"]) \\\n   [[\"age\", \"fare\", \"sibsp\", \"parch\"]] \\\n   .mean()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  .groupby([\"survivedC\", \"embarkedC\"]) \\\n\n\n\n  \n    \n\n\n\n\n\n\n\nage\nfare\nsibsp\nparch\n\n\nsurvivedC\nembarkedC\n\n\n\n\n\n\n\n\nDied\nCherbourg\n34.468750\n40.255592\n0.316667\n0.225000\n\n\nQueenstown\n30.202703\n11.615349\n0.379747\n0.177215\n\n\nSouthampton\n29.945385\n21.546160\n0.580328\n0.368852\n\n\nSurvived\nCherbourg\n31.037248\n80.000807\n0.466667\n0.486667\n\n\nQueenstown\n24.153846\n13.833998\n0.272727\n0.000000\n\n\nSouthampton\n27.989881\n39.183470\n0.490132\n0.542763\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\npd.crosstab() Examples\n\nAlternatively we can use the pd.crosstab() function with an aggfunc to define our summarization to produce\n\nExample: Get summary for numeric type variables for each survival status\n\nA bit awkward in this case as we don’t really have a ‘column’ variable\nMake a dummy variable for that\n\n\npd.crosstab(\n  sub_titanic_data.survivedC,\n  columns = [\"mean\" for _ in range(sub_titanic_data.shape[0])], #create variable with only the value 'mean'\n  values = sub_titanic_data.fare,\n  aggfunc = 'mean')\n\n\n  \n    \n\n\n\n\n\ncol_0\nmean\n\n\nsurvivedC\n\n\n\n\n\nDied\n23.353831\n\n\nSurvived\n49.361184\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nCan return multiple summaries at once by passing them as a list\n\n\npd.crosstab(\n  sub_titanic_data.survivedC,\n  columns = [\"stat\" for _ in range(sub_titanic_data.shape[0])],\n  values = sub_titanic_data.fare,\n  aggfunc = ['mean', 'median', 'std', 'count'])\n\n\n  \n    \n\n\n\n\n\n\nmean\nmedian\nstd\ncount\n\n\ncol_0\nstat\nstat\nstat\nstat\n\n\nsurvivedC\n\n\n\n\n\n\n\n\nDied\n23.353831\n10.5\n34.145096\n808\n\n\nSurvived\n49.361184\n26.0\n68.648795\n500\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nMore natural with two grouping variables\n\nExample: Get summary for numeric type variables for each survival status and embarked port\n\n\n\npd.crosstab(\n  sub_titanic_data.embarkedC,\n  sub_titanic_data.survivedC,\n  values = sub_titanic_data.fare,\n  aggfunc = ['mean', 'count'])\n\n\n  \n    \n\n\n\n\n\n\nmean\ncount\n\n\nsurvivedC\nDied\nSurvived\nDied\nSurvived\n\n\nembarkedC\n\n\n\n\n\n\n\n\nCherbourg\n40.255592\n80.000807\n120\n150\n\n\nQueenstown\n11.615349\n13.833998\n79\n44\n\n\nSouthampton\n21.546160\n39.183470\n609\n304",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Numerical Summaries"
    ]
  },
  {
    "objectID": "01_Programming_in_python/15-Pandas_Data_Frames.html#creating-a-dataframe",
    "href": "01_Programming_in_python/15-Pandas_Data_Frames.html#creating-a-dataframe",
    "title": "Pandas Data Frames",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\nMost of the time we’ll read data from a raw file directly into a DataFrame\nHowever, you can create one with the pd.DataFrame() function\n\n\nimport pandas as pd\nimport numpy as np\n\n\nCreating a Data Frame from Lists\n\nzip() lists of the same length together\nspecify columns via columns = list of appropriate length\nspecify row names via index = list of appropriate length (if you want!)\n\n\n#populate some lists, each of equal length\nname = ['Alice', 'Bob','Charlie','Dave','Eve','Francesca','Greg']\nage = [20, 21, 22, 23, 22, 21, 22]\nmajor = ['Statistics', 'History', 'Chemistry', 'English', 'Math', 'Civil Engineering','Statistics']\n\n#create the data frame using zip()\nmy_df = pd.DataFrame(zip(name, age, major), columns = [\"name\", \"age\", \"major\"])\nmy_df\n\n\n  \n    \n\n\n\n\n\n\nname\nage\nmajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n1\nBob\n21\nHistory\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n5\nFrancesca\n21\nCivil Engineering\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\nCreating a Data Frame from a Dictionary\n\nThe pd.DataFrame() function can create DataFrames from many objects\nFor a dictionary (dict object), the keys become the column names (values must be of the same length)\n\n\npeople = {'Name': ['Alice', 'Bob','Charlie','Dave','Eve','Francesca','Greg'],\n          'Age': [20, 21, 22, 23, 22, 21, 22],\n          'Major': ['Statistics', 'History', 'Chemistry', 'English', 'Math', 'Civil Engineering','Statistics'],\n         }\npeople\n\n{'Name': ['Alice', 'Bob', 'Charlie', 'Dave', 'Eve', 'Francesca', 'Greg'],\n 'Age': [20, 21, 22, 23, 22, 21, 22],\n 'Major': ['Statistics',\n  'History',\n  'Chemistry',\n  'English',\n  'Math',\n  'Civil Engineering',\n  'Statistics']}\n\n\n\nmy_df = pd.DataFrame(people)\nmy_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n1\nBob\n21\nHistory\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n5\nFrancesca\n21\nCivil Engineering\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\nCreating a Data Frame from a NumPy Array\n\nIf you have a 2D numpy array, the conversion to a DataFrame object is natural\nYou can specify the column names with columns = and the indices with index =\n\n\nmy_array = np.random.random((5,3))\nprint(my_array.shape)\nmy_array\n\n(5, 3)\n\n\narray([[0.44601505, 0.15726038, 0.63256689],\n       [0.74871631, 0.35006141, 0.58570382],\n       [0.14346733, 0.22706604, 0.02253265],\n       [0.57983249, 0.60743321, 0.88242121],\n       [0.20877059, 0.75132726, 0.04447515]])\n\n\n\nmy_df2 = pd.DataFrame(my_array, columns=[\"1st\", \"2nd\", \"3rd\"], index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nmy_df2\n\n\n  \n    \n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n\n\na\n0.446015\n0.157260\n0.632567\n\n\nb\n0.748716\n0.350061\n0.585704\n\n\nc\n0.143467\n0.227066\n0.022533\n\n\nd\n0.579832\n0.607433\n0.882421\n\n\ne\n0.208771\n0.751327\n0.044475",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas Data Frames"
    ]
  },
  {
    "objectID": "01_Programming_in_python/15-Pandas_Data_Frames.html#indexing-a-data-frame",
    "href": "01_Programming_in_python/15-Pandas_Data_Frames.html#indexing-a-data-frame",
    "title": "Pandas Data Frames",
    "section": "Indexing a Data Frame",
    "text": "Indexing a Data Frame\n\nIndexing Columns with []\n\nDataFrames have a .columns attribute\n\n\nmy_df2.columns\n\nIndex(['1st', '2nd', '3rd'], dtype='object')\n\n\n\nWe can access the columns using a string of the column names and ‘selection brackets’\n\n\nmy_df2[\"1st\"]\n\n\n\n\n\n\n\n\n1st\n\n\n\n\na\n0.446015\n\n\nb\n0.748716\n\n\nc\n0.143467\n\n\nd\n0.579832\n\n\ne\n0.208771\n\n\n\n\ndtype: float64\n\n\n\nNote that what gets returned is just a Series!\n\n\ntype(my_df2[\"1st\"])\n\n\n    pandas.core.series.Seriesdef __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/series.pyOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n&gt;&gt;&gt; ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n&gt;&gt;&gt; ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n&gt;&gt;&gt; r = [1, 2]\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\n[1, 2]\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n&gt;&gt;&gt; r = np.array([1, 2])\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\narray([999,   2])\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.\n      \n      \n\n\n\nWe can also return a column using the attribute syntax with the column name (a period at the end of the object followed by the column name)\n\n\nmy_df.Major\n\n\n\n\n\n\n\n\nMajor\n\n\n\n\n0\nStatistics\n\n\n1\nHistory\n\n\n2\nChemistry\n\n\n3\nEnglish\n\n\n4\nMath\n\n\n5\nCivil Engineering\n\n\n6\nStatistics\n\n\n\n\ndtype: object\n\n\n\ntype(my_df.Major) #again a Series!\n\n\n    pandas.core.series.Seriesdef __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool | lib.NoDefault=lib.no_default) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/core/series.pyOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\nname : Hashable, default None\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['a', 'b', 'c'])\n&gt;&gt;&gt; ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n&gt;&gt;&gt; d = {'a': 1, 'b': 2, 'c': 3}\n&gt;&gt;&gt; ser = pd.Series(data=d, index=['x', 'y', 'z'])\n&gt;&gt;&gt; ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n&gt;&gt;&gt; r = [1, 2]\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\n[1, 2]\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n&gt;&gt;&gt; r = np.array([1, 2])\n&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n&gt;&gt;&gt; ser.iloc[0] = 999\n&gt;&gt;&gt; r\narray([999,   2])\n&gt;&gt;&gt; ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.\n      \n      \n\n\n\nReturning more than one column is easy\nYou can give a list of the column names you want to the selection brackets\n\n\nmy_df[['Name', 'Age']]\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\n\n\n\n\n0\nAlice\n20\n\n\n1\nBob\n21\n\n\n2\nCharlie\n22\n\n\n3\nDave\n23\n\n\n4\nEve\n22\n\n\n5\nFrancesca\n21\n\n\n6\nGreg\n22\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nNote you can’t use slicing for columns using just [] (we’ll need to us .iloc[] or .loc[], which we cover in a moment)\nIf you try to index with slicing you get back appropriate rows (see below)\n\n\n\nIndexing Rows by Slicing with []\n\nSimilarly, you can index the rows using [] if you use a slice or a boolean array of appropriate length\n\n\nmy_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n1\nBob\n21\nHistory\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n5\nFrancesca\n21\nCivil Engineering\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmy_df[3:5] #get the 3rd and 4th rows\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nmy_df2\n\n\n  \n    \n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n\n\na\n0.446015\n0.157260\n0.632567\n\n\nb\n0.748716\n0.350061\n0.585704\n\n\nc\n0.143467\n0.227066\n0.022533\n\n\nd\n0.579832\n0.607433\n0.882421\n\n\ne\n0.208771\n0.751327\n0.044475\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmy_df2[1:5] #get the 2nd through 5th rows (counting starts at 0!)\n\n\n  \n    \n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n\n\nb\n0.748716\n0.350061\n0.585704\n\n\nc\n0.143467\n0.227066\n0.022533\n\n\nd\n0.579832\n0.607433\n0.882421\n\n\ne\n0.208771\n0.751327\n0.044475\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nOddly, you can’t return a single row with just a number\nYou can return it using slicing (recall : usually doesn’t return the last value)\n\n\nmy_df2[1] #throws an error\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key)\n   3804         try:\n-&gt; 3805             return self._engine.get_loc(casted_key)\n   3806         except KeyError as err:\n\nindex.pyx in pandas._libs.index.IndexEngine.get_loc()\n\nindex.pyx in pandas._libs.index.IndexEngine.get_loc()\n\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 1\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-23-90bcbc95bda4&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 my_df2[1] #throws an error\n\n/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py in __getitem__(self, key)\n   4100             if self.columns.nlevels &gt; 1:\n   4101                 return self._getitem_multilevel(key)\n-&gt; 4102             indexer = self.columns.get_loc(key)\n   4103             if is_integer(indexer):\n   4104                 indexer = [indexer]\n\n/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key)\n   3810             ):\n   3811                 raise InvalidIndexError(key)\n-&gt; 3812             raise KeyError(key) from err\n   3813         except TypeError:\n   3814             # If we have a listlike key, _check_indexing_error will raise\n\nKeyError: 1\n\n\n\n\nmy_df2[1:2] #return just one row\n\n\n  \n    \n\n\n\n\n\n\n1st\n2nd\n3rd\n\n\n\n\nb\n0.748716\n0.350061\n0.585704\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\nIndexing Rows Using a Boolean Array with []\n\nOften we use a Boolean object to subset the rows (rows with a True get returned, False do not)\nThis comes up when we use a condition found using a variable from our data frame to do the subsetting\n\n\nmy_df['Name'] == 'Alice' #create a boolean array\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nTrue\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n5\nFalse\n\n\n6\nFalse\n\n\n\n\ndtype: bool\n\n\n\nmy_df[my_df['Name'] == 'Alice'] #return just the True rows\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\nmy_df[my_df['Age'] &gt; 21] #return only rows that match\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\nCompound Logicals\n\nAll the standard compound logical operators exist\n& (and), | (or), ~ (not), ^ (xor - exclusive or)\n\nFirst, two boolean vectors:\n\n(my_df['Name'] == 'Alice')\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nTrue\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n5\nFalse\n\n\n6\nFalse\n\n\n\n\ndtype: bool\n\n\n\n(my_df['Name'] == 'Greg')\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nFalse\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n5\nFalse\n\n\n6\nTrue\n\n\n\n\ndtype: bool\n\n\n\nGet either/or for these two booleans\n\n\n(my_df['Name'] == 'Alice') | (my_df['Name'] == 'Greg')\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nTrue\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n5\nFalse\n\n\n6\nTrue\n\n\n\n\ndtype: bool\n\n\n\nNow we can subset the data based on this compound condition!\n\n\nmy_df[(my_df['Name'] == 'Alice') | (my_df['Name'] == 'Greg')]\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nWhen doing lots of logicals, you want to be careful and use () to keep things straight!\n\n\nmy_df[((my_df['Name'] == 'Alice') | (my_df['Name'] == 'Greg')) & (my_df['Age'] &gt; 21)]\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n6\nGreg\n22\nStatistics",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas Data Frames"
    ]
  },
  {
    "objectID": "01_Programming_in_python/15-Pandas_Data_Frames.html#operations-on-data-frames",
    "href": "01_Programming_in_python/15-Pandas_Data_Frames.html#operations-on-data-frames",
    "title": "Pandas Data Frames",
    "section": "Operations on Data Frames",
    "text": "Operations on Data Frames\n\n.head() and .tail() methods give the first few and last rows, respectively\n\n\nmy_df.head()\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n0\nAlice\n20\nStatistics\n\n\n1\nBob\n21\nHistory\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nmy_df.tail()\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nMajor\n\n\n\n\n2\nCharlie\n22\nChemistry\n\n\n3\nDave\n23\nEnglish\n\n\n4\nEve\n22\nMath\n\n\n5\nFrancesca\n21\nCivil Engineering\n\n\n6\nGreg\n22\nStatistics\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n.shape attribute contains the dimensions of the data frame\n\n\nmy_df.shape\n\n(7, 3)\n\n\n\n.info() method gives information about the data frame\n\n\nmy_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7 entries, 0 to 6\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    7 non-null      object\n 1   Age     7 non-null      int64 \n 2   Major   7 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 296.0+ bytes\n\n\n\nmy_df2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 5 entries, a to e\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   1st     5 non-null      float64\n 1   2nd     5 non-null      float64\n 2   3rd     5 non-null      float64\ndtypes: float64(3)\nmemory usage: 332.0+ bytes\n\n\n\nObtain a quick contingency table with the .value_counts() method on a column (so a .Series method really)\n\n\nmy_df[\"Major\"].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\nMajor\n\n\n\n\n\nStatistics\n2\n\n\nHistory\n1\n\n\nChemistry\n1\n\n\nEnglish\n1\n\n\nMath\n1\n\n\nCivil Engineering\n1\n\n\n\n\ndtype: int64",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas Data Frames"
    ]
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html#creating-an-array",
    "href": "01_Programming_in_python/12-Numpy.html#creating-an-array",
    "title": "NumPy",
    "section": "Creating an Array",
    "text": "Creating an Array\n\nArrays are like lists but process much faster\nThey also require that the data be of the same type\nThey can be multidimensional (like a matrix or even higher dimension\n\nThe picture below from https://predictivehacks.com/tips-about-numpy-arrays/ shows a 1D, 2D, and 3D array visually.\n\n\nTo create an ndarray object, pass a list, tuple, or any array-like object to np.array()\n\n\na = np.array(1)\na\n\narray(1)\n\n\n\ntype(a)\n\nnumpy.ndarray\n\n\n\nndarrays have a shape attribute\nAttributes can be accessed like methods except we don’t use () at the end\nWe did this with the .__doc__ attribute on functions\n\n\na.shape\n\n()\n\n\n\nb = np.array([1, 2, 3])\nprint(b)\nprint(type(b))\nprint(b.shape)\n\n[1 2 3]\n&lt;class 'numpy.ndarray'&gt;\n(3,)\n\n\n\n\nArray Dimension\n\n0D arrays are a scalar (sort of… see here for discussion)\n1D arrays are vectors\n2D arrays are matrices\n3D and up are just called arrays\n.shape attribute returns the dimensions of an array as a tuple\n\n\nc = np.array([1, \"a\", True])\nprint(c)\nc.shape\n\n['1' 'a' 'True']\n\n\n(3,)\n\n\n\nd = np.array([\n  [1, 2, 3],\n  [4, 5, 6]]\n  )\nprint(d)\nd.shape\n\n[[1 2 3]\n [4 5 6]]\n\n\n(2, 3)\n\n\n\n\nFunctions for Fillling/Creating Arrays\nCreating a vector or matrix of all zeros\n\nRow vector\n\n\nA0 = np.zeros(4) #row vector of length 4\nA0\n\narray([0., 0., 0., 0.])\n\n\n\nColumn vector\n\n\nA0 = np.zeros((4,1)) #column vector of length 4\nA0\n\narray([[0.],\n       [0.],\n       [0.],\n       [0.]])\n\n\n\nMatrix of zeros\n\n\nA = np.zeros((4,2)) #matrix with dimension 4, 2, given as a tuple\nA\n\narray([[0., 0.],\n       [0., 0.],\n       [0., 0.],\n       [0., 0.]])\n\n\n\nA.shape\n\n(4, 2)\n\n\n\nRow of all ones\n\n\nb = np.ones(4) #row vector\nb\n\narray([1., 1., 1., 1.])\n\n\n\nMatrix of all ones\n\n\nB = np.ones((2,3))\nB\n\narray([[1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\nMatrix of 10’s\n\n\nC = np.ones((2, 3)) * 10\nC\n\narray([[10., 10., 10.],\n       [10., 10., 10.]])\n\n\n\nnp.full() does this automatically\n\n\nC = np.full((2,3), 10) #specify the value to fill with after the tuple giving dimension\nC\n\narray([[10, 10, 10],\n       [10, 10, 10]])\n\n\n\nBe careful! C is an integer valued array\n\n\nC = np.full((2,3), 10)\nC[0,0] = 6.5                 #replace the top left element\nC\n\narray([[ 6, 10, 10],\n       [10, 10, 10]])\n\n\n\nAvoid by creating the matrix with a float instead\n\n\nC = np.full((2,3), 10.0)  #or C = np.ones((2, 3)) * 10.0\nC[0,0] = 6.5\nC\n\narray([[ 6.5, 10. , 10. ],\n       [10. , 10. , 10. ]])\n\n\n\nCreate an identity matrix with np.eye() (this has 1’s on the diagonal and 0’s elsewhere)\n\n\nD = np.eye(3)\nD\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nCreate a random matrix (values between 0 and 1) with np.random.random()\n\n\nE = np.random.random((3,5))\nE\n\narray([[0.82782158, 0.92669984, 0.28811706, 0.8048095 , 0.31863604],\n       [0.43125583, 0.95565594, 0.81946103, 0.96181153, 0.10190225],\n       [0.92238437, 0.66130983, 0.8828503 , 0.06677584, 0.78615673]])\n\n\n\nMany more ways to create!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "`NumPy`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html#reshaping-an-array",
    "href": "01_Programming_in_python/12-Numpy.html#reshaping-an-array",
    "title": "NumPy",
    "section": "Reshaping an Array",
    "text": "Reshaping an Array\n\nReshape an array with the .reshape() method\nChanges the dimension in some way\nWe’ll need to do this type of thing when fitting models!\n\n\nF = np.random.random((10,1))\nF\n\narray([[0.38620732],\n       [0.02246848],\n       [0.75057807],\n       [0.64596504],\n       [0.9782189 ],\n       [0.3074028 ],\n       [0.20987403],\n       [0.73177229],\n       [0.8167644 ],\n       [0.03675048]])\n\n\n\nF.shape\n\n(10, 1)\n\n\n\nG = F.reshape(1, -1) #-1 flattens to a 1D array\nG\n\narray([[0.38620732, 0.02246848, 0.75057807, 0.64596504, 0.9782189 ,\n        0.3074028 , 0.20987403, 0.73177229, 0.8167644 , 0.03675048]])\n\n\n\nG.shape\n\n(1, 10)\n\n\n\nG = F.reshape(2, 5)\nG\n\narray([[0.38620732, 0.02246848, 0.75057807, 0.64596504, 0.9782189 ],\n       [0.3074028 , 0.20987403, 0.73177229, 0.8167644 , 0.03675048]])\n\n\n\nCareful! G is actually a view of the original array\nView means that we haven’t created a new array, just a different way of viewing the values (essentially). The data is still stored in the same memory\n.base attribute will tell you whether you are referencing another array\n\n\nG.base\n\narray([[0.38620732],\n       [0.02246848],\n       [0.75057807],\n       [0.64596504],\n       [0.9782189 ],\n       [0.3074028 ],\n       [0.20987403],\n       [0.73177229],\n       [0.8167644 ],\n       [0.03675048]])\n\n\n\nG.base is None #a way to return a bool based on whether it is a view or not\n\nFalse\n\n\n\n\nCopying an Array\n\nTo avoid getting a view, copy the array with .copy() method\n\n\nH = F.reshape(2, 5).copy()\nH.base is None\n\nTrue\n\n\n\nH.base",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "`NumPy`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html#indexing-an-array",
    "href": "01_Programming_in_python/12-Numpy.html#indexing-an-array",
    "title": "NumPy",
    "section": "Indexing an Array",
    "text": "Indexing an Array\n\nAccess in the same was as lists []\nWith multiple dimensions, separate the indices you want with a ,\n\n\nb = np.array([1, 2, 3]) #row vector\nb\n\narray([1, 2, 3])\n\n\n\nprint(b[0], b[1], b[2])\n\n1 2 3\n\n\n\nb[0] = 5 #overwrite the 0 element\nb\n\narray([5, 2, 3])\n\n\n\nDepending on the dimensions, you add the required commas\nHere we have a 3D array so we have three slots\nNotation: array[1stD, 2ndD, 3rdD]\n\n\nE = np.random.random((3, 2, 2))\nE\n\narray([[[0.44271423, 0.36194369],\n        [0.67811074, 0.36893479]],\n\n       [[0.18957687, 0.89085357],\n        [0.40869827, 0.1685411 ]],\n\n       [[0.28849053, 0.65884175],\n        [0.71058619, 0.41460453]]])\n\n\n\nE[0, 0, 0]\n\n0.4427142296735752\n\n\n\nE[0, 1, 0]\n\n0.6781107434665593\n\n\n\nE[1, 0, 1]\n\n0.8908535743830176\n\n\n\n\nSlicing an Array\n\nRecall [start:end] for slicing sequence type objects. We can do that with arrays as well\n\nReturns everything from start up to and excluding end\nLeaving start blank implies a 0\nLeaving end blank returns everything from start through the end of the array\n\n\n\nA = np.array([\n  [1,2,3,4],\n  [5,6,7,8],\n  [9,10,11,12]])\nA\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\n\nB = A[:2, 1:3]\nB\n\narray([[2, 3],\n       [6, 7]])\n\n\n\nCareful with modifying! We have a view here so the values in both A and B are referencing the same computer memory\nChanging an element of B changes A!\n\n\nB[0, 0] = 919\nA\n\narray([[  1, 919,   3,   4],\n       [  5,   6,   7,   8],\n       [  9,  10,  11,  12]])\n\n\n\nReturning All of One Index\nUse a : with nothing else\n\n\nA = np.array([\n  [1,2,3,4],\n  [5,6,7,8],\n  [9,10,11,12]])\nA1 = A[1, :]\nA1\n\narray([5, 6, 7, 8])\n\n\n\nA1.shape\n\n(4,)\n\n\n\nA2 = A[1:3, :]\nA2\n\narray([[ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\n\nA2.shape\n\n(2, 4)",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "`NumPy`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/12-Numpy.html#operations-on-arrays",
    "href": "01_Programming_in_python/12-Numpy.html#operations-on-arrays",
    "title": "NumPy",
    "section": "Operations on Arrays",
    "text": "Operations on Arrays\n\nWe saw that multiplying by a constant was performed elementwise\n\nAll basic functions act elementwise\n\n\nx = np.array([\n  [1,2],\n  [3,4]])\ny = np.array([\n  [5,6],\n  [7,8]])\n\nx\n\narray([[1, 2],\n       [3, 4]])\n\n\n\ny\n\narray([[5, 6],\n       [7, 8]])\n\n\n\nx + 10\n\narray([[11, 12],\n       [13, 14]])\n\n\n\nLots of methods exist such as the .add() method for adding arrays elementwise\n\n\nnp.add(x, y)\n\narray([[ 6,  8],\n       [10, 12]])\n\n\n\nIf we just do something like x * y we get elementwise multiplication\n\n\nx * y\n\narray([[ 5, 12],\n       [21, 32]])\n\n\n\nThe .multiply() method does elementwise multiplication too\nCan also add in conditions on when to multiply though!\n\nwhere = argument gives the condition on when to do the multiplication\nout = tells it which values to use if you don’t do the multiplication\n\n\n\nnp.multiply(x, y, where = (x &gt;= 3), out = x)\n\narray([[ 1,  2],\n       [21, 32]])\n\n\n\nElementwise division\n\n\nx / y\n\narray([[0.2       , 0.33333333],\n       [3.        , 4.        ]])\n\n\n\nWe can do matrix multiplication (if you are familiar with that) using the .matmul() method\n\n\nnp.matmul(x, y)\n\narray([[ 19,  22],\n       [329, 382]])\n\n\n\nsqrt() function can be used to find the square roots of the elements of a matrix\n\n\nnp.sqrt(x)\n\narray([[1.        , 1.41421356],\n       [4.58257569, 5.65685425]])\n\n\n\nnp.linalg.inv() will provide the inverse of a square matrix (if you’re familiar with that type of thing!)\n\n\nnp.linalg.inv(x)\n\narray([[-3.2,  0.2],\n       [ 2.1, -0.1]])\n\n\n\nComputations on Arrays\n\nNumPy has some useful functions for performing basic computations on arrays\n\n\nx = np.array([\n  [1,2,10],\n  [3,4,11]])\nnp.sum(x)\n\n31\n\n\n\nColumn-wise and row-wise sums\n\n\nx.shape\n\n(2, 3)\n\n\n\nnp.sum(x, axis=0)\n\narray([ 4,  6, 21])\n\n\n\nnp.sum(x, axis=1)\n\narray([13, 18])\n\n\n\nCombine arrays (appropriately sized)\n\n\nx = np.array([\n  [1,2],\n  [3,4]])\ny = np.array([\n  [5,6],\n  [7,8]])\n\nnp.hstack((x, y))\n\narray([[1, 2, 5, 6],\n       [3, 4, 7, 8]])\n\n\n\nnp.vstack((x, y))\n\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]])\n\n\n\nLots of other operations!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "`NumPy`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#constructing-a-list",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#constructing-a-list",
    "title": "Lists and Tuples",
    "section": "Constructing a List",
    "text": "Constructing a List\nEarlier we saw there were four ways to construct lists (although we didn’t go through them all) - [element1, element2] - list((element1, element2, ...)) - an empty list and use the .append() method to add elements - list comprehensions\n\n\nConstructing a list from and empty list\n\nCreate an empty list and use the append method to add elements\n\n\nmylist = []\n# or\nmylist = list()\n\n\nAdd elements with .append()\n\n\nmylist.append(\"Dog\")\nmylist.append(\"Cat\")\nmylist\n\n['Dog', 'Cat']\n\n\n\n\n\nConstructing a List using []\n\nCreate an empty list and use the append method to add elements\nOften used with a for loop\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nmylist = []\n\nfor x in animals:\n    if \"o\" in x:\n        mylist.append(x)\n\nmylist\n\n['Dog', 'Horse', 'Frog', 'Cow', 'Buffalo', 'Fox', 'Racoon']\n\n\n\n\n\nConstructing a list using list comprehensions\n\nRather than write the loop out, you can use list comprehensions (shorthand!)\nThe general syntax for list comprehensions is:\n[expression for member in iterable]\nLet’s do a quick example. First, the for loop way:\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nmylist = []\nfor x in animals:\n    mylist.append(x)\n\n\nNow, we can do the same thing with shorthand!\n\n\nmylist = [x for x in animals] #for x in animals, return x (essentially)\nmylist\n\n['Dog',\n 'Cat',\n 'Horse',\n 'Frog',\n 'Cow',\n 'Buffalo',\n 'Deer',\n 'Fish',\n 'Bird',\n 'Fox',\n 'Racoon']\n\n\n\nYou can do more complicated things with list comprehensions as well. For instance, we can include condition logic.\n[expression for member in iterable (if conditional)]\nFirst the for loop way:\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nfor x in animals:\n    if \"o\" in x:\n        mylist.append(x)\n\n\nNow using a list comprehension:\n\n\nmylist = [x for x in animals if \"o\" in x]\nmylist\n\n['Dog', 'Horse', 'Frog', 'Cow', 'Buffalo', 'Fox', 'Racoon']\n\n\n\nWe can also modify the thing that gets put into the loop. Check out this example where we upper case the string.\nFirst the for loop way:\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nmylist = []\n\nfor x in animals:\n    if \"o\" in x:\n        mylist.append(x.upper()) #upper case prior to appending\n\nmylist\n\n['DOG', 'HORSE', 'FROG', 'COW', 'BUFFALO', 'FOX', 'RACOON']\n\n\n\nNow using a list comprehension:\n\n\nmylist = [x.upper() for x in animals if \"o\" in x]\nmylist\n\n['DOG', 'HORSE', 'FROG', 'COW', 'BUFFALO', 'FOX', 'RACOON']",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Lists and Tuples"
    ]
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#reminder-about-strings",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#reminder-about-strings",
    "title": "Lists and Tuples",
    "section": "Reminder About Strings",
    "text": "Reminder About Strings\n\nStrings are a sequence type object (so you can iterate over them naturally)\n\n\nmylist = []\nfor x in \"Man do I love learning all this python!\":\n    if x in \"aeiou\":\n        mylist.append(x)\nmylist\n\n['a', 'o', 'o', 'e', 'e', 'a', 'i', 'a', 'i', 'o']\n\n\n\nThat means we can do something like the for loop above using list comprehensions!\n\n\nmylist = [x for x in \"Man do I love learning all this python!\" if x in \"aeiou\"]\nmylist\n\n['a', 'o', 'o', 'e', 'e', 'a', 'i', 'a', 'i', 'o']",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Lists and Tuples"
    ]
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#list-operations-indexing-slicing",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#list-operations-indexing-slicing",
    "title": "Lists and Tuples",
    "section": "List Operations (Indexing & Slicing)",
    "text": "List Operations (Indexing & Slicing)\nRecall:\n\nIndex with a [] (just like strings)\nCounting starts at 0\n\n\nx = [10, 15, 10, 100, \"Help!\"]\nx[0]\n\n10\n\n\n\nx[1]\n\n15\n\n\n\nx[-1]\n\n'Help!'\n\n\n\nMultiple elements at once with :\nRemember the last number isn’t included and the counting starts at 0\n\n:2 is really giving you 0 and 1\n\n\n\nx[:2]\n\n[10, 15]\n\n\n\nx[1:]\n\n[15, 10, 100, 'Help!']\n\n\n\nx[1:3]\n\n[15, 10]\n\n\n\nx[1:4:2]\n\n[15, 100]",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Lists and Tuples"
    ]
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#lists-are-mutable",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#lists-are-mutable",
    "title": "Lists and Tuples",
    "section": "Lists are Mutable",
    "text": "Lists are Mutable\n\nThat is, we can replace or change elements of a list\n\n\nx = [10, 15, 10, 100, \"Help!\"]\nx[0] = 11\nx\n\n[11, 15, 10, 100, 'Help!']\n\n\n\nx[1] = [\"hi\", \"ho\"]\nx\n\n[11, ['hi', 'ho'], 10, 100, 'Help!']\n\n\n\nx[1:3] = [1, 2]\nx\n\n[11, 1, 2, 100, 'Help!']",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Lists and Tuples"
    ]
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#list-methods",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#list-methods",
    "title": "Lists and Tuples",
    "section": "List Methods",
    "text": "List Methods\nMany useful methods to modify lists:\n\nmylist.append(object_to_add)\n\n\nx = [x for x in range(1,10)]\ny = [y for y in \"abcde\"]\nx.append(y) #modifies x\nx\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, ['a', 'b', 'c', 'd', 'e']]\n\n\n\nmylist.extend(object_to_add)\n\n\nx = [x for x in range(1,10)]\ny = [y for y in \"abcde\"]\nx.extend(y) #modifies x and iterates over list elements rather than appending a list into x\nx\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 'a', 'b', 'c', 'd', 'e']\n\n\n\nUsing + is similar but doesn’t overwrite x\n\n\nx + y\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 'a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e']\n\n\n\nmylist.insert(index, object_to_add)\n\n\ny = [y for y in \"abcde\"]\ny.insert(2, 30) #modifies y\ny\n\n['a', 'b', 30, 'c', 'd', 'e']\n\n\n\nmylist.remove(element_to_remove)\n\n\ny.remove(\"d\") #modifies y\ny\n\n['a', 'b', 30, 'c', 'e']\n\n\n\nmylist.count(value)\n\n\nw = [x for x in range(0, 4)] * 4\nprint(w)\nw.count(1) #count the number of times 1 occurs\n\n[0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n\n\n4\n\n\n\nmylist.index(value)\n\n\nv = [y for y in \"abcde\"]\nv.extend([\"z\", \"y\"] * 3)\nprint(v)\nprint(v.index(\"y\")) #get the index corresponding to the first 'y'\nprint(v.index(\"y\", v.index(\"y\") + 1)) #index corresponding to second 'y'\n\n['a', 'b', 'c', 'd', 'e', 'z', 'y', 'z', 'y', 'z', 'y']\n6\n8\n\n\n\n\nList Packing & Unpacking\n\nWe can pack a list. That is, put a bunch of values in a list in one line of code.\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nshort_animals = animals[:3]\n\n#pack the values first, second, and third\nfirst, second, third = short_animals\nprint(first + \" \" + second + \" \" + third)\n\nDog Cat Horse\n\n\n\nWe can also pack leftover elements into a list using *name\n\n\nfirst, second, third, *other = animals\nprint(first + \" \" + second + \" \" + third)\nprint(other)\n\nDog Cat Horse\n['Frog', 'Cow', 'Buffalo', 'Deer', 'Fish', 'Bird', 'Fox', 'Racoon']\n\n\n\nYou can pack in different orders as well!\n\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\n*other, second_last, last = animals\nprint(other)\nprint(second_last + \" \" + last)\n\n['Dog', 'Cat', 'Horse', 'Frog', 'Cow', 'Buffalo', 'Deer', 'Fish', 'Bird']\nFox Racoon\n\n\n\nIf we want to ignore some of the values we can use our _ temporary variable with packing and *:\n\n\nfirst, *_, last = animals\nprint(first + \" \" + last)\n\nDog Racoon\n\n\n\nLater we’ll look at unpacking a list when calling functions\n\n\ndef my_fun(a, b, c):\n    print(a, b, c)\n\nfav_animals = [\"cat\", \"dog\", \"cow\"]\nmy_fun(*fav_animals)\n\ncat dog cow",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Lists and Tuples"
    ]
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#constructing-tuples",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#constructing-tuples",
    "title": "Lists and Tuples",
    "section": "Constructing Tuples",
    "text": "Constructing Tuples\n\nWe create by separating elements with a ,, ( ), or tuple(())\n\n\ntup1 = 3, 10, \"word\", True\ntup1\n\n(3, 10, 'word', True)\n\n\n\ntup2 = (1, 2, \"word\", False)\ntup2\n\n(1, 2, 'word', False)\n\n\n\ntup3 = tuple((tup1, tup2))\ntup3\n\n((3, 10, 'word', True), (1, 2, 'word', False))\n\n\n\ntup4 = tup1 + tup2 #like other sequence type objects we can concatenate them with +\ntup4\n\n(3, 10, 'word', True, 1, 2, 'word', False)\n\n\n\ntup5 = (1, [1, 3])\ntup5\n\n(1, [1, 3])\n\n\n\ntup5 * 3\n\n(1, [1, 3], 1, [1, 3], 1, [1, 3])\n\n\nOne interesting thing is that although we can’t modify the tuple, we can modify mutable elements within the tuple!\n\ntup5[1][1] = 5\n\n\ntup5\n\n(1, [1, 5])",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Lists and Tuples"
    ]
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#constructing-tuples-from-list-comprehensions",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#constructing-tuples-from-list-comprehensions",
    "title": "Lists and Tuples",
    "section": "Constructing Tuples from list comprehensions",
    "text": "Constructing Tuples from list comprehensions\n\nTo populate a tuple we can wrap a list comprehension with tuple()\n\n\ny = [x for x in range(1, 10)]\ny = tuple(y)\ny\n\n(1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n\n\nCan sort of edit a tuple by switching it between a list and a tuple… but this isn’t really editing it!\n\n\ny = list(y)\ny.append(\"new element\")\ny = tuple(y)\ny\n\n(1, 2, 3, 4, 5, 6, 7, 8, 9, 'new element')",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Lists and Tuples"
    ]
  },
  {
    "objectID": "01_Programming_in_python/10-Lists_Tuples.html#tuple-operations-methods",
    "href": "01_Programming_in_python/10-Lists_Tuples.html#tuple-operations-methods",
    "title": "Lists and Tuples",
    "section": "Tuple Operations & Methods",
    "text": "Tuple Operations & Methods\nAs with strings and lists:\n\nWe can index and slice using [:]\nConcatenate with + and *\nSome similar functions like len() and count()\nSome similar methods like .index() and .count()\nCan do packing and unpacking",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Lists and Tuples"
    ]
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#function-creation-syntax",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#function-creation-syntax",
    "title": "User Defined Functions",
    "section": "Function Creation Syntax",
    "text": "Function Creation Syntax\nTo create our own functions, we just need to\n\nuse the keyword def and give the function name with arguments\ntab in (four spaces) our function body (code that the function runs).\nat the top of the function body we usually add a multi-line string (via triple quotes) explaining the function purpose and arguments (called a doc string)\nwe use return to return an object\n\ndef function_name(arg1, arg2, arg3 = default_arg3):\n    \"\"\"\n    Documentation string\n    \"\"\"\n    Function body\n    return object",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "User Defined Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#write-our-own-mean-function",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#write-our-own-mean-function",
    "title": "User Defined Functions",
    "section": "Write Our Own Mean Function",
    "text": "Write Our Own Mean Function\nWe discussed common tasks for data. Of course one was simply describing a data set that we have. One way to describe the center of a numeric variable’s distribution is through the sample mean.\n\nGiven data points labeled as \\(y_1, y_2, ..., y_n\\) (\\(n\\) is the number of observations), the sample mean is\n\n\\[\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i\\]\nLet’s write a function to calculate the mean of a list of numbers using the sum() and len() functions.\n\ndef find_mean(y):\n    \"\"\"\n    Quick function to find the mean of a list\n    Assumes we have a list with individual numeric type data elements\n    \"\"\"\n    return sum(y)/len(y)\n\nNow let’s apply our function to a list of numeric values. We can create a sequence of values using the range() function. This function takes two arguments, the starting point and the ending point (which isn’t included).\nrange() itself is an immutable iterable type object. It isn’t the values themselves but an object that can be used to create the values. In the case of range() it can be described as a lazy list. We’ll discuss iterators more shortly.\nOne way to get the range() object to create its values is by running list() on it. This tells python to iterate over the range() object and produce the numbers.\n\nseq = range(0,11) #same as range(11)\nseq #doesn't show values\n\nrange(0, 11)\n\n\n\nlist(seq)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\nfind_mean(list(seq))\n\n5.0\n\n\nIterators (and iterator type objects) are often used to save memory as you often don’t need the entire sequence, but do want to use them in some kind of order.\nBy iterating across the elements and not saving the entire object, we can save memory. We only need to know where we are on the iteration and how the iteration should be done!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "User Defined Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#add-a-default-argument",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#add-a-default-argument",
    "title": "User Defined Functions",
    "section": "Add a Default Argument",
    "text": "Add a Default Argument\nOften we want to give default arguments to our function. That is, arguments that are used unless the user specifies something else.\n\nSuppose we want to add in a trimmed mean functionality\nThis is a mean where we first remove the smallest p% of values and the largest p% of values. We then take the mean of the remaining numbers.\nA trimmed mean is more robust to outliers. For instance,\n\n\nfind_mean([1,2,3,4,5,100]) #the mean is greatly affected by the large value\n\n19.166666666666668\n\n\n\nfind_mean([1,2,3,4,5]) #remove the large value to get a better idea about 'most' of the data values\n\n3.0\n\n\nTo create a trimmed mean function (or option at least), we need to do the following:\n\nSort the observations\nRemove the lowest p% and highest p%\nFind mean on the remaining values\n\n\n#can pull in the floor and sqrt functions from math to help us out\nfrom math import floor, sqrt\n#generate 50 random values from the standard normal distribution (covered shortly)\nimport numpy as np\ny = np.random.default_rng(1).standard_normal(50)\n#convert to a list just so we are working with an object we've studied\ny = list(y)\ny[0:10]\n\n[0.345584192064786,\n 0.8216181435011584,\n 0.33043707618338714,\n -1.303157231604361,\n 0.9053558666731177,\n 0.4463745723640113,\n -0.5369532353602852,\n 0.5811181041963531,\n 0.36457239618607573,\n 0.294132496655526]\n\n\nNote that lists have a .sort() method but this modifies the list in place. Instead we can use the sorted() function which returns a new sorted version of the list.\n\nsort_y = sorted(y)\nprint(sort_y[0:10])\n\n[-2.7111624789659685, -1.8890132459676727, -1.6480751708556527, -1.303157231604361, -1.2273520542445742, -1.1120207626922813, -0.9447516230607774, -0.7819084623568421, -0.7364540870016669, -0.6832266617805622]\n\n\nNow, given a value of p, we can remove the lowest and high p% of values. We can do this with the floor() function. This gives us the largest interger below a given value.\n\nprint(floor(4))\nprint(floor(4.2))\nprint(floor(4.9))\n\n4\n4\n4\n\n\nGiven a p (for proportion) we can determine the number of observations corresponding to that proportion using the length of y.\n\np = 0.11\nprint(p*len(sort_y))\nto_remove = floor(p*len(sort_y))\nto_remove\n\n5.5\n\n\n5\n\n\nWe can remove observations by simply subsetting our list using the : operator we studied (slicing). Remember that this operator doesn’t include the last value. (i.e. 2:5 gives the 2, 3, and 4 values)\n\nprint([to_remove, len(sort_y)-to_remove])#values we want to keep are between these\n\n[5, 45]\n\n\n\nRemember, counting starts at 0\nWe want the remove the first 5 values so we should start with the 5th index (the 6th actual value!)\nWith a length 50 list, we want to remove the 46-50th elements which correspond to the 45-49 indices\nSince we don’t include our last index, we can end on 45\n\n\n#elements we want for a 11% trimmed mean\nsort_y[to_remove:(len(sort_y)-to_remove)]\n\n[-1.1120207626922813,\n -0.9447516230607774,\n -0.7819084623568421,\n -0.7364540870016669,\n -0.6832266617805622,\n -0.5369532353602852,\n -0.5140063716874629,\n -0.5062916583143148,\n -0.48211931267997826,\n -0.42219041157635356,\n -0.37760500712699807,\n -0.2924567509650886,\n -0.2756029052993704,\n -0.2571922406188707,\n -0.17477209205516195,\n -0.16290994799305278,\n -0.09826996785221727,\n -0.07204367972722743,\n 0.008142180518343508,\n 0.02842224131579679,\n 0.03558623705548571,\n 0.03972210748165899,\n 0.09548302746945433,\n 0.10901408782154753,\n 0.16746474422274113,\n 0.2136429974986111,\n 0.21732193102256359,\n 0.294132496655526,\n 0.33043707618338714,\n 0.345584192064786,\n 0.36457239618607573,\n 0.4463745723640113,\n 0.5467129866124469,\n 0.5811181041963531,\n 0.5937480717858228,\n 0.5988462126346276,\n 0.6467029962018469,\n 0.6630633723762617,\n 0.8216181435011584,\n 0.8911669542823284]\n\n\n\nModify the function arguments\nNow that we have the process down (this is a good way to write functions by the way, write them outside of a function first and then put the pieces into the function), we can add our arguments/calculations.\nWe’ll add a - method = argument with a default value of None. None is a special name that defines no value in python + If this argument takes on Trim, we’ll do a trimmed mean. + This can be done using if Boolean: with the resulting code to execute tabbed in four spaces (covered shortly!) - a p = argument to specify the proportion to remove with a default value set to 0.\n\ndef find_mean(y, method = None, p = 0):\n    \"\"\"\n    Quick function to find the mean\n    Assumes we have a list with only numeric type data\n    If method is set to Trim, will remove outer most p values off the data\n    before finding the mean\n    \"\"\"\n    if method == \"Trim\": #we'll cover if shortly! The indented code only runs if this condition is met\n      sort_y = sorted(y)\n      to_remove = floor(p*len(sort_y))\n      y = sort_y[to_remove:(len(sort_y)-to_remove)] #replace y with the modified version\n    return sum(y)/len(y)\n\nLet’s test the function!\n\nfind_mean(y, method = \"Trim\", p = 0) #usual mean\n\n-0.03607807742830818\n\n\n\nfind_mean(y, method = \"Trim\", p = 0.05) #5% trimmed mean\n\n-0.029659532804894563\n\n\n\nfind_mean(y, method = \"trim\", p = 0.05) #usual mean not trimmed if method is not set correctly\n\n-0.03607807742830817",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "User Defined Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#positional-vs-named-arguments",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#positional-vs-named-arguments",
    "title": "User Defined Functions",
    "section": "Positional vs Named Arguments",
    "text": "Positional vs Named Arguments\n\nA function can be called using positional or named args\n\n\n#def find_mean(y, method = None, p = 0):\nprint(find_mean(y, None))\nprint(find_mean(method = \"Trim\", p = 0.1, y = y))\nprint(find_mean(y, \"Trim\", 0.1))\n\n-0.03607807742830817\n-0.009797451217442077\n-0.009797451217442077\n\n\n\nYou can’t place positional args after a keyword though!\n\n\nfind_mean(y = x, \"Trim\") #throws an error\n\n\n  File \"&lt;ipython-input-20-39dc4eceb262&gt;\", line 1\n    find_mean(y = x, \"Trim\")\n                           ^\nSyntaxError: positional argument follows keyword argument",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "User Defined Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#defining-the-type-of-argument",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#defining-the-type-of-argument",
    "title": "User Defined Functions",
    "section": "Defining the Type of Argument",
    "text": "Defining the Type of Argument\n\nA function definition may look like:\n\ndef f(pos1, pos2, /, pos_or_kwd, *, kwd1, kwd2):\n           -----------    ----------     ----------\n           |              |                  |\n           |         Positional or keyword   |\n           |                                 - Keyword only\n           -- Positional only\n\ndef print_it(x, y, /):\n    print(\"Must pass x and y positionally!\" + x + y)\n\ndef print_it(x, /, y):\n    print(\"x must be passed positionally.  y can be positional or named\" + x + y)\n\ndef print_it(x, /, y, *, z):\n    print(\"Now z must be passed as a named argument\" + x + y + z)\n\nLet’s modify our mean function and show this.\n\n#with this, y must be passes positionally!\ndef find_mean(y, /, method = None, p = 0):\n    \"\"\"\n    Quick function to find the mean\n    Assumes we have a list with only numeric type data\n    If method is set to Trim, will remove outer most p values off the data\n    before finding the mean\n    \"\"\"\n    if method == \"Trim\": #we'll cover if shortly! The indented code only runs if this condition is met\n      sort_y = sorted(y)\n      to_remove = floor(p*len(sort_y))\n      y = sort_y[to_remove:(len(sort_y)-to_remove)] #replace y with the modified version\n    return sum(y)/len(y)\n\n\nfind_mean(y, \"Trim\", p = 0.1)\n\n-0.009797451217442077\n\n\n\nfind_mean(y = y, method = \"Trim\", p = 0.1) #this won't work!\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-24-665a7ded1b54&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 find_mean(y = y, method = \"Trim\", p = 0.1) #this won't work!\n\nTypeError: find_mean() got some positional-only arguments passed as keyword arguments: 'y'",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "User Defined Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#write-our-own-correlation-function",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#write-our-own-correlation-function",
    "title": "User Defined Functions",
    "section": "Write Our Own Correlation Function",
    "text": "Write Our Own Correlation Function\nJust to demonstrate something more complicated, let’s write our own function to compute the (usual) sample correlation between two variables, call them x and y.\n\nPearson’s correlation:\n\n\\[r = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\]\nwhere - \\((x_i, y_i)\\) are numeric variables observed on the same \\(n\\) units, \\(i=1,...,n\\)\n\nPlan\nFunction inputs: - \\(x\\), \\(y\\), lists with numeric entries only\nFunction body: - Find sample means for \\(x\\) and \\(y\\) - Compute numerator sum and denominator sums - Find quotient and return that value\n\nFinding Means\nLet’s create some example data. \\(x\\) and \\(y\\) won’t be related here so the sample correlation shoudl be near 0!\n\nx = list(range(1,51))\nprint(x[1:10])\nxbar = find_mean(x)\nxbar\n\n[2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n25.5\n\n\n\n#use same y as before\ny = list(np.random.default_rng(1).standard_normal(50))\nprint(y[1:10])\nybar = find_mean(y)\nybar\n\n[0.8216181435011584, 0.33043707618338714, -1.303157231604361, 0.9053558666731177, 0.4463745723640113, -0.5369532353602852, 0.5811181041963531, 0.36457239618607573, 0.294132496655526]\n\n\n-0.03607807742830817\n\n\nAgiain, these two vectors are not related and should have a near 0 correlation!\nNext, we need to find the numerator and denominator sums. Finding the sums will be easier once we learn arrays, but for now we’ll peak at a for loop and the zip() function.\nLet’s start with computation of \\[\\sum_{i=1}^n(x_i-\\bar{x})^2\\]\n\n#computation in one of our sums (we want this across all 50 values, then added up)\n(x[0]-xbar)**2\n\n600.25\n\n\nSo really we want to find all of these values:\n(x[0]-xbar)**2\n(x[1]-xbar)**2\n...\n(x[49]-xbar)**2\nWe can use for to iterate over the values of 0, 1, …, 49. Similar to function definitions and if statements, we just tab in (four spaces) the code to be executed at each iteration of the for loop.\n\n#initialize a value to store the sum in\nden_x = 0\n#use a for loop to iterate across values (studies more later!)\nfor i in x:\n    den_x += (i-xbar)**2\nden_x\n\n10412.5\n\n\nWe can very easily get a similar computation for \\(y\\)’s portion of the denominator.\nTo get the numerator, that’s a bit more work. We really need to find\n(x[0]-xbar)(y[0]-ybar)\n(x[1]-xbar)(y[1]-ybar)\n...\n(x[49]-xbar)(y[49]-ybar)\nWe can zip() the \\(x\\) and \\(y\\) lists together. This essentially just pairs the 0th elements, the 1st elements, etc. Then we can iterate over the values together.\n\nnum = 0\nfor i, j in zip(x, y): #i corresponds to the x elements and j the y elements\n    num += (i-xbar)*(j-ybar)\nnum\n\n-51.69981003655184\n\n\nOk, now we are ready to put these together and calculate our correlation!\n\ndef find_corr(x, y):\n    \"\"\"\n    Compute Pearson's Correlation Coefficient\n    x and y are assumed to be lists with numeric values\n    Data is assumed to have no missing values\n    \"\"\"\n    xbar = find_mean(x)\n    ybar = find_mean(y)\n    num = 0\n    den_x = 0\n    den_y = 0\n    for i, j in zip(x, y):\n        num +=(i-xbar)*(j-ybar)\n        den_x +=(i-xbar)**2\n        den_y +=(j-ybar)**2\n    return num/sqrt(den_x*den_y)\n\nLet’s test our function on our data!\n\nfind_corr(x, y) #near 0!\n\n-0.0813179110596017\n\n\nNote that all functions with a doc string have a .__doc__ attribute that you can look at to understand that function (assuming the doc string is useful!).\n\nprint(find_corr.__doc__)\n\n\n    Compute Pearson's Correlation Coefficient\n    x and y are assumed to be lists with numeric values\n    Data is assumed to have no missing values\n    \n\n\n\nprint(len.__doc__) #another example on a common function\n\nReturn the number of items in a container.\n\n\n\nprint(np.random.default_rng.__doc__) #another example\n\ndefault_rng(seed=None)\nConstruct a new Generator with the default BitGenerator (PCG64).\n\n    Parameters\n    ----------\n    seed : {None, int, array_like[ints], SeedSequence, BitGenerator, Generator}, optional\n        A seed to initialize the `BitGenerator`. If None, then fresh,\n        unpredictable entropy will be pulled from the OS. If an ``int`` or\n        ``array_like[ints]`` is passed, then it will be passed to\n        `SeedSequence` to derive the initial `BitGenerator` state. One may also\n        pass in a `SeedSequence` instance.\n        Additionally, when passed a `BitGenerator`, it will be wrapped by\n        `Generator`. If passed a `Generator`, it will be returned unaltered.\n\n    Returns\n    -------\n    Generator\n        The initialized generator object.\n\n    Notes\n    -----\n    If ``seed`` is not a `BitGenerator` or a `Generator`, a new `BitGenerator`\n    is instantiated. This function does not manage a default global instance.\n\n    See :ref:`seeding_and_entropy` for more information about seeding.\n    \n    Examples\n    --------\n    ``default_rng`` is the recommended constructor for the random number class\n    ``Generator``. Here are several ways we can construct a random \n    number generator using ``default_rng`` and the ``Generator`` class. \n    \n    Here we use ``default_rng`` to generate a random float:\n \n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; rng = np.random.default_rng(12345)\n    &gt;&gt;&gt; print(rng)\n    Generator(PCG64)\n    &gt;&gt;&gt; rfloat = rng.random()\n    &gt;&gt;&gt; rfloat\n    0.22733602246716966\n    &gt;&gt;&gt; type(rfloat)\n    &lt;class 'float'&gt;\n     \n    Here we use ``default_rng`` to generate 3 random integers between 0 \n    (inclusive) and 10 (exclusive):\n        \n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; rng = np.random.default_rng(12345)\n    &gt;&gt;&gt; rints = rng.integers(low=0, high=10, size=3)\n    &gt;&gt;&gt; rints\n    array([6, 2, 7])\n    &gt;&gt;&gt; type(rints[0])\n    &lt;class 'numpy.int64'&gt;\n    \n    Here we specify a seed so that we have reproducible results:\n    \n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n    &gt;&gt;&gt; print(rng)\n    Generator(PCG64)\n    &gt;&gt;&gt; arr1 = rng.random((3, 3))\n    &gt;&gt;&gt; arr1\n    array([[0.77395605, 0.43887844, 0.85859792],\n           [0.69736803, 0.09417735, 0.97562235],\n           [0.7611397 , 0.78606431, 0.12811363]])\n\n    If we exit and restart our Python interpreter, we'll see that we\n    generate the same random numbers again:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n    &gt;&gt;&gt; arr2 = rng.random((3, 3))\n    &gt;&gt;&gt; arr2\n    array([[0.77395605, 0.43887844, 0.85859792],\n           [0.69736803, 0.09417735, 0.97562235],\n           [0.7611397 , 0.78606431, 0.12811363]])\n\n    \n\n\nAttributes are another important thing we’ll learn about, especially when we get into pyspark. We now have\n\nfunctions() which go prior to the object\n.methods() that go on the end of the object\n\nand\n\n.attributes that also go on the end of an object just with no ().",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "User Defined Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#other-things-to-note",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#other-things-to-note",
    "title": "User Defined Functions",
    "section": "Other Things to Note",
    "text": "Other Things to Note\n\nWhen executing a function, a new symbol table is used for the local variables\nThis keeps us from accidentally overwriting something\n\n\nimport numpy as np\ny = np.array(range(1,11))\n\ndef square(z):\n    y = z**2\n    print(\"In the function environment, z = \" + str(z) + \" and y = \" + str(y))\n    return(y)\n\nprint(square(y))\nprint(y) #y is not changed\n\nIn the function environment, z = [ 1  2  3  4  5  6  7  8  9 10] and y = [  1   4   9  16  25  36  49  64  81 100]\n[  1   4   9  16  25  36  49  64  81 100]\n[ 1  2  3  4  5  6  7  8  9 10]\n\n\n\nprint(z) #z isn't defined outside the function call! error\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-36-7d7ac3dfdf36&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 print(z) #z isn't defined outside the function call! error\n\nNameError: name 'z' is not defined\n\n\n\n\nYou can define global variables from within a function using global\n\n\ndef square(z):\n    global y\n    y = z**2\n    print(\"In the function environment, z = \" + str(z) + \" and y = \" + str(y))\n    return(y)\n\nprint(square(y))\nprint(y) #modified globally now\n\nIn the function environment, z = [ 1  2  3  4  5  6  7  8  9 10] and y = [  1   4   9  16  25  36  49  64  81 100]\n[  1   4   9  16  25  36  49  64  81 100]\n[  1   4   9  16  25  36  49  64  81 100]\n\n\n\nIf nothing is returned from a function (with return) then it actually returns the special None\n\n\ndef square_it(a):\n    if (type(a) == int) or (type(a) == float):\n      return a**2\n    else:\n      return\n\nprint(square_it(10))\nprint(square_it(10.5))\nprint(square_it(\"10\"))\n\n100\n110.25\nNone\n\n\n\nDefault values are only evaluated once - at the time of the function definition\nMutable objects can cause an issue! (Lists are mutable as they can be changed, some objects, like tuples, are immutable and can’t be modified.)\n\n\n#append a value to a list but give a default empty list if not given\ndef my_append(value, L = []):\n    L.append(value)\n    return L\n\n#correctly appends \"A\" to the list\nprint(my_append(\"A\"))\n#appends \"B\" to the previous list as L = [] was only evaluated at the time the function was created!\nprint(my_append(\"B\"))\n\n['A']\n['A', 'B']\n\n\n\nTo avoid this behavior, instead define the default value as None and take care of things within the function body\n\n\ndef my_append(value, L = None):\n    if L is None:\n        L = []\n    L.append(value)\n    return L\n\nprint(my_append(\"A\"))\nprint(my_append(\"B\"))\n\n['A']\n['B']",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "User Defined Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/08-User_Defined_Functions.html#video-demo",
    "href": "01_Programming_in_python/08-User_Defined_Functions.html#video-demo",
    "title": "User Defined Functions",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video demo gives another example of creating our own function! Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=ae1858b3-74cf-4065-8ec7-b0f800e4f827&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "User Defined Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#big-picture",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#big-picture",
    "title": "List Basics & Strings",
    "section": "Big Picture",
    "text": "Big Picture\nWe’ve learned a little about how python and our Jupyterlab coding environment works.\nNext, we’ll go through and look at a number of common data structures used in python. We’ll try to follow a similar introduction for each data struture where we\n\nintroduce the data structure\ndiscuss common functions and methods\ndo some quick examples of using them\n\nAlong the way we’ll learn some things we want to do with data along with control flow operators (if/then/else, looping, etc.)!\nNote: These types of webpages are built from Jupyter notebooks (.ipynb files). You can access your own versions of them by clicking here. It is highly recommended that you go through and run the notebooks yourself, modifying and rerunning things where you’d like!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#data-structures",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#data-structures",
    "title": "List Basics & Strings",
    "section": "Data Structures",
    "text": "Data Structures\n\nWe’ll start by discussing the most important built-in data types\n\nStrings, Numeric types, Booleans\nCompound data types (Lists, Tuples, Dictionaries)\n\nThen we’ll move to commonly used data structures from modules we use in statistics/data science\n\nNumPy arrays\nPandas data frames\n\n\nLists, Tuples, Strings, and arrays are all sequences (ish) so they have similar functions and behavior! It is important to recognize these common behaviors",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#constructing-a-list",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#constructing-a-list",
    "title": "List Basics & Strings",
    "section": "Constructing a List",
    "text": "Constructing a List\nFour major ways to create a list\n\n[element1, element2]\nlist((element1, element2, ...))\ncreate an empty list and use the append method to add elements\nlist comprehensions\n\n\n#first create via [1st_element, 2nd_element, etc]\nx = [10, 15, 10, 100, \"Help!\"]\nprint(type(x))\nx\n\n&lt;class 'list'&gt;\n\n\n[10, 15, 10, 100, 'Help!']\n\n\n\n#create via list()\n#Note the 'extra' set of () needed within\ny = list((\"Python\", \"List\", 5))\ny\n\n['Python', 'List', 5]\n\n\n\n#range() is a function that is 'iterable'. By putting it in a list, we get the values out\nrange(1,10)\n\nrange(1, 10)\n\n\n\n#notice range doesn't give the 'last' value\nz = list(range(1,10))\nz\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nOn sequence type objects, * replicates the object a certain number of times. This is common behavior to remember!\n\nz * 2\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nAs lists don’t really restrict what its elements can be, lists can contain lists!\n\nw = [list(range(1,3)), z, 3]\nw\n\n[[1, 2], [1, 2, 3, 4, 5, 6, 7, 8, 9], 3]",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#list-operations",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#list-operations",
    "title": "List Basics & Strings",
    "section": "List Operations",
    "text": "List Operations\n\nIndexing\nVery often we want to obtain pieces or elements of an object. We can easily do this with lists.\n\nIndex with a [] after the object name\nCounting starts at 0\nNegative index counts from reverse (starting at 1…)\n\n\nx = [10, 15, 10, 100, \"Help!\"]\nprint(x[0])\nprint(x[1])\nprint(x[-1])\nprint(x[-2])\n\n10\n15\nHelp!\n100\n\n\n\nw = [list(range(1,5)), x, 3]\nprint(w)\n#the first element is a list so the list is returned\nprint(w[0])\n#similar with the second element\nprint(w[1])\n\n[[1, 2, 3, 4], [10, 15, 10, 100, 'Help!'], 3]\n[1, 2, 3, 4]\n[10, 15, 10, 100, 'Help!']\n\n\nWe can do more than one level of indexing with a single line of code (when applicable). As w[1] returns a list we can use [] after w[1] to return a specific element or slice from that list.\n\nprint(w[1][0])\n\n10\n\n\n\n\n\nSlicing\nOften we want to return more than one element at a time with our sequence type objects. This is called slicing.\n\nWe can return multiple elements at once with :\n\nLeaving it blank on the left gives everything up until the index prior to the number given\nBlank on the right gives everything after the desired starting index (counting starts at 0)\n\n\n\nx = [10, 15, 10, 100, \"Help!\"]\nx\n\n[10, 15, 10, 100, 'Help!']\n\n\n\nx[:2]\n\n[10, 15]\n\n\n\nx[:3]\n\n[10, 15, 10]\n\n\n\nx[1:]\n\n[15, 10, 100, 'Help!']\n\n\n\nx[1:3]\n\n[15, 10]\n\n\nAgain, if we have a list with lists (or other sequence type objects in them) slicing will still return those objects as a list.\n\nw = [list(range(1,5)), x, 3]\nw\n\n[[1, 2, 3, 4], [10, 15, 10, 100, 'Help!'], 3]\n\n\n\n#here a list of lists\nw[:2]\n\n[[1, 2, 3, 4], [10, 15, 10, 100, 'Help!']]\n\n\n\n#here just the single list\nw[1]\n\n[10, 15, 10, 100, 'Help!']\n\n\n\n#can index what gets returned if that makes sense to do!\nw[1][1:3]\n\n[15, 10]",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#functions-methods",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#functions-methods",
    "title": "List Basics & Strings",
    "section": "Functions & Methods",
    "text": "Functions & Methods\nRecall: Two major ways to do an operation on a variable/object: functions and methods\n\nFunctions: function_name(myvar, other_args)\nWe saw the len() and max() functions earlier\n\n\nmyList = [1, 10, 100, 1000]\nprint(len(myList))\nmax(myList)\n\n4\n\n\n1000\n\n\n\nMethods: myvar.method(other_args)\nRecall that .pop() returns and removes the last element\n\n\nmyList.pop(3)\n\n1000\n\n\n\nmyList\n\n[1, 10, 100]\n\n\n\nThe .append() method adds an element to the end of the list\n\n\nmyList.append(100000)\nmyList\n\n[1, 10, 100, 100000]\n\n\nThe methods for lists are listed at the top of this page of the python 3 documentation.\nSome of the common functions in python are listed on this page of the documentation.",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#constructing-strings",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#constructing-strings",
    "title": "List Basics & Strings",
    "section": "Constructing Strings",
    "text": "Constructing Strings\n\nText is represented as a sequence of characters (letters, digits, and symbols) called a string (Nice reference)\n\nData type: str\nCreated using single or double quotes\n\n\n\n#can use either ' or \" to create a string\n'wolf'\n\n'wolf'\n\n\n\n\"pack\"\n\n'pack'\n\n\n\nx = 'wolf'\nprint(type(x))\nprint(x)\n\n&lt;class 'str'&gt;\nwolf\n\n\n\nInstead of ’ or “, you can use str() to create a string. This is called casting\n\n\nx = str(10)\nx\n\n'10'",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#string-operations",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#string-operations",
    "title": "List Basics & Strings",
    "section": "String Operations",
    "text": "String Operations\n\nIndexing\nRemember that strings and lists are both sequence type objects. Therefore, we have similar operations on these objects.\n\nmy_string = \"wolf pack\"\n\n\nEach element of the my_string variable contains a different character from \"wolf pack\"\nAs with lists, we access these elements using []\nThe first element is indexed by 0\n\n\nmy_string[0]\n\n'w'\n\n\n\nmy_string[1]\n\n'o'\n\n\n\nAccess the elements of the my_string variable in reverse order using a - (start with 1 not 0 for the last element though!)\n\n\nmy_string[-1]\n\n'k'\n\n\n\n\n\nSlicing\n\nmy_string = \"wolf pack\"\n\n\nSlicing a string refers to returning more than one character of a string (similar to lists!)\n\nSlice using :\n\n\n\nmy_string[4:]\n\n' pack'\n\n\n\nmy_string[:3]\n\n'wol'\n\n\n\nmy_string[3:4]\n\n'f'\n\n\n\n#s[:i] + s[i:] gives back s\nmy_string[:3] + my_string[3:]\n\n'wolf pack'\n\n\n\n\n\nConcatenating\nSeveral built-in operations on strings\n\n+ will concatenate two strings together\n\n\n'wolf' + ' pack'\n\n'wolf pack'\n\n\n\n'wolf' + ' pack' + \" is\" + \" cool\"\n\n'wolf pack is cool'\n\n\n\nString literals next to each other are automatically concatenated\n\n\n'wolf' ' pack'\n'wolf' ' pack' ' is' ' cool'\n\n'wolf pack is cool'\n\n\n\nThis won’t work on variables though!\n\n\nx = 'wolf'\n#throws an error\nx ' pack'\n\n\n  File \"/tmp/ipython-input-2305203029.py\", line 3\n    x ' pack'\n      ^\nSyntaxError: invalid syntax\n\n\n\n\n\nx + ' pack'\n\n'10 pack'\n\n\nThis behavior actually works with lists as well!\n\n[1, 2, 3] + [\"a\", [5, 6,]]\n\n[1, 2, 3, 'a', [5, 6]]\n\n\n\n\n\nNo Implicit Coercion\nYou might wonder what happens when an operator like + is applied to a string and a numeric value.\n\n#throws an error\n'wolfpack' + 2\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/tmp/ipython-input-3286917817.py in &lt;cell line: 0&gt;()\n      1 #throws an error\n----&gt; 2 'wolfpack' + 2\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\n\nIf you come from R, R does this implicit coercion for you without warning (dangerous but you get used to it!)\nIn python, to join a string and number cast the number as a string!\n\n\n'Four score and ' + str(7) + ' years ago'\n\n'Four score and 7 years ago'\n\n\n\nString Operations (Concatenating Repeats)\nYou can also repeat strings with the * operator and an integer (again similar to a list)\n\nprint('go pack ' * 3)\nprint('go pack ' * 0) #returns an empty string ''\nprint('go pack ' * 5)\n\ngo pack go pack go pack \n\ngo pack go pack go pack go pack go pack",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#functions-methods-1",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#functions-methods-1",
    "title": "List Basics & Strings",
    "section": "Functions & Methods",
    "text": "Functions & Methods\n\nThere are several built-in operations on strings\n\nlen() returns the number of characters\nsorted() returns the sorted values as a list\n\n\n\nlen('wolf pack')\n\n9\n\n\n\nlen('241!')\n\n4\n\n\n\nlen(' ')\n\n1\n\n\n\nlen(\"\")\n\n0\n\n\n\nsorted(\"wolf pack\")\n\n[' ', 'a', 'c', 'f', 'k', 'l', 'o', 'p', 'w']\n\n\n\nMany methods as well. Some common examples are below:\n\n\nmy_string = '  wolf pack  '\n\n\n#create an upper case version of the string\nmy_string.upper()\n\n'  WOLF PACK  '\n\n\n\n#this doesn't overwrite the string though!\nmy_string\n\n'  wolf pack  '\n\n\n\n#remove whitespace from the ends\nmy_string.strip()\n\n'wolf pack'\n\n\n\n#replace elements\nmy_string.replace(\"a\", \"e\")\n\n'  wolf peck  '\n\n\n\n#split the string by a character (here a space) (note this returns a list!)\nmy_string.strip().split(\" \")\n\n['wolf', 'pack']",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#immutability-of-strings",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#immutability-of-strings",
    "title": "List Basics & Strings",
    "section": "Immutability of Strings",
    "text": "Immutability of Strings\n\nWe saw that lists could be modified. That means they are mutable\nStrings are immutable\n\nIndividual characters can’t be modified\n\n\n\nmy_string = \"wolf pack\"\n#this will throw an error\nmy_string[1] = \"a\"\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/tmp/ipython-input-3255422241.py in &lt;cell line: 0&gt;()\n      1 my_string = \"wolf pack\"\n      2 #this will throw an error\n----&gt; 3 my_string[1] = \"a\"\n\nTypeError: 'str' object does not support item assignment",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#inserting-values-into-strings",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#inserting-values-into-strings",
    "title": "List Basics & Strings",
    "section": "Inserting Values Into Strings",
    "text": "Inserting Values Into Strings\nSometimes we want to place certain elements into a string via variables or values. This can be done using the .format() method.\n\nyears = 3\nsalary = 100000\nmyorder = \"I have {1} years of experience and would like a salary of {0}.\"\nprint(myorder.format(salary, years))\n\nI have 3 years of experience and would like a salary of 100000.\n\n\n\nDon’t need the numbers, but then you must position correctly\n\n\nmyorder = \"I have {} years of experience and would like a salary of {}.\"\nprint(myorder.format(years, salary))\n\nI have 3 years of experience and would like a salary of 100000.\n\n\nThere are a few other ways to do this that we’ll visit later on!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/05-List_Basics_Strings.html#video-demo",
    "href": "01_Programming_in_python/05-List_Basics_Strings.html#video-demo",
    "title": "List Basics & Strings",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video demonstration shows some quick exercises with strings and lists. Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src = \"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=72bd0292-4c48-4064-8977-b0ef017167f6&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "List Basics & Strings"
    ]
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#what-is-a-module",
    "href": "01_Programming_in_python/03-Modules.html#what-is-a-module",
    "title": "Modules",
    "section": "What is a Module?",
    "text": "What is a Module?\n\nA collection of (related) definitions and statements that are grouped together in a single file (a .py file)\n\nGives access to additional functionality\n\nSome come standard, others must be installed (i.e. downloaded)\n\nModules are then imported into your session\n\nFully imported with import module_name\nSelective import with from module_name import thing1 thing2",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Modules"
    ]
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#commonly-used-built-in-already-downloaded-modules",
    "href": "01_Programming_in_python/03-Modules.html#commonly-used-built-in-already-downloaded-modules",
    "title": "Modules",
    "section": "Commonly Used Built-in (Already Downloaded) Modules",
    "text": "Commonly Used Built-in (Already Downloaded) Modules\nThese modules are already downloaded but not loaded in when starting python or a Jupyterlab notebook.\n\nmath\n\nMath constants (pi, e, etc.)\nFunctions commonly used functions (exp(), sin(), sqrt(), etc.)\n\nrandom\n\nRandom sampling and random number generation\n\nstatistics\n\nSummary stats (but scipy and pandas have a lot more)\n\ndatetime\n\nFunctionality for working with dates",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Modules"
    ]
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#importing-a-module",
    "href": "01_Programming_in_python/03-Modules.html#importing-a-module",
    "title": "Modules",
    "section": "Importing a Module",
    "text": "Importing a Module\n\nFor built-in modules or modules we’ve downloaded ourselves, we can load the entire module into our session with import module_name\n\n\nimport math\ntype(math)\n\nmodule\n\n\n\nWe can see the functionality using help() on the module but it is usually better to find the documentation on the web!\n\n\nhelp(math)\n\nHelp on built-in module math:\n\nNAME\n    math\n\nDESCRIPTION\n    This module provides access to the mathematical functions\n    defined by the C standard.\n\nFUNCTIONS\n    acos(x, /)\n        Return the arc cosine (measured in radians) of x.\n\n        The result is between 0 and pi.\n\n    acosh(x, /)\n        Return the inverse hyperbolic cosine of x.\n\n    asin(x, /)\n        Return the arc sine (measured in radians) of x.\n\n        The result is between -pi/2 and pi/2.\n\n    asinh(x, /)\n        Return the inverse hyperbolic sine of x.\n\n    atan(x, /)\n        Return the arc tangent (measured in radians) of x.\n\n        The result is between -pi/2 and pi/2.\n\n    atan2(y, x, /)\n        Return the arc tangent (measured in radians) of y/x.\n\n        Unlike atan(y/x), the signs of both x and y are considered.\n\n    atanh(x, /)\n        Return the inverse hyperbolic tangent of x.\n\n    cbrt(x, /)\n        Return the cube root of x.\n\n    ceil(x, /)\n        Return the ceiling of x as an Integral.\n\n        This is the smallest integer &gt;= x.\n\n    comb(n, k, /)\n        Number of ways to choose k items from n items without repetition and without order.\n\n        Evaluates to n! / (k! * (n - k)!) when k &lt;= n and evaluates\n        to zero when k &gt; n.\n\n        Also called the binomial coefficient because it is equivalent\n        to the coefficient of k-th term in polynomial expansion of the\n        expression (1 + x)**n.\n\n        Raises TypeError if either of the arguments are not integers.\n        Raises ValueError if either of the arguments are negative.\n\n    copysign(x, y, /)\n        Return a float with the magnitude (absolute value) of x but the sign of y.\n\n        On platforms that support signed zeros, copysign(1.0, -0.0)\n        returns -1.0.\n\n    cos(x, /)\n        Return the cosine of x (measured in radians).\n\n    cosh(x, /)\n        Return the hyperbolic cosine of x.\n\n    degrees(x, /)\n        Convert angle x from radians to degrees.\n\n    dist(p, q, /)\n        Return the Euclidean distance between two points p and q.\n\n        The points should be specified as sequences (or iterables) of\n        coordinates.  Both inputs must have the same dimension.\n\n        Roughly equivalent to:\n            sqrt(sum((px - qx) ** 2.0 for px, qx in zip(p, q)))\n\n    erf(x, /)\n        Error function at x.\n\n    erfc(x, /)\n        Complementary error function at x.\n\n    exp(x, /)\n        Return e raised to the power of x.\n\n    exp2(x, /)\n        Return 2 raised to the power of x.\n\n    expm1(x, /)\n        Return exp(x)-1.\n\n        This function avoids the loss of precision involved in the direct evaluation of exp(x)-1 for small x.\n\n    fabs(x, /)\n        Return the absolute value of the float x.\n\n    factorial(n, /)\n        Find n!.\n\n        Raise a ValueError if x is negative or non-integral.\n\n    floor(x, /)\n        Return the floor of x as an Integral.\n\n        This is the largest integer &lt;= x.\n\n    fmod(x, y, /)\n        Return fmod(x, y), according to platform C.\n\n        x % y may differ.\n\n    frexp(x, /)\n        Return the mantissa and exponent of x, as pair (m, e).\n\n        m is a float and e is an int, such that x = m * 2.**e.\n        If x is 0, m and e are both 0.  Else 0.5 &lt;= abs(m) &lt; 1.0.\n\n    fsum(seq, /)\n        Return an accurate floating-point sum of values in the iterable seq.\n\n        Assumes IEEE-754 floating-point arithmetic.\n\n    gamma(x, /)\n        Gamma function at x.\n\n    gcd(*integers)\n        Greatest Common Divisor.\n\n    hypot(...)\n        hypot(*coordinates) -&gt; value\n\n        Multidimensional Euclidean distance from the origin to a point.\n\n        Roughly equivalent to:\n            sqrt(sum(x**2 for x in coordinates))\n\n        For a two dimensional point (x, y), gives the hypotenuse\n        using the Pythagorean theorem:  sqrt(x*x + y*y).\n\n        For example, the hypotenuse of a 3/4/5 right triangle is:\n\n            &gt;&gt;&gt; hypot(3.0, 4.0)\n            5.0\n\n    isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0)\n        Determine whether two floating-point numbers are close in value.\n\n          rel_tol\n            maximum difference for being considered \"close\", relative to the\n            magnitude of the input values\n          abs_tol\n            maximum difference for being considered \"close\", regardless of the\n            magnitude of the input values\n\n        Return True if a is close in value to b, and False otherwise.\n\n        For the values to be considered close, the difference between them\n        must be smaller than at least one of the tolerances.\n\n        -inf, inf and NaN behave similarly to the IEEE 754 Standard.  That\n        is, NaN is not close to anything, even itself.  inf and -inf are\n        only close to themselves.\n\n    isfinite(x, /)\n        Return True if x is neither an infinity nor a NaN, and False otherwise.\n\n    isinf(x, /)\n        Return True if x is a positive or negative infinity, and False otherwise.\n\n    isnan(x, /)\n        Return True if x is a NaN (not a number), and False otherwise.\n\n    isqrt(n, /)\n        Return the integer part of the square root of the input.\n\n    lcm(*integers)\n        Least Common Multiple.\n\n    ldexp(x, i, /)\n        Return x * (2**i).\n\n        This is essentially the inverse of frexp().\n\n    lgamma(x, /)\n        Natural logarithm of absolute value of Gamma function at x.\n\n    log(...)\n        log(x, [base=math.e])\n        Return the logarithm of x to the given base.\n\n        If the base is not specified, returns the natural logarithm (base e) of x.\n\n    log10(x, /)\n        Return the base 10 logarithm of x.\n\n    log1p(x, /)\n        Return the natural logarithm of 1+x (base e).\n\n        The result is computed in a way which is accurate for x near zero.\n\n    log2(x, /)\n        Return the base 2 logarithm of x.\n\n    modf(x, /)\n        Return the fractional and integer parts of x.\n\n        Both results carry the sign of x and are floats.\n\n    nextafter(x, y, /, *, steps=None)\n        Return the floating-point value the given number of steps after x towards y.\n\n        If steps is not specified or is None, it defaults to 1.\n\n        Raises a TypeError, if x or y is not a double, or if steps is not an integer.\n        Raises ValueError if steps is negative.\n\n    perm(n, k=None, /)\n        Number of ways to choose k items from n items without repetition and with order.\n\n        Evaluates to n! / (n - k)! when k &lt;= n and evaluates\n        to zero when k &gt; n.\n\n        If k is not specified or is None, then k defaults to n\n        and the function returns n!.\n\n        Raises TypeError if either of the arguments are not integers.\n        Raises ValueError if either of the arguments are negative.\n\n    pow(x, y, /)\n        Return x**y (x to the power of y).\n\n    prod(iterable, /, *, start=1)\n        Calculate the product of all the elements in the input iterable.\n\n        The default start value for the product is 1.\n\n        When the iterable is empty, return the start value.  This function is\n        intended specifically for use with numeric values and may reject\n        non-numeric types.\n\n    radians(x, /)\n        Convert angle x from degrees to radians.\n\n    remainder(x, y, /)\n        Difference between x and the closest integer multiple of y.\n\n        Return x - n*y where n*y is the closest integer multiple of y.\n        In the case where x is exactly halfway between two multiples of\n        y, the nearest even value of n is used. The result is always exact.\n\n    sin(x, /)\n        Return the sine of x (measured in radians).\n\n    sinh(x, /)\n        Return the hyperbolic sine of x.\n\n    sqrt(x, /)\n        Return the square root of x.\n\n    sumprod(p, q, /)\n        Return the sum of products of values from two iterables p and q.\n\n        Roughly equivalent to:\n\n            sum(itertools.starmap(operator.mul, zip(p, q, strict=True)))\n\n        For float and mixed int/float inputs, the intermediate products\n        and sums are computed with extended precision.\n\n    tan(x, /)\n        Return the tangent of x (measured in radians).\n\n    tanh(x, /)\n        Return the hyperbolic tangent of x.\n\n    trunc(x, /)\n        Truncates the Real x to the nearest Integral toward 0.\n\n        Uses the __trunc__ magic method.\n\n    ulp(x, /)\n        Return the value of the least significant bit of the float x.\n\nDATA\n    e = 2.718281828459045\n    inf = inf\n    nan = nan\n    pi = 3.141592653589793\n    tau = 6.283185307179586\n\nFILE\n    (built-in)\n\n\n\n\n\n\nUsing a Module’s Function/Objects\n\nFunctions contained in our module cannot be called as a built-in function when using import module_name:\nFor instance, the math module contains the sqrt() function\n\n\nsqrt(9)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipython-input-3918740153.py in &lt;cell line: 0&gt;()\n----&gt; 1 sqrt(9)\n\nNameError: name 'sqrt' is not defined\n\n\n\n\nAn error!\nWe must use the module prefix when calling the function:\n\n\nprint(math.sqrt(9))\nmath.factorial(5)\n\n3.0\n\n\n120\n\n\n\nWith the math module we could do a common statistical computation such as evaulating the Normal distribution PDF\n\n\\[\nf(1;\\mu = 3, \\sigma = 1) = \\frac{1}{\\sqrt{2\\pi}}\\exp^{-\\frac{1}{2}(1 - 3)^2}\n\\]\n\nThis Normal distribution PDF expression can be evaluated using:\n\n\n(1.0/math.sqrt(2*math.pi))*math.exp(-0.5*(1 - 3.0)**2)\n\n0.05399096651318806",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Modules"
    ]
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#module-variables",
    "href": "01_Programming_in_python/03-Modules.html#module-variables",
    "title": "Modules",
    "section": "Module Variables",
    "text": "Module Variables\nModules can contain more than functions!\n\nThe math module also defines variables like e and pi\n\n\nprint(math.e)\nprint(math.pi)\nradius = 5\nprint('The area is', math.pi * radius ** 2)\n\n2.718281828459045\n3.141592653589793\nThe area is 78.53981633974483\n\n\n\nYou can overwrite these values (just like built-in objects) but, you know, don’t do that!\n\n\n\nrandom Module\n\nWe’ll deal with random sampling from time to time\n\nThe random module gives functionality to do so (although we’ll use other modules when we do this later)\n\n\nimport random\nhelp(random)\n\nHelp on module random:\n\nNAME\n    random - Random variable generators.\n\nMODULE REFERENCE\n    https://docs.python.org/3.12/library/random.html\n\n    The following documentation is automatically generated from the Python\n    source files.  It may be incomplete, incorrect or include features that\n    are considered implementation detail and may vary between Python\n    implementations.  When in doubt, consult the module reference at the\n    location listed above.\n\nDESCRIPTION\n        bytes\n        -----\n               uniform bytes (values between 0 and 255)\n\n        integers\n        --------\n               uniform within range\n\n        sequences\n        ---------\n               pick random element\n               pick random sample\n               pick weighted random sample\n               generate random permutation\n\n        distributions on the real line:\n        ------------------------------\n               uniform\n               triangular\n               normal (Gaussian)\n               lognormal\n               negative exponential\n               gamma\n               beta\n               pareto\n               Weibull\n\n        distributions on the circle (angles 0 to 2pi)\n        ---------------------------------------------\n               circular uniform\n               von Mises\n\n        discrete distributions\n        ----------------------\n               binomial\n\n\n    General notes on the underlying Mersenne Twister core generator:\n\n    * The period is 2**19937-1.\n    * It is one of the most extensively tested generators in existence.\n    * The random() method is implemented in C, executes in a single Python step,\n      and is, therefore, threadsafe.\n\nCLASSES\n    _random.Random(builtins.object)\n        Random\n            SystemRandom\n\n    class Random(_random.Random)\n     |  Random(x=None)\n     |\n     |  Random number generator base class used by bound module functions.\n     |\n     |  Used to instantiate instances of Random to get generators that don't\n     |  share state.\n     |\n     |  Class Random can also be subclassed if you want to use a different basic\n     |  generator of your own devising: in that case, override the following\n     |  methods:  random(), seed(), getstate(), and setstate().\n     |  Optionally, implement a getrandbits() method so that randrange()\n     |  can cover arbitrarily large ranges.\n     |\n     |  Method resolution order:\n     |      Random\n     |      _random.Random\n     |      builtins.object\n     |\n     |  Methods defined here:\n     |\n     |  __getstate__(self)\n     |      Helper for pickle.\n     |\n     |  __init__(self, x=None)\n     |      Initialize an instance.\n     |\n     |      Optional argument x controls seeding, as for Random.seed().\n     |\n     |  __reduce__(self)\n     |      Helper for pickle.\n     |\n     |  __setstate__(self, state)\n     |\n     |  betavariate(self, alpha, beta)\n     |      Beta distribution.\n     |\n     |      Conditions on the parameters are alpha &gt; 0 and beta &gt; 0.\n     |      Returned values range between 0 and 1.\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = alpha / (alpha + beta)\n     |          Var[X] = alpha * beta / ((alpha + beta)**2 * (alpha + beta + 1))\n     |\n     |  binomialvariate(self, n=1, p=0.5)\n     |      Binomial random variable.\n     |\n     |      Gives the number of successes for *n* independent trials\n     |      with the probability of success in each trial being *p*:\n     |\n     |          sum(random() &lt; p for i in range(n))\n     |\n     |      Returns an integer in the range:   0 &lt;= X &lt;= n\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = n * p\n     |          Var[x] = n * p * (1 - p)\n     |\n     |  choice(self, seq)\n     |      Choose a random element from a non-empty sequence.\n     |\n     |  choices(self, population, weights=None, *, cum_weights=None, k=1)\n     |      Return a k sized list of population elements chosen with replacement.\n     |\n     |      If the relative weights or cumulative weights are not specified,\n     |      the selections are made with equal probability.\n     |\n     |  expovariate(self, lambd=1.0)\n     |      Exponential distribution.\n     |\n     |      lambd is 1.0 divided by the desired mean.  It should be\n     |      nonzero.  (The parameter would be called \"lambda\", but that is\n     |      a reserved word in Python.)  Returned values range from 0 to\n     |      positive infinity if lambd is positive, and from negative\n     |      infinity to 0 if lambd is negative.\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = 1 / lambd\n     |          Var[X] = 1 / lambd ** 2\n     |\n     |  gammavariate(self, alpha, beta)\n     |      Gamma distribution.  Not the gamma function!\n     |\n     |      Conditions on the parameters are alpha &gt; 0 and beta &gt; 0.\n     |\n     |      The probability distribution function is:\n     |\n     |                  x ** (alpha - 1) * math.exp(-x / beta)\n     |        pdf(x) =  --------------------------------------\n     |                    math.gamma(alpha) * beta ** alpha\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = alpha * beta\n     |          Var[X] = alpha * beta ** 2\n     |\n     |  gauss(self, mu=0.0, sigma=1.0)\n     |      Gaussian distribution.\n     |\n     |      mu is the mean, and sigma is the standard deviation.  This is\n     |      slightly faster than the normalvariate() function.\n     |\n     |      Not thread-safe without a lock around calls.\n     |\n     |  getstate(self)\n     |      Return internal state; can be passed to setstate() later.\n     |\n     |  lognormvariate(self, mu, sigma)\n     |      Log normal distribution.\n     |\n     |      If you take the natural logarithm of this distribution, you'll get a\n     |      normal distribution with mean mu and standard deviation sigma.\n     |      mu can have any value, and sigma must be greater than zero.\n     |\n     |  normalvariate(self, mu=0.0, sigma=1.0)\n     |      Normal distribution.\n     |\n     |      mu is the mean, and sigma is the standard deviation.\n     |\n     |  paretovariate(self, alpha)\n     |      Pareto distribution.  alpha is the shape parameter.\n     |\n     |  randbytes(self, n)\n     |      Generate n random bytes.\n     |\n     |  randint(self, a, b)\n     |      Return random integer in range [a, b], including both end points.\n     |\n     |  randrange(self, start, stop=None, step=1)\n     |      Choose a random item from range(stop) or range(start, stop[, step]).\n     |\n     |      Roughly equivalent to ``choice(range(start, stop, step))`` but\n     |      supports arbitrarily large ranges and is optimized for common cases.\n     |\n     |  sample(self, population, k, *, counts=None)\n     |      Chooses k unique random elements from a population sequence.\n     |\n     |      Returns a new list containing elements from the population while\n     |      leaving the original population unchanged.  The resulting list is\n     |      in selection order so that all sub-slices will also be valid random\n     |      samples.  This allows raffle winners (the sample) to be partitioned\n     |      into grand prize and second place winners (the subslices).\n     |\n     |      Members of the population need not be hashable or unique.  If the\n     |      population contains repeats, then each occurrence is a possible\n     |      selection in the sample.\n     |\n     |      Repeated elements can be specified one at a time or with the optional\n     |      counts parameter.  For example:\n     |\n     |          sample(['red', 'blue'], counts=[4, 2], k=5)\n     |\n     |      is equivalent to:\n     |\n     |          sample(['red', 'red', 'red', 'red', 'blue', 'blue'], k=5)\n     |\n     |      To choose a sample from a range of integers, use range() for the\n     |      population argument.  This is especially fast and space efficient\n     |      for sampling from a large population:\n     |\n     |          sample(range(10000000), 60)\n     |\n     |  seed(self, a=None, version=2)\n     |      Initialize internal state from a seed.\n     |\n     |      The only supported seed types are None, int, float,\n     |      str, bytes, and bytearray.\n     |\n     |      None or no argument seeds from current time or from an operating\n     |      system specific randomness source if available.\n     |\n     |      If *a* is an int, all bits are used.\n     |\n     |      For version 2 (the default), all of the bits are used if *a* is a str,\n     |      bytes, or bytearray.  For version 1 (provided for reproducing random\n     |      sequences from older versions of Python), the algorithm for str and\n     |      bytes generates a narrower range of seeds.\n     |\n     |  setstate(self, state)\n     |      Restore internal state from object returned by getstate().\n     |\n     |  shuffle(self, x)\n     |      Shuffle list x in place, and return None.\n     |\n     |  triangular(self, low=0.0, high=1.0, mode=None)\n     |      Triangular distribution.\n     |\n     |      Continuous distribution bounded by given lower and upper limits,\n     |      and having a given mode value in-between.\n     |\n     |      http://en.wikipedia.org/wiki/Triangular_distribution\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = (low + high + mode) / 3\n     |          Var[X] = (low**2 + high**2 + mode**2 - low*high - low*mode - high*mode) / 18\n     |\n     |  uniform(self, a, b)\n     |      Get a random number in the range [a, b) or [a, b] depending on rounding.\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = (a + b) / 2\n     |          Var[X] = (b - a) ** 2 / 12\n     |\n     |  vonmisesvariate(self, mu, kappa)\n     |      Circular data distribution.\n     |\n     |      mu is the mean angle, expressed in radians between 0 and 2*pi, and\n     |      kappa is the concentration parameter, which must be greater than or\n     |      equal to zero.  If kappa is equal to zero, this distribution reduces\n     |      to a uniform random angle over the range 0 to 2*pi.\n     |\n     |  weibullvariate(self, alpha, beta)\n     |      Weibull distribution.\n     |\n     |      alpha is the scale parameter and beta is the shape parameter.\n     |\n     |  ----------------------------------------------------------------------\n     |  Class methods defined here:\n     |\n     |  __init_subclass__(**kwargs)\n     |      Control how subclasses generate random integers.\n     |\n     |      The algorithm a subclass can use depends on the random() and/or\n     |      getrandbits() implementation available to it and determines\n     |      whether it can generate random integers from arbitrarily large\n     |      ranges.\n     |\n     |  ----------------------------------------------------------------------\n     |  Data descriptors defined here:\n     |\n     |  __dict__\n     |      dictionary for instance variables\n     |\n     |  __weakref__\n     |      list of weak references to the object\n     |\n     |  ----------------------------------------------------------------------\n     |  Data and other attributes defined here:\n     |\n     |  VERSION = 3\n     |\n     |  ----------------------------------------------------------------------\n     |  Methods inherited from _random.Random:\n     |\n     |  getrandbits(self, k, /)\n     |      getrandbits(k) -&gt; x.  Generates an int with k random bits.\n     |\n     |  random(self, /)\n     |      random() -&gt; x in the interval [0, 1).\n     |\n     |  ----------------------------------------------------------------------\n     |  Static methods inherited from _random.Random:\n     |\n     |  __new__(*args, **kwargs) class method of _random.Random\n     |      Create and return a new object.  See help(type) for accurate signature.\n\n    class SystemRandom(Random)\n     |  SystemRandom(x=None)\n     |\n     |  Alternate random number generator using sources provided\n     |  by the operating system (such as /dev/urandom on Unix or\n     |  CryptGenRandom on Windows).\n     |\n     |   Not available on all systems (see os.urandom() for details).\n     |\n     |  Method resolution order:\n     |      SystemRandom\n     |      Random\n     |      _random.Random\n     |      builtins.object\n     |\n     |  Methods defined here:\n     |\n     |  getrandbits(self, k)\n     |      getrandbits(k) -&gt; x.  Generates an int with k random bits.\n     |\n     |  getstate = _notimplemented(self, *args, **kwds)\n     |\n     |  randbytes(self, n)\n     |      Generate n random bytes.\n     |\n     |  random(self)\n     |      Get the next random number in the range 0.0 &lt;= X &lt; 1.0.\n     |\n     |  seed(self, *args, **kwds)\n     |      Stub method.  Not used for a system random number generator.\n     |\n     |  setstate = _notimplemented(self, *args, **kwds)\n     |\n     |  ----------------------------------------------------------------------\n     |  Methods inherited from Random:\n     |\n     |  __getstate__(self)\n     |      Helper for pickle.\n     |\n     |  __init__(self, x=None)\n     |      Initialize an instance.\n     |\n     |      Optional argument x controls seeding, as for Random.seed().\n     |\n     |  __reduce__(self)\n     |      Helper for pickle.\n     |\n     |  __setstate__(self, state)\n     |\n     |  betavariate(self, alpha, beta)\n     |      Beta distribution.\n     |\n     |      Conditions on the parameters are alpha &gt; 0 and beta &gt; 0.\n     |      Returned values range between 0 and 1.\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = alpha / (alpha + beta)\n     |          Var[X] = alpha * beta / ((alpha + beta)**2 * (alpha + beta + 1))\n     |\n     |  binomialvariate(self, n=1, p=0.5)\n     |      Binomial random variable.\n     |\n     |      Gives the number of successes for *n* independent trials\n     |      with the probability of success in each trial being *p*:\n     |\n     |          sum(random() &lt; p for i in range(n))\n     |\n     |      Returns an integer in the range:   0 &lt;= X &lt;= n\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = n * p\n     |          Var[x] = n * p * (1 - p)\n     |\n     |  choice(self, seq)\n     |      Choose a random element from a non-empty sequence.\n     |\n     |  choices(self, population, weights=None, *, cum_weights=None, k=1)\n     |      Return a k sized list of population elements chosen with replacement.\n     |\n     |      If the relative weights or cumulative weights are not specified,\n     |      the selections are made with equal probability.\n     |\n     |  expovariate(self, lambd=1.0)\n     |      Exponential distribution.\n     |\n     |      lambd is 1.0 divided by the desired mean.  It should be\n     |      nonzero.  (The parameter would be called \"lambda\", but that is\n     |      a reserved word in Python.)  Returned values range from 0 to\n     |      positive infinity if lambd is positive, and from negative\n     |      infinity to 0 if lambd is negative.\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = 1 / lambd\n     |          Var[X] = 1 / lambd ** 2\n     |\n     |  gammavariate(self, alpha, beta)\n     |      Gamma distribution.  Not the gamma function!\n     |\n     |      Conditions on the parameters are alpha &gt; 0 and beta &gt; 0.\n     |\n     |      The probability distribution function is:\n     |\n     |                  x ** (alpha - 1) * math.exp(-x / beta)\n     |        pdf(x) =  --------------------------------------\n     |                    math.gamma(alpha) * beta ** alpha\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = alpha * beta\n     |          Var[X] = alpha * beta ** 2\n     |\n     |  gauss(self, mu=0.0, sigma=1.0)\n     |      Gaussian distribution.\n     |\n     |      mu is the mean, and sigma is the standard deviation.  This is\n     |      slightly faster than the normalvariate() function.\n     |\n     |      Not thread-safe without a lock around calls.\n     |\n     |  lognormvariate(self, mu, sigma)\n     |      Log normal distribution.\n     |\n     |      If you take the natural logarithm of this distribution, you'll get a\n     |      normal distribution with mean mu and standard deviation sigma.\n     |      mu can have any value, and sigma must be greater than zero.\n     |\n     |  normalvariate(self, mu=0.0, sigma=1.0)\n     |      Normal distribution.\n     |\n     |      mu is the mean, and sigma is the standard deviation.\n     |\n     |  paretovariate(self, alpha)\n     |      Pareto distribution.  alpha is the shape parameter.\n     |\n     |  randint(self, a, b)\n     |      Return random integer in range [a, b], including both end points.\n     |\n     |  randrange(self, start, stop=None, step=1)\n     |      Choose a random item from range(stop) or range(start, stop[, step]).\n     |\n     |      Roughly equivalent to ``choice(range(start, stop, step))`` but\n     |      supports arbitrarily large ranges and is optimized for common cases.\n     |\n     |  sample(self, population, k, *, counts=None)\n     |      Chooses k unique random elements from a population sequence.\n     |\n     |      Returns a new list containing elements from the population while\n     |      leaving the original population unchanged.  The resulting list is\n     |      in selection order so that all sub-slices will also be valid random\n     |      samples.  This allows raffle winners (the sample) to be partitioned\n     |      into grand prize and second place winners (the subslices).\n     |\n     |      Members of the population need not be hashable or unique.  If the\n     |      population contains repeats, then each occurrence is a possible\n     |      selection in the sample.\n     |\n     |      Repeated elements can be specified one at a time or with the optional\n     |      counts parameter.  For example:\n     |\n     |          sample(['red', 'blue'], counts=[4, 2], k=5)\n     |\n     |      is equivalent to:\n     |\n     |          sample(['red', 'red', 'red', 'red', 'blue', 'blue'], k=5)\n     |\n     |      To choose a sample from a range of integers, use range() for the\n     |      population argument.  This is especially fast and space efficient\n     |      for sampling from a large population:\n     |\n     |          sample(range(10000000), 60)\n     |\n     |  shuffle(self, x)\n     |      Shuffle list x in place, and return None.\n     |\n     |  triangular(self, low=0.0, high=1.0, mode=None)\n     |      Triangular distribution.\n     |\n     |      Continuous distribution bounded by given lower and upper limits,\n     |      and having a given mode value in-between.\n     |\n     |      http://en.wikipedia.org/wiki/Triangular_distribution\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = (low + high + mode) / 3\n     |          Var[X] = (low**2 + high**2 + mode**2 - low*high - low*mode - high*mode) / 18\n     |\n     |  uniform(self, a, b)\n     |      Get a random number in the range [a, b) or [a, b] depending on rounding.\n     |\n     |      The mean (expected value) and variance of the random variable are:\n     |\n     |          E[X] = (a + b) / 2\n     |          Var[X] = (b - a) ** 2 / 12\n     |\n     |  vonmisesvariate(self, mu, kappa)\n     |      Circular data distribution.\n     |\n     |      mu is the mean angle, expressed in radians between 0 and 2*pi, and\n     |      kappa is the concentration parameter, which must be greater than or\n     |      equal to zero.  If kappa is equal to zero, this distribution reduces\n     |      to a uniform random angle over the range 0 to 2*pi.\n     |\n     |  weibullvariate(self, alpha, beta)\n     |      Weibull distribution.\n     |\n     |      alpha is the scale parameter and beta is the shape parameter.\n     |\n     |  ----------------------------------------------------------------------\n     |  Class methods inherited from Random:\n     |\n     |  __init_subclass__(**kwargs)\n     |      Control how subclasses generate random integers.\n     |\n     |      The algorithm a subclass can use depends on the random() and/or\n     |      getrandbits() implementation available to it and determines\n     |      whether it can generate random integers from arbitrarily large\n     |      ranges.\n     |\n     |  ----------------------------------------------------------------------\n     |  Data descriptors inherited from Random:\n     |\n     |  __dict__\n     |      dictionary for instance variables\n     |\n     |  __weakref__\n     |      list of weak references to the object\n     |\n     |  ----------------------------------------------------------------------\n     |  Data and other attributes inherited from Random:\n     |\n     |  VERSION = 3\n     |\n     |  ----------------------------------------------------------------------\n     |  Static methods inherited from _random.Random:\n     |\n     |  __new__(*args, **kwargs) class method of _random.Random\n     |      Create and return a new object.  See help(type) for accurate signature.\n\nFUNCTIONS\n    betavariate(alpha, beta) method of Random instance\n        Beta distribution.\n\n        Conditions on the parameters are alpha &gt; 0 and beta &gt; 0.\n        Returned values range between 0 and 1.\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = alpha / (alpha + beta)\n            Var[X] = alpha * beta / ((alpha + beta)**2 * (alpha + beta + 1))\n\n    binomialvariate(n=1, p=0.5) method of Random instance\n        Binomial random variable.\n\n        Gives the number of successes for *n* independent trials\n        with the probability of success in each trial being *p*:\n\n            sum(random() &lt; p for i in range(n))\n\n        Returns an integer in the range:   0 &lt;= X &lt;= n\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = n * p\n            Var[x] = n * p * (1 - p)\n\n    choice(seq) method of Random instance\n        Choose a random element from a non-empty sequence.\n\n    choices(population, weights=None, *, cum_weights=None, k=1) method of Random instance\n        Return a k sized list of population elements chosen with replacement.\n\n        If the relative weights or cumulative weights are not specified,\n        the selections are made with equal probability.\n\n    expovariate(lambd=1.0) method of Random instance\n        Exponential distribution.\n\n        lambd is 1.0 divided by the desired mean.  It should be\n        nonzero.  (The parameter would be called \"lambda\", but that is\n        a reserved word in Python.)  Returned values range from 0 to\n        positive infinity if lambd is positive, and from negative\n        infinity to 0 if lambd is negative.\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = 1 / lambd\n            Var[X] = 1 / lambd ** 2\n\n    gammavariate(alpha, beta) method of Random instance\n        Gamma distribution.  Not the gamma function!\n\n        Conditions on the parameters are alpha &gt; 0 and beta &gt; 0.\n\n        The probability distribution function is:\n\n                    x ** (alpha - 1) * math.exp(-x / beta)\n          pdf(x) =  --------------------------------------\n                      math.gamma(alpha) * beta ** alpha\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = alpha * beta\n            Var[X] = alpha * beta ** 2\n\n    gauss(mu=0.0, sigma=1.0) method of Random instance\n        Gaussian distribution.\n\n        mu is the mean, and sigma is the standard deviation.  This is\n        slightly faster than the normalvariate() function.\n\n        Not thread-safe without a lock around calls.\n\n    getrandbits(k, /) method of Random instance\n        getrandbits(k) -&gt; x.  Generates an int with k random bits.\n\n    getstate() method of Random instance\n        Return internal state; can be passed to setstate() later.\n\n    lognormvariate(mu, sigma) method of Random instance\n        Log normal distribution.\n\n        If you take the natural logarithm of this distribution, you'll get a\n        normal distribution with mean mu and standard deviation sigma.\n        mu can have any value, and sigma must be greater than zero.\n\n    normalvariate(mu=0.0, sigma=1.0) method of Random instance\n        Normal distribution.\n\n        mu is the mean, and sigma is the standard deviation.\n\n    paretovariate(alpha) method of Random instance\n        Pareto distribution.  alpha is the shape parameter.\n\n    randbytes(n) method of Random instance\n        Generate n random bytes.\n\n    randint(a, b) method of Random instance\n        Return random integer in range [a, b], including both end points.\n\n    random() method of Random instance\n        random() -&gt; x in the interval [0, 1).\n\n    randrange(start, stop=None, step=1) method of Random instance\n        Choose a random item from range(stop) or range(start, stop[, step]).\n\n        Roughly equivalent to ``choice(range(start, stop, step))`` but\n        supports arbitrarily large ranges and is optimized for common cases.\n\n    sample(population, k, *, counts=None) method of Random instance\n        Chooses k unique random elements from a population sequence.\n\n        Returns a new list containing elements from the population while\n        leaving the original population unchanged.  The resulting list is\n        in selection order so that all sub-slices will also be valid random\n        samples.  This allows raffle winners (the sample) to be partitioned\n        into grand prize and second place winners (the subslices).\n\n        Members of the population need not be hashable or unique.  If the\n        population contains repeats, then each occurrence is a possible\n        selection in the sample.\n\n        Repeated elements can be specified one at a time or with the optional\n        counts parameter.  For example:\n\n            sample(['red', 'blue'], counts=[4, 2], k=5)\n\n        is equivalent to:\n\n            sample(['red', 'red', 'red', 'red', 'blue', 'blue'], k=5)\n\n        To choose a sample from a range of integers, use range() for the\n        population argument.  This is especially fast and space efficient\n        for sampling from a large population:\n\n            sample(range(10000000), 60)\n\n    seed(a=None, version=2) method of Random instance\n        Initialize internal state from a seed.\n\n        The only supported seed types are None, int, float,\n        str, bytes, and bytearray.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If *a* is an int, all bits are used.\n\n        For version 2 (the default), all of the bits are used if *a* is a str,\n        bytes, or bytearray.  For version 1 (provided for reproducing random\n        sequences from older versions of Python), the algorithm for str and\n        bytes generates a narrower range of seeds.\n\n    setstate(state) method of Random instance\n        Restore internal state from object returned by getstate().\n\n    shuffle(x) method of Random instance\n        Shuffle list x in place, and return None.\n\n    triangular(low=0.0, high=1.0, mode=None) method of Random instance\n        Triangular distribution.\n\n        Continuous distribution bounded by given lower and upper limits,\n        and having a given mode value in-between.\n\n        http://en.wikipedia.org/wiki/Triangular_distribution\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = (low + high + mode) / 3\n            Var[X] = (low**2 + high**2 + mode**2 - low*high - low*mode - high*mode) / 18\n\n    uniform(a, b) method of Random instance\n        Get a random number in the range [a, b) or [a, b] depending on rounding.\n\n        The mean (expected value) and variance of the random variable are:\n\n            E[X] = (a + b) / 2\n            Var[X] = (b - a) ** 2 / 12\n\n    vonmisesvariate(mu, kappa) method of Random instance\n        Circular data distribution.\n\n        mu is the mean angle, expressed in radians between 0 and 2*pi, and\n        kappa is the concentration parameter, which must be greater than or\n        equal to zero.  If kappa is equal to zero, this distribution reduces\n        to a uniform random angle over the range 0 to 2*pi.\n\n    weibullvariate(alpha, beta) method of Random instance\n        Weibull distribution.\n\n        alpha is the scale parameter and beta is the shape parameter.\n\nDATA\n    __all__ = ['Random', 'SystemRandom', 'betavariate', 'binomialvariate',...\n\nFILE\n    /usr/lib/python3.12/random.py\n\n\n\n\n\nWhen we do random number generation, we are really getting pseudo random values\nThe values are actually generated from some algorithm\nWe can se the starting point of that algorithm by setting a seed\nThis allows us to ensure reproducibility of our process!\n\nImportant: A seed sets a starting point for the ‘random’ number generator. This allows you to get the same ‘random’ numbers the next time you run the code.\n\n\n\nrandom.seed(101)\nprint(random.random())\nprint(random.random())\nprint(random.random())\n\n0.5811521325045647\n0.1947544955341367\n0.9652511070611112\n\n\n\n#notice we can get the same 'random value if we set the seed back to the same starting point\nrandom.seed(101)\nprint(random.random())\n\n0.5811521325045647\n\n\n\nWe can also randomly obtain integers from a particular range of values\nWe use the range() function to return an iterator that is able to produce values\nHere we obtain random integers between 10 and 25 and use random.sample() to obtain four values from that range\n\n\nrandom.sample(range(10, 25), 4)\n\n[13, 18, 15, 17]",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Modules"
    ]
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#importing-as",
    "href": "01_Programming_in_python/03-Modules.html#importing-as",
    "title": "Modules",
    "section": "Importing as",
    "text": "Importing as\n\nOften we want to use a shorter module name for brevity\nWe can do so with as when we use import\n\n\nimport random as ran\nran.seed(101)\nprint(ran.random())\nprint(ran.random())\nprint(ran.random())\n\n0.5811521325045647\n0.1947544955341367\n0.9652511070611112\n\n\n\nObtain random integers between 11 and 26\n\n\nran.sample(range(11, 26), 4)\n\n[25, 16, 18, 11]\n\n\nWe’ll see that most of the commonly used modules have common aliases. For instance, numpy as np, pandas as pd, and pyspark as ps.",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Modules"
    ]
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#selective-import",
    "href": "01_Programming_in_python/03-Modules.html#selective-import",
    "title": "Modules",
    "section": "Selective Import",
    "text": "Selective Import\n\nWe can also selectively import functions and variables from a module directly into our main namespace\nThis allows us to call the functions without the module prefix\nUse from module import object1, object2 to do this\n\n\nfrom math import sqrt, pi\nprint(sqrt(9))\npi\n\n3.0\n\n\n3.141592653589793\n\n\n\nAlternatively, if you intend to use a function often you can assign it to a local name\n\n\nsample = random.random\nsample()\n\n0.6634706445300605\n\n\n\nOr import everything from a module into the current namespace. Be careful with this as you can overwrite things you rely on!\n\n\nfrom math import *",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Modules"
    ]
  },
  {
    "objectID": "01_Programming_in_python/03-Modules.html#installing-modules",
    "href": "01_Programming_in_python/03-Modules.html#installing-modules",
    "title": "Modules",
    "section": "Installing Modules",
    "text": "Installing Modules\n\npip is a package manager for python\nUsed through the command line usually\nWe’ll use it through a code cell with ! first\n\nCan see what modules you have\n\npip list\n\nUse pip install module_name to install new modules\n\npip install scipy\n\nColab has most everything we need for now but we’ll need to do some installing later in the course!\n\n! pip list\n\nPackage                                  Version\n---------------------------------------- --------------------\nabsl-py                                  1.4.0\naccelerate                               1.12.0\naccess                                   1.1.9\naffine                                   2.4.0\naiofiles                                 24.1.0\naiohappyeyeballs                         2.6.1\naiohttp                                  3.13.2\naiosignal                                1.4.0\naiosqlite                                0.21.0\nalabaster                                1.0.0\nalbucore                                 0.0.24\nalbumentations                           2.0.8\nale-py                                   0.11.2\nalembic                                  1.17.2\naltair                                   5.5.0\nannotated-types                          0.7.0\nantlr4-python3-runtime                   4.9.3\nanyio                                    4.11.0\nanywidget                                0.9.21\nargon2-cffi                              25.1.0\nargon2-cffi-bindings                     25.1.0\narray_record                             0.8.3\narrow                                    1.4.0\narviz                                    0.22.0\nastropy                                  7.1.1\nastropy-iers-data                        0.2025.11.24.0.39.11\nastunparse                               1.6.3\natpublic                                 5.1\nattrs                                    25.4.0\naudioread                                3.1.0\nAuthlib                                  1.6.5\nautograd                                 1.8.0\nbabel                                    2.17.0\nbackcall                                 0.2.0\nbeartype                                 0.22.6\nbeautifulsoup4                           4.13.5\nbetterproto                              2.0.0b6\nbigframes                                2.29.1\nbigquery-magics                          0.10.3\nbleach                                   6.3.0\nblinker                                  1.9.0\nblis                                     1.3.3\nblobfile                                 3.1.0\nblosc2                                   3.11.1\nbokeh                                    3.7.3\nBottleneck                               1.4.2\nbqplot                                   0.12.45\nbranca                                   0.8.2\nbrotli                                   1.2.0\nCacheControl                             0.14.4\ncachetools                               6.2.2\ncatalogue                                2.0.10\ncertifi                                  2025.11.12\ncffi                                     2.0.0\nchardet                                  5.2.0\ncharset-normalizer                       3.4.4\nchex                                     0.1.90\nclarabel                                 0.11.1\nclick                                    8.3.1\nclick-plugins                            1.1.1.2\ncligj                                    0.7.2\ncloudpathlib                             0.23.0\ncloudpickle                              3.1.2\ncmake                                    3.31.10\ncmdstanpy                                1.3.0\ncolorcet                                 3.1.0\ncolorlover                               0.3.0\ncolour                                   0.1.5\ncommunity                                1.0.0b1\nconfection                               0.1.5\ncons                                     0.4.7\ncontourpy                                1.3.3\ncramjam                                  2.11.0\ncryptography                             43.0.3\ncuda-bindings                            12.9.4\ncuda-core                                0.3.2\ncuda-pathfinder                          1.3.2\ncuda-python                              12.9.4\ncuda-toolkit                             12.9.1\ncudf-cu12                                25.10.0\ncudf-polars-cu12                         25.10.0\ncufflinks                                0.17.3\ncuml-cu12                                25.10.0\ncupy-cuda12x                             13.6.0\ncurl_cffi                                0.13.0\ncvxopt                                   1.3.2\ncvxpy                                    1.6.7\ncycler                                   0.12.1\ncyipopt                                  1.5.0\ncymem                                    2.0.13\nCython                                   3.0.12\ndask                                     2025.9.1\ndask-cuda                                25.10.0\ndask-cudf-cu12                           25.10.0\ndataproc-spark-connect                   0.8.3\ndatasets                                 4.0.0\ndb-dtypes                                1.4.4\ndbus-python                              1.2.18\ndebugpy                                  1.8.15\ndecorator                                4.4.2\ndefusedxml                               0.7.1\ndeprecation                              2.1.0\ndiffusers                                0.35.2\ndill                                     0.3.8\ndistributed                              2025.9.1\ndistributed-ucxx-cu12                    0.46.0\ndistro                                   1.9.0\ndlib                                     19.24.6\ndm-tree                                  0.1.9\ndocstring_parser                         0.17.0\ndocutils                                 0.21.2\ndopamine_rl                              4.1.2\nduckdb                                   1.3.2\nearthengine-api                          1.5.24\neasydict                                 1.13\neditdistance                             0.8.1\neerepr                                   0.1.2\neinops                                   0.8.1\nen_core_web_sm                           3.8.0\nentrypoints                              0.4\nesda                                     2.8.0\net_xmlfile                               2.0.0\netils                                    1.13.0\netuples                                  0.3.10\nFarama-Notifications                     0.0.4\nfastai                                   2.8.5\nfastapi                                  0.118.3\nfastcore                                 1.8.16\nfastdownload                             0.0.7\nfastjsonschema                           2.21.2\nfastprogress                             1.0.3\nfastrlock                                0.8.3\nfasttransform                            0.0.2\nffmpy                                    1.0.0\nfilelock                                 3.20.0\nfiona                                    1.10.1\nfirebase-admin                           6.9.0\nFlask                                    3.1.2\nflatbuffers                              25.9.23\nflax                                     0.10.7\nfolium                                   0.20.0\nfonttools                                4.60.1\nfqdn                                     1.5.1\nfrozendict                               2.4.7\nfrozenlist                               1.8.0\nfsspec                                   2025.3.0\nfuture                                   1.0.0\ngast                                     0.6.0\ngcsfs                                    2025.3.0\nGDAL                                     3.8.4\ngdown                                    5.2.0\ngeemap                                   0.35.3\ngeocoder                                 1.38.1\ngeographiclib                            2.1\ngeopandas                                1.1.1\ngeopy                                    2.4.1\ngiddy                                    2.3.6\ngin-config                               0.5.0\ngitdb                                    4.0.12\nGitPython                                3.1.45\nglob2                                    0.7\ngoogle                                   3.0.0\ngoogle-adk                               1.19.0\ngoogle-ai-generativelanguage             0.6.15\ngoogle-api-core                          2.28.1\ngoogle-api-python-client                 2.187.0\ngoogle-auth                              2.43.0\ngoogle-auth-httplib2                     0.2.1\ngoogle-auth-oauthlib                     1.2.2\ngoogle-cloud-aiplatform                  1.128.0\ngoogle-cloud-appengine-logging           1.7.0\ngoogle-cloud-audit-log                   0.4.0\ngoogle-cloud-bigquery                    3.38.0\ngoogle-cloud-bigquery-connection         1.19.0\ngoogle-cloud-bigquery-storage            2.34.0\ngoogle-cloud-bigtable                    2.34.0\ngoogle-cloud-core                        2.5.0\ngoogle-cloud-dataproc                    5.23.0\ngoogle-cloud-datastore                   2.21.0\ngoogle-cloud-discoveryengine             0.13.12\ngoogle-cloud-firestore                   2.21.0\ngoogle-cloud-functions                   1.21.0\ngoogle-cloud-language                    2.18.0\ngoogle-cloud-logging                     3.12.1\ngoogle-cloud-monitoring                  2.28.0\ngoogle-cloud-resource-manager            1.15.0\ngoogle-cloud-secret-manager              2.25.0\ngoogle-cloud-spanner                     3.59.0\ngoogle-cloud-speech                      2.34.0\ngoogle-cloud-storage                     3.6.0\ngoogle-cloud-trace                       1.17.0\ngoogle-cloud-translate                   3.23.0\ngoogle-colab                             1.0.0\ngoogle-crc32c                            1.7.1\ngoogle-genai                             1.52.0\ngoogle-generativeai                      0.8.5\ngoogle-pasta                             0.2.0\ngoogle-resumable-media                   2.8.0\ngoogleapis-common-protos                 1.72.0\ngoogledrivedownloader                    1.1.0\ngradio                                   5.50.0\ngradio_client                            1.14.0\ngraphviz                                 0.21\ngreenlet                                 3.2.4\ngroovy                                   0.1.2\ngrpc-google-iam-v1                       0.14.3\ngrpc-interceptor                         0.15.4\ngrpcio                                   1.76.0\ngrpcio-status                            1.71.2\ngrpclib                                  0.4.8\ngspread                                  6.2.1\ngspread-dataframe                        4.0.0\ngym                                      0.25.2\ngym-notices                              0.1.0\ngymnasium                                1.2.2\nh11                                      0.16.0\nh2                                       4.3.0\nh5netcdf                                 1.7.3\nh5py                                     3.15.1\nhdbscan                                  0.8.40\nhf_transfer                              0.1.9\nhf-xet                                   1.2.0\nhighspy                                  1.12.0\nholidays                                 0.85\nholoviews                                1.22.0\nhpack                                    4.1.0\nhtml5lib                                 1.1\nhttpcore                                 1.0.9\nhttpimport                               1.4.1\nhttplib2                                 0.31.0\nhttpx                                    0.28.1\nhttpx-sse                                0.4.3\nhuggingface-hub                          0.36.0\nhumanize                                 4.14.0\nhyperframe                               6.1.0\nhyperopt                                 0.2.7\nibis-framework                           9.5.0\nidna                                     3.11\nImageIO                                  2.37.2\nimageio-ffmpeg                           0.6.0\nimagesize                                1.4.1\nimbalanced-learn                         0.14.0\nimmutabledict                            4.2.2\nimportlib_metadata                       8.7.0\nimportlib_resources                      6.5.2\nimutils                                  0.5.4\ninequality                               1.1.2\ninflect                                  7.5.0\niniconfig                                2.3.0\nintel-cmplr-lib-ur                       2025.3.1\nintel-openmp                             2025.3.1\nipyevents                                2.0.4\nipyfilechooser                           0.6.0\nipykernel                                6.17.1\nipyleaflet                               0.20.0\nipyparallel                              8.8.0\nipython                                  7.34.0\nipython-genutils                         0.2.0\nipython-sql                              0.5.0\nipytree                                  0.2.2\nipywidgets                               7.7.1\nisoduration                              20.11.0\nitsdangerous                             2.2.0\njaraco.classes                           3.4.0\njaraco.context                           6.0.1\njaraco.functools                         4.3.0\njax                                      0.7.2\njax-cuda12-pjrt                          0.7.2\njax-cuda12-plugin                        0.7.2\njaxlib                                   0.7.2\njeepney                                  0.9.0\njieba                                    0.42.1\nJinja2                                   3.1.6\njiter                                    0.12.0\njoblib                                   1.5.2\njsonpatch                                1.33\njsonpickle                               4.1.1\njsonpointer                              3.0.0\njsonschema                               4.25.1\njsonschema-specifications                2025.9.1\njupyter_client                           7.4.9\njupyter-console                          6.6.3\njupyter_core                             5.9.1\njupyter-events                           0.12.0\njupyter_kernel_gateway                   2.5.2\njupyter-leaflet                          0.20.0\njupyter_server                           2.14.0\njupyter_server_terminals                 0.5.3\njupyterlab_pygments                      0.3.0\njupyterlab_widgets                       3.0.16\njupytext                                 1.18.1\nkaggle                                   1.7.4.5\nkagglehub                                0.3.13\nkeras                                    3.10.0\nkeras-hub                                0.21.1\nkeras-nlp                                0.21.1\nkeyring                                  25.7.0\nkeyrings.google-artifactregistry-auth    1.1.2\nkiwisolver                               1.4.9\nlangchain                                1.1.0\nlangchain-core                           1.1.0\nlanggraph                                1.0.3\nlanggraph-checkpoint                     3.0.1\nlanggraph-prebuilt                       1.0.5\nlanggraph-sdk                            0.2.10\nlangsmith                                0.4.47\nlark                                     1.3.1\nlaunchpadlib                             1.10.16\nlazr.restfulclient                       0.14.4\nlazr.uri                                 1.0.6\nlazy_loader                              0.4\nlibclang                                 18.1.1\nlibcudf-cu12                             25.10.0\nlibcugraph-cu12                          25.10.1\nlibcuml-cu12                             25.10.0\nlibkvikio-cu12                           25.10.0\nlibpysal                                 4.13.0\nlibraft-cu12                             25.10.0\nlibrmm-cu12                              25.10.0\nlibrosa                                  0.11.0\nlibucx-cu12                              1.19.0\nlibucxx-cu12                             0.46.0\nlightgbm                                 4.6.0\nlinkify-it-py                            2.0.3\nllvmlite                                 0.43.0\nlocket                                   1.0.0\nlogical-unification                      0.4.7\nlxml                                     6.0.2\nMako                                     1.3.10\nmapclassify                              2.10.0\nMarkdown                                 3.10\nmarkdown-it-py                           4.0.0\nMarkupSafe                               3.0.3\nmatplotlib                               3.10.0\nmatplotlib-inline                        0.2.1\nmatplotlib-venn                          1.1.2\nmcp                                      1.22.0\nmdit-py-plugins                          0.5.0\nmdurl                                    0.1.2\nmgwr                                     2.2.1\nminiKanren                               1.0.5\nmissingno                                0.5.2\nmistune                                  3.1.4\nmizani                                   0.13.5\nmkl                                      2025.3.0\nml_dtypes                                0.5.4\nmlxtend                                  0.23.4\nmomepy                                   0.10.0\nmore-itertools                           10.8.0\nmoviepy                                  1.0.3\nmpmath                                   1.3.0\nmsgpack                                  1.1.2\nmultidict                                6.7.0\nmultipledispatch                         1.0.0\nmultiprocess                             0.70.16\nmultitasking                             0.0.12\nmurmurhash                               1.0.15\nmusic21                                  9.9.1\nnamex                                    0.1.0\nnarwhals                                 2.12.0\nnatsort                                  8.4.0\nnbclassic                                1.3.3\nnbclient                                 0.10.2\nnbconvert                                7.16.6\nnbformat                                 5.10.4\nndindex                                  1.10.1\nnest-asyncio                             1.6.0\nnetworkx                                 3.6\nnibabel                                  5.3.2\nnltk                                     3.9.1\nnotebook                                 6.5.7\nnotebook_shim                            0.2.4\nnumba                                    0.60.0\nnumba-cuda                               0.19.1\nnumexpr                                  2.14.1\nnumpy                                    2.0.2\nnvidia-cublas-cu12                       12.6.4.1\nnvidia-cuda-cccl-cu12                    12.9.27\nnvidia-cuda-cupti-cu12                   12.6.80\nnvidia-cuda-nvcc-cu12                    12.5.82\nnvidia-cuda-nvrtc-cu12                   12.6.77\nnvidia-cuda-runtime-cu12                 12.6.77\nnvidia-cudnn-cu12                        9.10.2.21\nnvidia-cufft-cu12                        11.3.0.4\nnvidia-cufile-cu12                       1.11.1.6\nnvidia-curand-cu12                       10.3.7.77\nnvidia-cusolver-cu12                     11.7.1.2\nnvidia-cusparse-cu12                     12.5.4.2\nnvidia-cusparselt-cu12                   0.7.1\nnvidia-ml-py                             13.580.82\nnvidia-nccl-cu12                         2.27.5\nnvidia-nvjitlink-cu12                    12.6.85\nnvidia-nvshmem-cu12                      3.3.20\nnvidia-nvtx-cu12                         12.6.77\nnvtx                                     0.2.13\nnx-cugraph-cu12                          25.10.0\noauth2client                             4.1.3\noauthlib                                 3.3.1\nomegaconf                                2.3.0\nonemkl-license                           2025.3.0\nopenai                                   2.8.1\nopencv-contrib-python                    4.12.0.88\nopencv-python                            4.12.0.88\nopencv-python-headless                   4.12.0.88\nopenpyxl                                 3.1.5\nopentelemetry-api                        1.37.0\nopentelemetry-exporter-gcp-logging       1.11.0a0\nopentelemetry-exporter-gcp-monitoring    1.11.0a0\nopentelemetry-exporter-gcp-trace         1.11.0\nopentelemetry-exporter-otlp-proto-common 1.37.0\nopentelemetry-exporter-otlp-proto-http   1.37.0\nopentelemetry-proto                      1.37.0\nopentelemetry-resourcedetector-gcp       1.11.0a0\nopentelemetry-sdk                        1.37.0\nopentelemetry-semantic-conventions       0.58b0\nopt_einsum                               3.4.0\noptax                                    0.2.6\noptree                                   0.18.0\norbax-checkpoint                         0.11.28\norjson                                   3.11.4\normsgpack                                1.12.0\nosqp                                     1.0.5\noverrides                                7.7.0\npackaging                                25.0\npandas                                   2.2.2\npandas-datareader                        0.10.0\npandas-gbq                               0.30.0\npandas-stubs                             2.2.2.240909\npandocfilters                            1.5.1\npanel                                    1.8.3\nparam                                    2.3.0\nparso                                    0.8.5\nparsy                                    2.2\npartd                                    1.4.2\npatsy                                    1.0.2\npeewee                                   3.18.3\npeft                                     0.18.0\npexpect                                  4.9.0\npickleshare                              0.7.5\npillow                                   11.3.0\npip                                      24.1.2\nplatformdirs                             4.5.0\nplotly                                   5.24.1\nplotnine                                 0.14.5\npluggy                                   1.6.0\nplum-dispatch                            2.6.0\nply                                      3.11\npointpats                                2.5.2\npolars                                   1.31.0\npooch                                    1.8.2\nportpicker                               1.5.2\npreshed                                  3.0.12\nprettytable                              3.17.0\nproglog                                  0.1.12\nprogressbar2                             4.5.0\nprometheus_client                        0.23.1\npromise                                  2.3\nprompt_toolkit                           3.0.52\npropcache                                0.4.1\nprophet                                  1.2.1\nproto-plus                               1.26.1\nprotobuf                                 5.29.5\npsutil                                   5.9.5\npsycopg2                                 2.9.11\npsygnal                                  0.15.0\nptyprocess                               0.7.0\nPuLP                                     3.3.0\npy-cpuinfo                               9.0.0\npy4j                                     0.10.9.7\npyarrow                                  18.1.0\npyasn1                                   0.6.1\npyasn1_modules                           0.4.2\npycairo                                  1.29.0\npycocotools                              2.0.10\npycparser                                2.23\npycryptodomex                            3.23.0\npydantic                                 2.12.3\npydantic_core                            2.41.4\npydantic-settings                        2.12.0\npydata-google-auth                       1.9.1\npydot                                    4.0.1\npydotplus                                2.0.2\nPyDrive2                                 1.21.3\npydub                                    0.25.1\npyerfa                                   2.0.1.5\npygame                                   2.6.1\npygit2                                   1.19.0\nPygments                                 2.19.2\nPyGObject                                3.48.2\nPyJWT                                    2.10.1\npylibcudf-cu12                           25.10.0\npylibcugraph-cu12                        25.10.1\npylibraft-cu12                           25.10.0\npymc                                     5.26.1\npynndescent                              0.5.13\npyogrio                                  0.11.1\npyomo                                    6.9.5\nPyOpenGL                                 3.1.10\npyOpenSSL                                24.2.1\npyparsing                                3.2.5\npyperclip                                1.11.0\npyproj                                   3.7.2\npysal                                    25.7\npyshp                                    3.0.2.post1\nPySocks                                  1.7.1\npyspark                                  3.5.1\npytensor                                 2.35.1\npytest                                   8.4.2\npython-apt                               0.0.0\npython-box                               7.3.2\npython-dateutil                          2.9.0.post0\npython-dotenv                            1.2.1\npython-json-logger                       4.0.0\npython-louvain                           0.16\npython-multipart                         0.0.20\npython-slugify                           8.0.4\npython-snappy                            0.7.3\npython-utils                             3.9.1\npytz                                     2025.2\npyviz_comms                              3.0.6\nPyWavelets                               1.9.0\nPyYAML                                   6.0.3\npyzmq                                    26.2.1\nquantecon                                0.10.1\nraft-dask-cu12                           25.10.0\nrapids-dask-dependency                   25.10.0\nrapids-logger                            0.1.19\nrasterio                                 1.4.3\nrasterstats                              0.20.0\nratelim                                  0.1.6\nreferencing                              0.37.0\nregex                                    2025.11.3\nrequests                                 2.32.4\nrequests-oauthlib                        2.0.0\nrequests-toolbelt                        1.0.0\nrequirements-parser                      0.9.0\nrfc3339-validator                        0.1.4\nrfc3986-validator                        0.1.1\nrfc3987-syntax                           1.1.0\nrich                                     13.9.4\nrmm-cu12                                 25.10.0\nroman-numerals-py                        3.1.0\nrpds-py                                  0.29.0\nrpy2                                     3.5.17\nrsa                                      4.9.1\nrtree                                    1.4.1\nruff                                     0.14.6\nsafehttpx                                0.1.7\nsafetensors                              0.7.0\nscikit-image                             0.25.2\nscikit-learn                             1.6.1\nscipy                                    1.16.3\nscooby                                   0.11.0\nscs                                      3.2.9\nseaborn                                  0.13.2\nSecretStorage                            3.5.0\nsegregation                              2.5.3\nsemantic-version                         2.10.0\nSend2Trash                               1.8.3\nsentence-transformers                    5.1.2\nsentencepiece                            0.2.1\nsentry-sdk                               2.46.0\nsetuptools                               75.2.0\nshap                                     0.50.0\nshapely                                  2.1.2\nshellingham                              1.5.4\nsimple-parsing                           0.1.7\nsimplejson                               3.20.2\nsimsimd                                  6.5.3\nsix                                      1.17.0\nsklearn-pandas                           2.2.0\nslicer                                   0.0.8\nsmart_open                               7.5.0\nsmmap                                    5.0.2\nsniffio                                  1.3.1\nsnowballstemmer                          3.0.1\nsortedcontainers                         2.4.0\nsoundfile                                0.13.1\nsoupsieve                                2.8\nsoxr                                     1.0.0\nspacy                                    3.8.11\nspacy-legacy                             3.0.12\nspacy-loggers                            1.0.5\nspaghetti                                1.7.6\nspanner-graph-notebook                   1.1.8\nspglm                                    1.1.0\nSphinx                                   8.2.3\nsphinxcontrib-applehelp                  2.0.0\nsphinxcontrib-devhelp                    2.0.0\nsphinxcontrib-htmlhelp                   2.1.0\nsphinxcontrib-jsmath                     1.0.1\nsphinxcontrib-qthelp                     2.0.0\nsphinxcontrib-serializinghtml            2.0.0\nspint                                    1.0.7\nsplot                                    1.1.7\nspopt                                    0.7.0\nspreg                                    1.8.4\nSQLAlchemy                               2.0.44\nsqlalchemy-spanner                       1.17.1\nsqlglot                                  25.20.2\nsqlparse                                 0.5.3\nsrsly                                    2.5.2\nsse-starlette                            3.0.3\nstanio                                   0.5.1\nstarlette                                0.48.0\nstatsmodels                              0.14.5\nstringzilla                              4.2.3\nstumpy                                   1.13.0\nsympy                                    1.14.0\ntables                                   3.10.2\ntabulate                                 0.9.0\ntbb                                      2022.3.0\ntblib                                    3.2.2\ntcmlib                                   1.4.1\ntenacity                                 9.1.2\ntensorboard                              2.19.0\ntensorboard-data-server                  0.7.2\ntensorflow                               2.19.0\ntensorflow-datasets                      4.9.9\ntensorflow_decision_forests              1.12.0\ntensorflow-hub                           0.16.1\ntensorflow-metadata                      1.17.2\ntensorflow-probability                   0.25.0\ntensorflow-text                          2.19.0\ntensorstore                              0.1.79\ntermcolor                                3.2.0\nterminado                                0.18.1\ntext-unidecode                           1.3\ntextblob                                 0.19.0\ntf_keras                                 2.19.0\ntf-slim                                  1.1.0\nthinc                                    8.3.10\nthreadpoolctl                            3.6.0\ntifffile                                 2025.10.16\ntiktoken                                 0.12.0\ntimm                                     1.0.22\ntinycss2                                 1.4.0\ntobler                                   0.12.1\ntokenizers                               0.22.1\ntoml                                     0.10.2\ntomlkit                                  0.13.3\ntoolz                                    0.12.1\ntorch                                    2.9.0+cu126\ntorchao                                  0.10.0\ntorchaudio                               2.9.0+cu126\ntorchdata                                0.11.0\ntorchsummary                             1.5.1\ntorchtune                                0.6.1\ntorchvision                              0.24.0+cu126\ntornado                                  6.5.1\ntqdm                                     4.67.1\ntraitlets                                5.7.1\ntraittypes                               0.2.3\ntransformers                             4.57.2\ntreelite                                 4.4.1\ntreescope                                0.1.10\ntriton                                   3.5.0\ntsfresh                                  0.21.1\ntweepy                                   4.16.0\ntypeguard                                4.4.4\ntyper                                    0.20.0\ntyper-slim                               0.20.0\ntypes-pytz                               2025.2.0.20251108\ntypes-setuptools                         80.9.0.20250822\ntyping_extensions                        4.15.0\ntyping-inspection                        0.4.2\ntzdata                                   2025.2\ntzlocal                                  5.3.1\nuc-micro-py                              1.0.3\nucxx-cu12                                0.46.0\numap-learn                               0.5.9.post2\numf                                      1.0.2\nuri-template                             1.3.0\nuritemplate                              4.2.0\nurllib3                                  2.5.0\nuvicorn                                  0.38.0\nvega-datasets                            0.9.0\nwadllib                                  1.3.6\nwandb                                    0.23.0\nwasabi                                   1.1.3\nwatchdog                                 6.0.0\nwcwidth                                  0.2.14\nweasel                                   0.4.3\nwebcolors                                25.10.0\nwebencodings                             0.5.1\nwebsocket-client                         1.9.0\nwebsockets                               15.0.1\nWerkzeug                                 3.1.3\nwheel                                    0.45.1\nwidgetsnbextension                       3.6.10\nwordcloud                                1.9.4\nwrapt                                    2.0.1\nwurlitzer                                3.1.1\nxarray                                   2025.11.0\nxarray-einstats                          0.9.1\nxgboost                                  3.1.2\nxlrd                                     2.0.2\nxxhash                                   3.6.0\nxyzservices                              2025.11.0\nyarl                                     1.22.0\nydf                                      0.13.0\nyellowbrick                              1.5\nyfinance                                 0.2.66\nzict                                     3.0.0\nzipp                                     3.23.0\nzstandard                                0.25.0",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Modules"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#getting-started",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#getting-started",
    "title": "Basic Use of Python",
    "section": "Getting Started",
    "text": "Getting Started\nWhen you open a new notebook in colab by default it will use python to run any ‘code cells’ (this can be changed in the ‘notebook settings’ under the View -&gt; ‘Notebook info’ menu).\nThere are two types of cells: - Code cells: allow you to submit code - Text cells: allow you to write text using ‘markdown’ (we’ll learn more about that shortly!)\nThese can be added in the top left of the notebook (+ Code and + Text). Below is a python code cell. These can be run by clicking ‘shift-enter’ when you click on the cell.\n\n#A comment - this text is not evaluated\n5 + 6\n10 * 2\n5**2\n\n25\n\n\n\nOnly the last bit of code is ‘printed’ unless you specifically print it. We’ll do this much of the time with print() function.\n\n\n# % is mod, // is floor\nprint(10 / 3)\nprint(10 % 3)\nprint(10 // 3)\n\n3.3333333333333335\n1\n3\n\n\n\nOperators are applied left to right, except for exponentiation\n\n\n3 + 4 - 5\n\n2\n\n\n\n(3 + 4) - 5\n\n2\n\n\n\n3**2**4\n\n43046721\n\n\n\n#interpreted this way\n3**(2**4)\n\n43046721\n\n\n\n#not this\n(3**2)**4\n\n6561",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#creating-variables",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#creating-variables",
    "title": "Basic Use of Python",
    "section": "Creating Variables",
    "text": "Creating Variables\nYou can create an object using =. This saves the result in a variable you can call later.\n\nx = \"Hello! \"\ny = 'How are you?'\nprint(x)\nprint(x + y)\n\nHello! \nHello! How are you?\n\n\n\nStrings are automatically concatenated using the + operator. As with most programming languages, there are special characters like \\ which indicate something. For instance, \\n is a line break. These appear differently depending on if you print something or just view the object.\n\n\nx = \"Hello! \\n\"\ny = 'Then I asked, \"How are you?\"'\nx\n\n'Hello! \\n'\n\n\n\nprint(x)\n\nHello! \n\n\n\n\nx + y\n\n'Hello! \\nThen I asked, \"How are you?\"'\n\n\n\nprint(x + y)\n\nHello! \nThen I asked, \"How are you?\"\n\n\n\nVariables can be used to simplify and generalize your code\n\n\ndegrees_celsius = 26.0\nprint(9 / 5 * degrees_celsius + 32)\ndegrees_celsius = 100\nprint(9 / 5 * degrees_celsius + 32)\n\n78.80000000000001\n212.0\n\n\nYou might try to add a code cell to this notebook and\n\nCreate a new string variable\nUse + to concatenate it with the strings above",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#object-types",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#object-types",
    "title": "Basic Use of Python",
    "section": "Object Types",
    "text": "Object Types\nThere are a number of built-in objects you can create. Some important ones are listed below:\n\nText Type: str\n\n\ny = \"text string\"\ntype(y)\n\nstr\n\n\n\nNumeric Types: int, float\n\n\ny = 10\nprint(type(y))\nx = 10.4\nprint(type(x))\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n\n\n\nBoolean Type: bool\n\n\ny = True\ntype(y)\n\nbool\n\n\n\nSequence Types: list, tuple\n\n\nz = [10, \"a\", 11.5, True]\ntype(z)\n\nlist\n\n\n\nMapping Type: dict\n\n\nw = {\"key1\": \"value1\",\n     \"key2\": [\"value2\", 10]}\ntype(w)\n\ndict\n\n\nWe’ll cover these data types and their uses shortly!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#multiple-assignment",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#multiple-assignment",
    "title": "Basic Use of Python",
    "section": "Multiple Assignment",
    "text": "Multiple Assignment\n\nAssigning multiple variables on one line is easy in python\n\n\nx, y, z = \"Orange\", \"Banana\", \"Cherry\"\nprint(x)\nprint(y)\nprint(z)\n\nOrange\nBanana\nCherry\n\n\n\nx = y = z = \"Orange\"\nprint(x)\nprint(y)\n\nOrange\nOrange\n\n\nThe use of * can allow you to ‘pack’ the remaining values into one object. Placement of the * is important here!\n\nx, *y = \"Orange\", \"Banana\", \"Cherry\"\nprint(x)\nprint(y)\ntype(y)\n\nOrange\n['Banana', 'Cherry']\n\n\nlist\n\n\n\n*x, y = \"Orange\", \"Banana\", \"Cherry\"\nprint(x)\nprint(y)\n\n['Orange', 'Banana']\nCherry\n\n\nWe’ll utilize packing and upnacking to simplify our code in many places!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#variable",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#variable",
    "title": "Basic Use of Python",
    "section": "_ Variable",
    "text": "_ Variable\nWhen doing python interactively (as with a JupyterLab notebook), the last evaluated expression is assigned to the variable _. This carries across code cells.\n\nx, y, z = \"Orange\", \"Banana\", \"Cherry\"\nx\n\n'Orange'\n\n\n\n_\n\n'Orange'\n\n\n\nx\n\n'Orange'\n\n\n\n#print doesn't count toward the _!\nprint(y)\n\nBanana\n\n\n\n_\n\n'Orange'\n\n\n\ny\n\n'Banana'\n\n\n\n_\n\n'Banana'\n\n\nWe’ll use this _ operator when doing computations where we don’t need to save things. For instance,\n\ndegrees_celcius = 100\n(9 / 5) * degrees_celcius + 32\n\n212.0\n\n\n\n_ - 10\n\n202.0\n\n\n\n(9 / 5) * degrees_celcius + 32 - 10\n\n202.0\n\n\n\n_ * 10\n\n2020.0\n\n\nWhere it really comes in handy is as a placeholder variable when doing computations in a for loop or list comprehension (again covered later more fully!).\nHere we replace the index of the for loop with _.\n\nsum_numbers = 0\n#no need to create a variable for the index\nfor _ in range(1,101):\n  sum_numbers += _\nsum_numbers\n\n5050",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#copying-vs-referencing",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#copying-vs-referencing",
    "title": "Basic Use of Python",
    "section": "Copying vs Referencing",
    "text": "Copying vs Referencing\nCareful when modifying elements of a compound object: ‘assignment statements do not copy objects, they create bindings between a target (a spot in computer memory) and an object’!\nIf you come from R, this is a very different behavior!\n\n#Changing the original compound object (list) modifies both variables\n#First, create a 'list' of four values\nx = [1, 2, 3, \"Cats Rule!\"]\n#Make y an alias for x (reference the same memory - this differs from how R works)\ny = x\n#note that they are the same when printing\nprint(x, y)\n\n[1, 2, 3, 'Cats Rule!'] [1, 2, 3, 'Cats Rule!']\n\n\nWe can modify a list element by using [] after the object name. Note that python starts counting at 0.\n\nHere we access and overwrite the 3 element (fourth element in the list)\n\n\n#Modifying x here actually modifies y too!\nx[3] = \"Dogs rule!\"\nprint(x, y)\n\n[1, 2, 3, 'Dogs rule!'] [1, 2, 3, 'Dogs rule!']\n\n\n\nIf you want to avoid this behavior, you can create a copy of the object instead of a reference\nTo do so, we use the .copy() method. Methods are like functions but we append them to the rear of the object after a .\n\n\n#Can create a (shallow) copy of the object rather than point to the same object in memory\ny = x.copy()\nx[2] = 10\nx[3]= \"No cats rule!\"\n#Note that y doesn't change its value\nprint(x, y)\n\n[1, 2, 10, 'No cats rule!'] [1, 2, 3, 'Dogs rule!']",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#variable-names",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#variable-names",
    "title": "Basic Use of Python",
    "section": "Variable Names",
    "text": "Variable Names\nVariable names can use letters, digits, and the underscore symbol (but cannot start with a digit)\nOk variable names:\n\nX, species5618, and degrees_celsius\n\nBad variable names:\n\n777 (begins with a digit)\nno-way! (includes punctuation)",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#augmented-assignment",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#augmented-assignment",
    "title": "Basic Use of Python",
    "section": "Augmented Assignment",
    "text": "Augmented Assignment\nPython has lots of shorthand notation!\n\nQuite often we want to take a value, add to it, and replace the old value\n\n\nwinnings = 100\nwinnings = winnings + 20\nwinnings\n\n120\n\n\n\n‘Augmented assignment’ gives a shorthand for doing this\n\n\nwinnings = 100\nwinnings += 20\nwinnings\n\n120\n\n\n\nThis works for all operators except negation\n\n\n#subtraction\nwinnings\nwinnings -= 30\nwinnings\n\n90\n\n\n\n#multiplication\nwinnings *= 40\nwinnings\n\n3600\n\n\n\n#exponentiation\nwinnings **= 1/2\nwinnings\n\n60.0\n\n\n\nAugmented Assignment Execution\nExecuted in the following way:\n\nEvaluate the expression on the right of the = sign to produce a value\nApply the operator to the variable on the left and the value produced\nStore this new value in the memory address of the variable on the left of the =.\n\nThis means the operator is applied after the expression on the right is evaluated.\n\nwinnings = 100\nwinnings += 100*10\nwinnings\n\n1100",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#continuing-a-line-of-code",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#continuing-a-line-of-code",
    "title": "Basic Use of Python",
    "section": "Continuing a Line of Code",
    "text": "Continuing a Line of Code\n\nFor long lines of code, we can break the code across multiple lines using \\ or by wrapping the code in ()\n\n\n10 + 20 - 100 * 60 \\\n/ 20\n\n-270.0\n\n\n\n(10 + 20 - 100 * 60\n/20)\n\n-270.0\n\n\nUsing \\ is going to come in very handy when we want to apply multiple methods on one object later in the semester!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#functions-methods",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#functions-methods",
    "title": "Basic Use of Python",
    "section": "Functions & Methods",
    "text": "Functions & Methods\nTwo major ways to do an operation on a variable/object:\n\nFunctions: function_name(myvar, other_args)\nMethods: myvar.method(other_args)\n\nFunctions are usually more generic actions that you could take on multiple types of objects. For instance, len() is a function we can run to see the ‘length’ of an object.\n\nmyList = [1, 10, 100, 1000]\n#len function\nlen(myList)\n\n4\n\n\nSimilarly, max() is another function we can use on many types of objects.\n\n#max function\nmax(myList)\n\n1000\n\n\nMethods on the other hand are specific to the type of object you are dealing with. Lists will have different methods than a dictionary, for instance.\nHere we use the .pop() method on a this list. This returns and removes the last element from the list.\n\n#pop method\nmyList.pop(3)\n\n1000\n\n\n\n#last element removed\nmyList\n\n[1, 10, 100]\n\n\nThe .append() method adds an element to the end of the list.\n\nmyList.append(100000)\nmyList\n\n[1, 10, 100, 100000]",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/02-Basic_Use_Of_Python.html#video-demo",
    "href": "01_Programming_in_python/02-Basic_Use_Of_Python.html#video-demo",
    "title": "Basic Use of Python",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video shows how to open a new Google Colab notebook and run some basic python code. I’d pop the video out into the panopto player using the arrow icon in the bottom right.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src = 'https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=bae161a8-bac0-4c44-a7a1-b0ef0163e90d&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all', width = '620', height = '380')",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Basic Use of Python"
    ]
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#what-is-markdown",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#what-is-markdown",
    "title": "Markdown Capabilities",
    "section": "What is Markdown?",
    "text": "What is Markdown?\n\nMost have heard of HTML (HyperText Mark-up Language)\n\nThere we write plain text that the browser interprets and renders\n\nMarkdown is a specific ‘mark-up’ language\n\nEasier syntax\n\nNot as powerful\n\nCan be used in ‘Text’ cells\nDouble click a text cell to see the plain text formatting used!\nShift enter to make the cell ‘render’",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Markdown Capabilities"
    ]
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#markdown-syntax",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#markdown-syntax",
    "title": "Markdown Capabilities",
    "section": "Markdown Syntax",
    "text": "Markdown Syntax\nLet’s just go through examples of some of the most commonly used syntax (you can always see the syntax used in our notebooks by opening them in Colab and double clicking a cell!)\n\nGeneral link: [link](URL or relative link)\n\nMarkdown Reference from Jupyter with much more info than what we’ll go through here\nTo open in a new tab, you need to use HTML code though. Double click the cell to see the HTML code to go to the link in a new tab.\n\n*italics* and _italics_\n**bold** and __bold__\n_**italics and bold!**_ is fancy and loud\n~~strikethrough~~ becomes strikethrough\n\n\n\n--- Three dashes for a thematic break\n# Level 1 Header, ## Level 2 Header, to six levels\nImportant!: The use of headers can automatically creates a Table of Contents! This is super important for jumping through your document.\n\nSee top left icon on Colab\n\nInclude an image: ![](path/to/file.png) (double click this cell to see the URL we pull this image from)\n\n\n\nSurround text with backticks to make ‘code font’ (this makes it look like you know what you are doing :)",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Markdown Capabilities"
    ]
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#widgets-and-such",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#widgets-and-such",
    "title": "Markdown Capabilities",
    "section": "Widgets and Such",
    "text": "Widgets and Such\nWe can bring in lots of HTML style widgets into our notebooks! Widget Info\n\nLots of fun widgets that you can add\n\nMaps\nSliders\nText input\n\nCheck boxes\nEtc.\n\nThings won’t always work quite right in Colab but will when we move to JupyterLab later in the semester!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Markdown Capabilities"
    ]
  },
  {
    "objectID": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#video-demo",
    "href": "01_Programming_in_python/04-JupyterLab_Notebooks_Markdown.html#video-demo",
    "title": "Markdown Capabilities",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video demo shows how we can include some interactive HTML content in our Colab notebook. (Of course you can see we are bringing in a video here as well!) Remember to pop the video out into the full player using the button in the bottom right.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src = 'https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=32a49a33-bc92-49ab-b767-b0ef0167e1b3&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all', width = '720', height = '405')",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Markdown Capabilities"
    ]
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#ints-floats",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#ints-floats",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Ints & Floats",
    "text": "Ints & Floats\n\n(Real) Numbers are stored as int or float types\n\nPython generally figures out which to use and changes to float when needed\n\n\n\nx = 10\ntype(x)\n\nint\n\n\n\ny = 10.4\ntype(y)\n\nfloat\n\n\n\nz = y - 0.4\nprint(z)\ntype(z)\n\n10.0\n\n\nfloat\n\n\n\ntype(x + 0.5)\n\nfloat\n\n\n\nYou can cast things (or explicitly coerce them) using int() and float()\n\n\nx = 10\nprint(type(x))\nx\n\n&lt;class 'int'&gt;\n\n\n10\n\n\n\nx = float(x)\nprint(type(x))\nx\n\n&lt;class 'float'&gt;\n\n\n10.0\n\n\n\nint(10.9) #returns just the integer part (no rounding done)\n\n10",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Numeric Types (Int and Float) & Booleans"
    ]
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#functions-operators",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#functions-operators",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Functions & Operators",
    "text": "Functions & Operators\n\nWe have all the numeric operators discussed previously and a few handy functions built in too\n\n\n#divide and discard remainder\n100 // 3\n\n33\n\n\n\n#modulus or remainder\n100 % 3\n\n1\n\n\n\n#whole number division and modulus\ndivmod(100, 3) #returns a 'tuple' (a sort of immutable list)\n\n(33, 1)\n\n\n\n#raise to a power 4^3\npow(4, 3)\n\n64\n\n\n\n#equivalent to\n4 ** 3\n\n64\n\n\n\nabs(-100)\n\n100\n\n\n\nround(10.4242, 2)\n\n10.42\n\n\n\nmath module\n\nAs we saw, the math module has a number of useful functions\nRecall we can import the math module to gain access to its functions. We then preface functions/objects from the module with math.\n\n\nx = 10.55\n#a boolean function (more on this shortly)\nx.is_integer()\n\nFalse\n\n\n\nimport math\nmath.floor(x)\n\n10\n\n\n\nmath.ceil(x)\n\n11\n\n\n\nmath.factorial(10)\n\n3628800",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Numeric Types (Int and Float) & Booleans"
    ]
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#things-to-be-aware-of",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#things-to-be-aware-of",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Things To Be Aware Of",
    "text": "Things To Be Aware Of\nFloats are not stored precisely!\n\n1.2-1.0\n\n0.19999999999999996\n\n\n\nComes from using a binary representation of floats\nNot worth getting into, but if you see something weird like this, that is why!\nMore info here",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Numeric Types (Int and Float) & Booleans"
    ]
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#things-to-noteremember",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#things-to-noteremember",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Things to Note/Remember",
    "text": "Things to Note/Remember\n\nAugmented assignment operators are useful\n\n\nx = 100\nx += 200\nx\n\n300\n\n\n\nMultiple assignment can be done\n\n\nx = y = z = 40\nprint(x, y)\n\n40 40\n\n\n\nx, y, z = 40, 50, 60\nprint(x, y)\nprint(z)\n\n40 50\n60",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Numeric Types (Int and Float) & Booleans"
    ]
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#more-formatting-strings",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#more-formatting-strings",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "More Formatting Strings",
    "text": "More Formatting Strings\nWe saw how to format strings earlier. Let’s revit that year!\n\nJob = \"Professor\"\nYears = 10.23\nmy_string = \"I am a {job} and I've been teaching for {years:d} years\"\n\nWe can use the .format() method on the string to place values in the placeholders. The years:d above specifies the type of formatting to use on the number, d stands for integer\n\nmy_string.format(job = Job, years = Years) #throws an error as it expects an integer for years\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/tmp/ipython-input-2749422516.py in &lt;cell line: 0&gt;()\n----&gt; 1 my_string.format(job = Job, years = Years) #throws an error as it expects an integer for years\n\nValueError: Unknown format code 'd' for object of type 'float'\n\n\n\n\nmy_string.format(job = Job, years = int(Years)) #cast years as an integer via int()\n\n\"I am a Professor and I've been teaching for 10 years\"\n\n\nWe can specify the type of number input for the .format() method to use via this name:number_type syntax:\n\nd - Integers\nf - Floating point numbers\n.f - Floating point numbers with a fixed amount of digits to the right of the dot.\n\n\nprint(\"I am a {job} and I've been teaching for {years:f} years\".format(job = Job, years = Years))\nprint(\"I am a {job} and I've been teaching for {years:.1f} years\".format(job = Job, years = Years))\n\nI am a Professor and I've been teaching for 10.230000 years\nI am a Professor and I've been teaching for 10.2 years\n\n\nActually four different ways to substitute into a string (if you are interested!)",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Numeric Types (Int and Float) & Booleans"
    ]
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#booleaninteger-relationship",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#booleaninteger-relationship",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Boolean/Integer Relationship",
    "text": "Boolean/Integer Relationship\n\nBooleans are actually a subtype of integers\n\nTrue treated as 1\nFalse treated as 0\n\n\n\nprint(3 + True, 3 * False)\n\n4 0\n\n\nOne thing of note is that when you do math on True or False it converts the result. Note the last computation result below.\n\nprint(str(True), str(False), str(True + 0))\n\nTrue False 1",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Numeric Types (Int and Float) & Booleans"
    ]
  },
  {
    "objectID": "01_Programming_in_python/06-Numeric_Types_Booleans.html#video-demo",
    "href": "01_Programming_in_python/06-Numeric_Types_Booleans.html#video-demo",
    "title": "Numeric Types (Int and Float) & Booleans",
    "section": "Video Demo",
    "text": "Video Demo\nThis quick video shows some useful functions from the .math module for dealing with integers, floats, and booleans. Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=d59f4288-1a98-446e-a82f-b0f0013e8445&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Numeric Types (Int and Float) & Booleans"
    ]
  },
  {
    "objectID": "01_Programming_in_python/09-Control_Flow.html#conditional-statements",
    "href": "01_Programming_in_python/09-Control_Flow.html#conditional-statements",
    "title": "Control Flow",
    "section": "Conditional Statements",
    "text": "Conditional Statements\n\nChoose which portions of your code to execute by using conditional statements!\nAn if statement changes how a program behaves based on a condition\n\nCondition comes in the form of a boolean\n\nRecall: Booleans are True or False\n\nCan be treated as 1 and 0\nMany functions to create bools (.is_*() methods, bool() function)\n\n\n\nif Syntax\n\nExample of if with no further conditional logic:\n\nif boolean:\n    #If boolean is true, execute the chunk of code that is indented\n    #Four spaces is recommended but any indentation can technically be used\n    statement1\n    statement2\n    \n#code not indented would then execute as normal\n\nif Example\nPrinting different strings using if statements\n\ntemp = 30\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n\n30 degrees is cold.\n\n\n\ntemp = 100\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n\nWe can have multiple statements that are executed within a block if the condition is True.\n\ntemp = 30\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\n\n30 degrees is cold.\nWear a jacket outside!\n\n\n\ntemp = 100\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nif temp &gt;= 50:\n    print(temp, \"degrees is not cold.\")\n\n100 degrees is not cold.\n\n\n\n\n\nif with else Syntax\nWith the last example above, we see something we often want to do: - check a condition (temp &lt; 50), if True execute some code - if that same condition is False - or the opposite is True (temp &gt;= 50) - then execute something else\nThis can be taken care of with the else statement. The else statement immediately following an if block allows for execution of code only when the above condition(s) were (all) False\nif boolean:\n    execute this code\nelse:\n    execute this code\nthis is logically equivalent to\nif boolean:\n    execute this code\nif not boolean:\n    execute this code\n\ntemp = 100\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nelse:\n    print(temp, \"degrees is not cold.\")\n\n100 degrees is not cold.\n\n\n\n\nif, elif, and else\nYou can check additional conditions using elif which stands for ‘else if’.\nThis condition is only checked if all the above conditions were False.\nif boolean1:\n    #if boolean1 is True\n    execute this code  \nelif boolean2:\n    #if boolean1 is False, check if boolean2 is True\n    #if True\n    execute this code\nelif boolean3:\n    #if boolean1 and boolean2 are False, check if boolean3 is True\n    #if True\n    execute this code\nelse:\n    # if no conditions met\n    execute this code\n\ntemp = 40\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nelif temp &lt; 70:\n    print(temp, \"degrees is kind of cold...\")\n    print(\"You may want to bring an umbrella in case it rains!\")\nelse:\n    print(temp, \"degrees is not cold.\")\n\n40 degrees is cold.\nWear a jacket outside!\n\n\n\ntemp = 60\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nelif temp &lt; 70:\n    print(temp, \"degrees is kind of cold...\")\n    print(\"You may want to bring an umbrella in case it rains!\")\nelse:\n    print(temp, \"degrees is not cold.\")\n\n60 degrees is kind of cold...\nYou may want to bring an umbrella in case it rains!\n\n\n\ntemp = 100\nif temp &lt; 50:\n    print(temp, \"degrees is cold.\")\n    print(\"Wear a jacket outside!\")\nelif temp &lt; 70:\n    print(temp, \"degrees is kind of cold...\")\n    print(\"You may want to bring an umbrella in case it rains!\")\nelse:\n    print(temp, \"degrees is not cold.\")\n\n100 degrees is not cold.",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Control Flow"
    ]
  },
  {
    "objectID": "01_Programming_in_python/09-Control_Flow.html#loops",
    "href": "01_Programming_in_python/09-Control_Flow.html#loops",
    "title": "Control Flow",
    "section": "Loops",
    "text": "Loops\nConditionally executing code is really useful! Another useful thing is to repeatedly execute code. Each execution you may want to change something in the computation.\n\nLoop Example\n\nSuppose we’ve observed the eye colors of 15 people\nEye color coded as either 1 (blue), 2 (brown), 3 (green), 4 (other)\nWant to create a new variable that has the descriptive values\n\n\n#data stored in a 'list'\neye_color = [3, 2, 2, 1, 2, 1, 2, 4, 3, 2, 2, 1, 2, 2]\n\nGoal: Create a new variable that has the descriptive values - Recall list elements are indexed using [index] (like other sequence type objects such as strings)\n\nprint(eye_color[0])\nprint(eye_color[1])\n\n3\n2\n\n\n\nWe could consider using conditional logic to print out the descriptive string\n\n\neye_color = [3, 2, 2, 1, 2, 1, 2, 4, 3, 2, 2, 1, 2, 2]\n\nif eye_color[0] == 1:\n    print(\"blue\")\nelif eye_color[0] == 2:\n    print(\"brown\")\nelif eye_color[0] == 3:\n    print(\"green\")\nelse:\n    print(\"other\")\n\ngreen\n\n\n\n\n\nLoop Syntax\n\nInstead of repeating and modifying code, use a loop!\n\nfor index in values:\n     code to be run\n\nindex argument defines a counter, or variable, that varies each time the code within the loop is executed\nvalues argument defines which values the index takes on in these iterations\n\nThese values do not have to be numeric!\n\n\n\nLoop Toy Examples\n\nRun the code below and feel free to modify things to see what happens!\n\n\nfor index in [\"cat\", \"hat\", \"worm\"]:\n    print(index)\n\ncat\nhat\nworm\n\n\n\nvalues = list(range(10)) #recall range is an iterator-type object, list gets the values out\nprint(values)\nfor i in values:\n    print(i)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n#we can get the indices of the eye_color object\nprint(list(range(len(eye_color))))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n\n\n\n#alternatively, we can just iterate over the range() object itself\nvalues = iter(range(len(eye_color)))\nprint(next(values))\nprint(next(values))\nprint(next(values))\n\n0\n1\n2\n\n\nBack to our example. We want to print out the more descriptive name depending on what the numeric value is.\nLet’s loop through all the eye_color values using a for loop. To do so, we’ll use the range() function with the len(eye_color) as its argument.\nRecall: range() is an iterator-type object. A for loop will automatically go over the values the range indicates. When we give range() just one argument it defaults to a range of 0 to that value (but that value isn’t included).\n\nfor i in range(len(eye_color)):\n    if eye_color[i] == 1:\n        print(\"blue\")\n    elif eye_color[i] == 2:\n        print(\"brown\")\n    elif eye_color[i] == 3:\n        print(\"green\")\n    else:\n        print(\"other\")\n\ngreen\nbrown\nbrown\nblue\nbrown\nblue\nbrown\nother\ngreen\nbrown\nbrown\nblue\nbrown\nbrown\n\n\nWe don’t really need to use a set of numeric values to iterate over (via range()). We can iterate over anything that is iterable. All sequence type objects are iterable (like lists and strings).\nHere we’ll iterate over the eye_color list itself.\n\nfor i in eye_color:\n    if i == 1:\n        print(\"blue\")\n    elif i == 2:\n        print(\"brown\")\n    elif i == 3:\n        print(\"green\")\n    else:\n        print(\"other\")\n\ngreen\nbrown\nbrown\nblue\nbrown\nblue\nbrown\nother\ngreen\nbrown\nbrown\nblue\nbrown\nbrown\n\n\n\n\n\n\nOther Looping Commands\n\nOccassionally we want to jump out of a for loop. This can be done with break\n\n\nfor i in range(5):\n    if i == 3:\n        break\n    print(i)\n\n0\n1\n2\n\n\n\nThe continue command jumps to the next iteration of the loop without finishing the current iteration\n\n\nfor i in range(5):\n    if i == 3:\n        continue\n    print(i)\n\n0\n1\n2\n4\n\n\n\n\n\nWhile Loops\n\nWhile loops are similar to for loops but they loop until a condition is reached\n\nUseful when we don’t know in advance how many loop iterations we should execute\n\nGeneral syntax of a while loop:\n\nwhile expression:\n    #code block to execute\n    block\n\nthe expression is sometimes called the loop condition\nAt each iteration of the loop, python evaluates the expression\n\nIf the expression evaluates to False, the loop exits\nIf the expression evaluates to True, the loop body is executed again\n\n\n\nWhile Loop Example\nWhen using while loops, we usually modify the condition within the body of the while loop (or use a break to jump out when needed).\n\nrabbits = 3\nwhile rabbits &gt; 0:\n    print(rabbits)\n    rabbits = rabbits - 1\n\n3\n2\n1\n\n\n\n\n\nVideo Demo\nThis quick video demo shows an example of implementing a loop in python. We’ll look at the Fizzbuzz example. Then we’ll write a quick number guessing game! Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=ed3118c1-feb1-4d7f-a98c-b0f800ebfd46&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Control Flow"
    ]
  },
  {
    "objectID": "01_Programming_in_python/11-Dictionaries.html#creating-a-dictionary",
    "href": "01_Programming_in_python/11-Dictionaries.html#creating-a-dictionary",
    "title": "Dictionaries",
    "section": "Creating a Dictionary",
    "text": "Creating a Dictionary\n\nCreating a dictionary using dict() or {}\n\nUse dict(supply_key_value_pairs) or {supply_key_value_pairs}\n\n\n{} #empty dictionary\nmydict = {\n  \"key1\": [12, -10, \"value1\"],  #key is specified first (must be immutable)\n  \"key2\": [11, \"value2\"],       #value is then given after a : (can be anything, doesn't need to match across keys)\n  \"key3\": \"value3\"\n  }\nmydict\n\n{'key1': [12, -10, 'value1'], 'key2': [11, 'value2'], 'key3': 'value3'}\n\n\n\nmydict2 = dict([\n  (1, ['hee', 'haw']),        #passing key value pairs as a tuple\n  (2, 'fa')\n  ])\nmydict2\n\n{1: ['hee', 'haw'], 2: 'fa'}\n\n\n\n\nCreating a dictionary using lists\n\nYou can create a dictionary using two lists and the zip() function\n\n\nkeys = [x for x in \"abcdefgh\"]\nvalues = [y for y in range(0,8)]\nprint(keys)\nprint(values)\n\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n[0, 1, 2, 3, 4, 5, 6, 7]\n\n\n\ndict(zip(keys, values))\n\n{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}\n\n\n\n\nCreating a dictionary with dictionary comprehensions\n\nCan create a dictionary using dictionary comprehensions!\nSmilar to list comprehensions but we use { instead of [\n\n\nmydict = {i: i for i in range(0, 6)}\nmydict\n\n{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n\n\n\nmydict = {\"abcdef\"[i]: i**2 for i in range(0, 6)}\nmydict\n\n{'a': 0, 'b': 1, 'c': 4, 'd': 9, 'e': 16, 'f': 25}",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Dictionaries"
    ]
  },
  {
    "objectID": "01_Programming_in_python/11-Dictionaries.html#dictionary-operations",
    "href": "01_Programming_in_python/11-Dictionaries.html#dictionary-operations",
    "title": "Dictionaries",
    "section": "Dictionary Operations",
    "text": "Dictionary Operations\n\nIndexing a Dictionary\n\nIndex with a [key] (remember unordered!)\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"]\n  }\nAFCDivisions[\"North\"]\n\n['Steelers', 'Browns', 'Ravens', 'Bengals']\n\n\n\nYou can access the returned object in the same line of code\n\n\nAFCDivisions[\"North\"][0]\n\n'Steelers'\n\n\n\nWe can add new key/value pairs by simply referencing a key that doesn’t exist\nHere we add the “South” key with a silly value\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"]\n  }\nAFCDivisions[\"South\"] = [1, 2]\nAFCDivisions\n\n{'North': ['Steelers', 'Browns', 'Ravens', 'Bengals'],\n 'East': ['Patriots', 'Jets', 'Dolphins', 'Bills'],\n 'West': ['Raiders', 'Chiefs', 'Chargers', 'Broncos'],\n 'South': [1, 2]}\n\n\n\nYou can iterate over the keys in a dictionary\n\n\nfor key in AFCDivisions:              #keys are what are iterated over\n    print(key, ' : ', AFCDivisions[key]) #we access the value associated with the key\n\nNorth  :  ['Steelers', 'Browns', 'Ravens', 'Bengals']\nEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nSouth  :  [1, 2]\n\n\n\nYou can overwrite the values similar to how you can add a key/value pair after the fact. Here we overwrite the “South” value.\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"]\n  }\nAFCDivisions[\"South\"] = [1, 2]\nAFCDivisions[\"South\"] = [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\nfor key in AFCDivisions:\n    print(key, ' : ', AFCDivisions[key])\n\nNorth  :  ['Steelers', 'Browns', 'Ravens', 'Bengals']\nEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nSouth  :  ['Texans', 'Colts', 'Jaguars', 'Titans']\n\n\n\n\n\nDictionary Packing & Unpacking\n\nWe can pack dictionaries using ** similar to how we packed a list!\nHere we create a dictionary called Divisions where we pack two dictionaries inside it\n\n\nAFCDivisions = {\n  \"AFCNorth\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"AFCEast\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"AFCWest\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"AFCSouth\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\n\nNFCDivisions = {\n  \"NFCNorth\" : [\"Lions\", \"Bears\", \"Packers\", \"Vikings\"],\n  \"NFCEast\"  : [\"Giants\", \"Cowboys\", \"Eagles\", \"Admirals\"]\n}\n\nDivisions = {**AFCDivisions, **NFCDivisions}\nfor key in Divisions:\n    print(key, ' : ', Divisions[key])\n\nAFCNorth  :  ['Steelers', 'Browns', 'Ravens', 'Bengals']\nAFCEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nAFCWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nAFCSouth  :  ['Texans', 'Colts', 'Jaguars', 'Titans']\nNFCNorth  :  ['Lions', 'Bears', 'Packers', 'Vikings']\nNFCEast  :  ['Giants', 'Cowboys', 'Eagles', 'Admirals']",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Dictionaries"
    ]
  },
  {
    "objectID": "01_Programming_in_python/11-Dictionaries.html#dictionary-methods",
    "href": "01_Programming_in_python/11-Dictionaries.html#dictionary-methods",
    "title": "Dictionaries",
    "section": "Dictionary Methods",
    "text": "Dictionary Methods\nMany useful methods\n\nWe saw how to index with [] similar to lists/tuples\nWe can index with .get() instead\n\nAdvantage is that it doesn’t throw an error if the key doesn’t exist\n\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"South\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\nAFCDivisions.get(\"South\")\n\n['Texans', 'Colts', 'Jaguars', 'Titans']\n\n\n\nAFCDivisions.get(\"Northeast\") #doesn't throw an error\n\n\nAFCDivisions[\"Northeast\"] #throws an error\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-15-4c30b3284c03&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 AFCDivisions[\"Northeast\"] #throws an error\n\nKeyError: 'Northeast'\n\n\n\n\nReturn keys with .keys(); values with .values()\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"South\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\nAFCDivisions.keys()\n\ndict_keys(['North', 'East', 'West', 'South'])\n\n\n\nAFCDivisions.values()\n\ndict_values([['Steelers', 'Browns', 'Ravens', 'Bengals'], ['Patriots', 'Jets', 'Dolphins', 'Bills'], ['Raiders', 'Chiefs', 'Chargers', 'Broncos'], ['Texans', 'Colts', 'Jaguars', 'Titans']])\n\n\n\nReturn and remove a specified key with .pop() (similar to the list method)\n\n\nAFCDivisions = {\n  \"North\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"East\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"West\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"South\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\nAFCDivisions.pop(\"North\") #modifies the dictionary\n\n['Steelers', 'Browns', 'Ravens', 'Bengals']\n\n\n\nfor key in AFCDivisions:\n    print(key, ' : ', AFCDivisions[key]) #North no longer exists!\n\nEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nSouth  :  ['Texans', 'Colts', 'Jaguars', 'Titans']\n\n\n\nMerge in another dictionary with .update()\nVery similar to dictionary packing done above but it modifies one of the dictionaries\n\n\nDivisions = {\n  \"AFCNorth\": [\"Steelers\", \"Browns\", \"Ravens\", \"Bengals\"],\n  \"AFCEast\" : [\"Patriots\", \"Jets\", \"Dolphins\", \"Bills\"],\n  \"AFCWest\" : [\"Raiders\", \"Chiefs\", \"Chargers\", \"Broncos\"],\n  \"AFCSouth\": [\"Texans\", \"Colts\", \"Jaguars\", \"Titans\"]\n  }\nNFCNorth = {\n  \"NFCNorth\": [\"Lions\", \"Packers\", \"Bears\", \"Vikings\"]\n  }\nDivisions.update(NFCNorth) #combine the dictionaries\nfor key in Divisions.keys():\n    print(key, \" : \", Divisions[key])\n\nAFCNorth  :  ['Steelers', 'Browns', 'Ravens', 'Bengals']\nAFCEast  :  ['Patriots', 'Jets', 'Dolphins', 'Bills']\nAFCWest  :  ['Raiders', 'Chiefs', 'Chargers', 'Broncos']\nAFCSouth  :  ['Texans', 'Colts', 'Jaguars', 'Titans']\nNFCNorth  :  ['Lions', 'Packers', 'Bears', 'Vikings']",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Dictionaries"
    ]
  },
  {
    "objectID": "01_Programming_in_python/14-Pandas_Series.html#creating-a-pandas-series",
    "href": "01_Programming_in_python/14-Pandas_Series.html#creating-a-pandas-series",
    "title": "Pandas Series",
    "section": "Creating a pandas Series",
    "text": "Creating a pandas Series\n\nCreate a series using the pd.Series() function\n\n\nimport numpy as np\nimport pandas as pd\nrng = np.random.default_rng(2) #set a seed\ns = pd.Series(rng.normal(size = 10, loc = 2, scale = 4)) #mean of 2 and std of 4\ns\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n2.756214\n\n\n1\n-0.090994\n\n\n2\n0.347746\n\n\n3\n-7.765870\n\n\n4\n9.198830\n\n\n5\n6.576663\n\n\n6\n0.698309\n\n\n7\n5.095226\n\n\n8\n3.124843\n\n\n9\n-0.215291\n\n\n\n\ndtype: float64",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "`Pandas` Series"
    ]
  },
  {
    "objectID": "01_Programming_in_python/14-Pandas_Series.html#indexing-a-series",
    "href": "01_Programming_in_python/14-Pandas_Series.html#indexing-a-series",
    "title": "Pandas Series",
    "section": "Indexing a Series",
    "text": "Indexing a Series\n\nLike lists, the ordering starts at 0\nLike numpy arrays, all elements in a Series must be of the same type\nUnlike numpy arrays, Series can be indexed by an index attribute (not just the numeric index)\n.index attribute returns just these indices\n\n\ns.index\n\nRangeIndex(start=0, stop=10, step=1)\n\n\n\ns[0] #is both the numeric index and the value of an index here\n\n2.756213527174132\n\n\n\ns2 = pd.Series(rng.normal(size = 10, loc = 2, scale = 4),\n               index = [x for x in \"abcdefghij\"])\ns2\n\n\n\n\n\n\n\n\n0\n\n\n\n\na\n5.910270\n\n\nb\n0.757774\n\n\nc\n0.684704\n\n\nd\n-1.168587\n\n\ne\n3.819832\n\n\nf\n1.603208\n\n\ng\n4.181155\n\n\nh\n-0.428743\n\n\ni\n2.507311\n\n\nj\n-1.569096\n\n\n\n\ndtype: float64\n\n\n\ns2.index\n\nIndex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'], dtype='object')\n\n\nWe can access elements with the numeric index or the index value itself but this behavior will go away soon and the .iloc[] method should be used instead (we discuss the similar DataFrames .iloc[] method shortly).\n\ns2[2]\n\nFutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  s2[2]\n\n\n0.6847043837681492\n\n\n\ns2[\"c\"]\n\n0.6847043837681492\n\n\n\nWe can obtain just the values with of a Series using the .values attribute\n\n\ns.values\n\narray([ 2.75621353, -0.09099377,  0.34774583, -7.76586953,  9.19882953,\n        6.57666349,  0.69830865,  5.09522635,  3.12484268, -0.21529135])\n\n\n\ns2.values\n\narray([ 5.9102698 ,  0.75777381,  0.68470438, -1.16858702,  3.81983228,\n        1.60320779,  4.18115486, -0.4287428 ,  2.50731139, -1.56909617])\n\n\n\nNote that when you return the values you get back just a numpy array!\n\n\ntype(s2.values)\n\nnumpy.ndarray",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "`Pandas` Series"
    ]
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#data-formats",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#data-formats",
    "title": "Pandas for Reading Raw Data",
    "section": "Data Formats",
    "text": "Data Formats\nRaw data comes in many different formats. Understanding the raw data format is essential for reading that data into python. Some raw data types include:\n\n‘Delimited’ data: Character (such as ‘,’ , ‘&gt;’, or [’ ’]) separated data\nFixed field data\nExcel data\nFrom other statistical software, Ex: SPSS formatted data or SAS data sets\nFrom an Application Programming Interface (API) (often returned as a JSON file - key/value pairs, similar to a dictionary)\nFrom a database",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas for Reading Raw Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#delimited-data",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#delimited-data",
    "title": "Pandas for Reading Raw Data",
    "section": "Delimited Data",
    "text": "Delimited Data\nLet’s start with delimited data.\n\nOne common format for raw data is delimited data\n\nData that has a character or characters that separates the data values\nCharacter(s) is (are) called delimiter(s)\n\nUsing pandas the read_csv() function can read in this kind of data (although csv stands for ‘comma separated value’, this function is used for reading most delimited data via pandas)\n\nIf the raw data is well-formatted, we just need to tell python where to find it!",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas for Reading Raw Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#locating-a-file",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#locating-a-file",
    "title": "Pandas for Reading Raw Data",
    "section": "Locating a File",
    "text": "Locating a File\n\nHow does python locate the file?\n\nNot in colab\n\nYou can give file full path name\n\nex: ‘S:/Documents/repos/ST-554/datasets/data.csv’\n\nex: ‘S:\\Documents\\repos\\ST-554\\datasets\\data.csv’\n\n\n\nOr use local paths!\n\nDetermine your working directory\nUse a path relative to that\nIf your working directory is ‘S:/Documents/repos/ST-554’ you can get to ‘data.csv’ via ‘datasets/data.csv’\n\nThe os module gives you access to function for finding and setting your working directory\n\nUsing a cloud-based platform complicates things a bit - In colab you can + Mount your google drive + Read files from URLs + Upload files via the menu on the left (folder icon, then upload a file via the icons there)\n\nimport os\n#getcwd() stands for get current working directory\nos.getcwd() #shows the directory you can get to via the folder icon on the left\n#chdir() stands for change current directory\n#os.chdir(\"S:/Documents/repos/ST-554\") #won't work in colab but would work on a local python session\n\n'/content'\n\n\nThis /content refers to the main folder on the left hand side of Colab!",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas for Reading Raw Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-files-locally-in-colab",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-files-locally-in-colab",
    "title": "Pandas for Reading Raw Data",
    "section": "Reading Files ‘Locally’ in Colab",
    "text": "Reading Files ‘Locally’ in Colab\n\nNicely formatted .csv files can be read in with the read_csv() function from pandas\nneuralgia.csv has been loaded into the folder on colab in my session. Therefore, it exists in my working directory. This won’t be the case for you unless you upload the data during your session! You can click on the folder icon on the left, then click the upload button to upload  this data set.\n\n\nneuralgia_data = pd.read_csv(\"neuralgia.csv\") #neuralgia.csv file was uploaded to colab for my session\nneuralgia_data.head() #this code block won't work unless you upload the data in your session\n\n\n  \n    \n\n\n\n\n\n\nTreatment\nSex\nAge\nDuration\nPain\n\n\n\n\n0\nP\nF\n68\n1\nNo\n\n\n1\nB\nM\n74\n16\nNo\n\n\n2\nP\nF\n67\n30\nNo\n\n\n3\nP\nM\n66\n26\nYes\n\n\n4\nB\nF\n67\n28\nNo\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nneuralgia_data.shape\n\n(60, 5)",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas for Reading Raw Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-from-a-url",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-from-a-url",
    "title": "Pandas for Reading Raw Data",
    "section": "Reading From a URL",
    "text": "Reading From a URL\n\nNicely formatted .csv files can be read in with the read_csv() function from pandas\nscoresFull.csv file at a URL given by ‘https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv’\n\n\nscores_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\")\nscores_data.head()\n\n\n  \n    \n\n\n\n\n\n\nweek\ndate\nday\nseason\nawayTeam\nAQ1\nAQ2\nAQ3\nAQ4\nAOT\n...\nhomeFumLost\nhomeNumPen\nhomePenYds\nhome3rdConv\nhome3rdAtt\nhome4thConv\nhome4thAtt\nhomeTOP\nHminusAScore\nhomeSpread\n\n\n\n\n0\n1\n5-Sep\nThu\n2002\nSan Francisco 49ers\n3\n0\n7\n6\n-1\n...\n0\n10\n80\n4\n8\n0\n1\n32.47\n-3\n-4.0\n\n\n1\n1\n8-Sep\nSun\n2002\nMinnesota Vikings\n3\n17\n0\n3\n-1\n...\n1\n4\n33\n2\n6\n0\n0\n28.48\n4\n4.5\n\n\n2\n1\n8-Sep\nSun\n2002\nNew Orleans Saints\n6\n7\n7\n0\n6\n...\n0\n8\n85\n1\n6\n0\n1\n31.48\n-6\n6.0\n\n\n3\n1\n8-Sep\nSun\n2002\nNew York Jets\n0\n17\n3\n11\n6\n...\n1\n10\n82\n4\n8\n2\n2\n39.13\n-6\n-3.0\n\n\n4\n1\n8-Sep\nSun\n2002\nArizona Cardinals\n10\n3\n3\n7\n-1\n...\n0\n7\n56\n6\n10\n1\n2\n34.40\n8\n6.0\n\n\n\n\n5 rows × 82 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nscores_data.shape\n\n(3471, 82)\n\n\n\nOddly, to read other types of delimited data, we also use read_csv()!\n\nSpecify the sep = argument\n\nchemical.txt file (space delimiter) stored at “https://www4.stat.ncsu.edu/~online/datasets/chemical.txt”\n\n\nchem_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/chemical.txt\", sep=\" \")\nchem_data.head()\n\n\n  \n    \n\n\n\n\n\n\ntemp\nconc\ntime\npercent\n\n\n\n\n0\n-1.0\n-1.0\n-1.0\n45.9\n\n\n1\n1.0\n-1.0\n-1.0\n60.6\n\n\n2\n-1.0\n1.0\n-1.0\n57.5\n\n\n3\n1.0\n1.0\n-1.0\n58.6\n\n\n4\n-1.0\n-1.0\n1.0\n53.3\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ncrabs.txt file (tab delimiter) stored at “https://www4.stat.ncsu.edu/~online/datasets/crabs.txt”\n\nTab is \\t\n\n\n\ncrabs_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/crabs.txt\", sep=\"\\t\")\ncrabs_data.head()\n\n\n  \n    \n\n\n\n\n\n\ncolor\nspine\nwidth\nsatell\nweight\ny\n\n\n\n\n0\n3\n3\n28.3\n8\n3050\n1\n\n\n1\n4\n3\n22.5\n0\n1550\n0\n\n\n2\n2\n1\n26.0\n9\n2300\n1\n\n\n3\n4\n3\n24.8\n0\n2100\n0\n\n\n4\n4\n3\n26.0\n4\n2600\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\numps2012.txt file (&gt; delimiter) stored at “https://www4.stat.ncsu.edu/~online/datasets/umps2012.txt”\n\nNo column names in raw file\nCan specify header = None and give column names when reading (via names = [list of names])\n\n\n\nump_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/umps2012.txt\",\n                      sep=\"&gt;\",\n                      header=None,\n                      names=[\"Year\", \"Month\", \"Day\", \"Home\", \"Away\", \"HPUmpire\"])\nump_data.head()\n\n\n  \n    \n\n\n\n\n\n\nYear\nMonth\nDay\nHome\nAway\nHPUmpire\n\n\n\n\n0\n2012\n4\n12\nMIN\nLAA\nD.J. Reyburn\n\n\n1\n2012\n4\n12\nSD\nARI\nMarty Foster\n\n\n2\n2012\n4\n12\nWSH\nCIN\nMike Everitt\n\n\n3\n2012\n4\n12\nPHI\nMIA\nJeff Nelson\n\n\n4\n2012\n4\n12\nCHC\nMIL\nFieldin Culbreth",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas for Reading Raw Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-excel-data",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-excel-data",
    "title": "Pandas for Reading Raw Data",
    "section": "Reading Excel Data",
    "text": "Reading Excel Data\n\nUse the ExcelFile() function from pandas\ncensusEd.xlsx file located at “https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx”\n\n\ned_data = pd.ExcelFile(\"https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx\")\ned_data\n\n&lt;pandas.io.excel._base.ExcelFile at 0x7d6ef1627880&gt;\n\n\n\nUnfortunately, there are different attributes associated with this data object!\n\n\n#ed_data.head(), ed_data.info() won't work!\ntype(ed_data)\n\n\n    pandas.io.excel._base.ExcelFiledef __init__(path_or_buffer, engine: str | None=None, storage_options: StorageOptions | None=None, engine_kwargs: dict | None=None) -&gt; None/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.pyClass for parsing tabular Excel sheets into DataFrame objects.\n\nSee read_excel for more documentation.\n\nParameters\n----------\npath_or_buffer : str, bytes, path object (pathlib.Path or py._path.local.LocalPath),\n    A file-like object, xlrd workbook or openpyxl workbook.\n    If a string or path object, expected to be a path to a\n    .xls, .xlsx, .xlsb, .xlsm, .odf, .ods, or .odt file.\nengine : str, default None\n    If io is not a buffer or path, this must be set to identify io.\n    Supported engines: ``xlrd``, ``openpyxl``, ``odf``, ``pyxlsb``, ``calamine``\n    Engine compatibility :\n\n    - ``xlrd`` supports old-style Excel files (.xls).\n    - ``openpyxl`` supports newer Excel file formats.\n    - ``odf`` supports OpenDocument file formats (.odf, .ods, .odt).\n    - ``pyxlsb`` supports Binary Excel files.\n    - ``calamine`` supports Excel (.xls, .xlsx, .xlsm, .xlsb)\n      and OpenDocument (.ods) file formats.\n\n    .. versionchanged:: 1.2.0\n\n       The engine `xlrd &lt;https://xlrd.readthedocs.io/en/latest/&gt;`_\n       now only supports old-style ``.xls`` files.\n       When ``engine=None``, the following logic will be\n       used to determine the engine:\n\n       - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n         then `odf &lt;https://pypi.org/project/odfpy/&gt;`_ will be used.\n       - Otherwise if ``path_or_buffer`` is an xls format,\n         ``xlrd`` will be used.\n       - Otherwise if ``path_or_buffer`` is in xlsb format,\n         `pyxlsb &lt;https://pypi.org/project/pyxlsb/&gt;`_ will be used.\n\n       .. versionadded:: 1.3.0\n\n       - Otherwise if `openpyxl &lt;https://pypi.org/project/openpyxl/&gt;`_ is installed,\n         then ``openpyxl`` will be used.\n       - Otherwise if ``xlrd &gt;= 2.0`` is installed, a ``ValueError`` will be raised.\n\n       .. warning::\n\n        Please do not report issues when using ``xlrd`` to read ``.xlsx`` files.\n        This is not supported, switch to using ``openpyxl`` instead.\nengine_kwargs : dict, optional\n    Arbitrary keyword arguments passed to excel engine.\n\nExamples\n--------\n&gt;&gt;&gt; file = pd.ExcelFile('myfile.xlsx')  # doctest: +SKIP\n&gt;&gt;&gt; with pd.ExcelFile(\"myfile.xls\") as xls:  # doctest: +SKIP\n...     df1 = pd.read_excel(xls, \"Sheet1\")  # doctest: +SKIP\n      \n      \n\n\n\ned_data.sheet_names\n\n['EDU01A',\n 'EDU01B',\n 'EDU01C',\n 'EDU01D',\n 'EDU01E',\n 'EDU01F',\n 'EDU01G',\n 'EDU01H',\n 'EDU01I',\n 'EDU01J']\n\n\n\nUse .parse() method with sheet to obtain a usual DataFrame\n\n\ned_data.parse('EDU01A').head()\n\n/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n\n\n\n  \n    \n\n\n\n\n\n\nArea_name\nSTCOU\nEDU010187F\nEDU010187D\nEDU010187N1\nEDU010187N2\nEDU010188F\nEDU010188D\nEDU010188N1\nEDU010188N2\n...\nEDU010194N1\nEDU010194N2\nEDU010195F\nEDU010195D\nEDU010195N1\nEDU010195N2\nEDU010196F\nEDU010196D\nEDU010196N1\nEDU010196N2\n\n\n\n\n0\nUNITED STATES\n0\n0\n40024299\n0\n0\n0\n39967624\n0\n0\n...\n0\n0\n0\n43993459\n0\n0\n0\n44715737\n0\n0\n\n\n1\nALABAMA\n1000\n0\n733735\n0\n0\n0\n728234\n0\n0\n...\n0\n0\n0\n727989\n0\n0\n0\n736825\n0\n0\n\n\n2\nAutauga, AL\n1001\n0\n6829\n0\n0\n0\n6900\n0\n0\n...\n0\n0\n0\n7568\n0\n0\n0\n7834\n0\n0\n\n\n3\nBaldwin, AL\n1003\n0\n16417\n0\n0\n0\n16465\n0\n0\n...\n0\n0\n0\n19961\n0\n0\n0\n20699\n0\n0\n\n\n4\nBarbour, AL\n1005\n0\n5071\n0\n0\n0\n5098\n0\n0\n...\n0\n0\n0\n5017\n0\n0\n0\n5053\n0\n0\n\n\n\n\n5 rows × 42 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAlternatively, use the read_excel() function from pandas\nThis reads things in to a standard DataFrame but you have to specify a sheet to read in (or it defaults to the 1st)\ncensusEd.xlsx file located at “https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx”\n\n\ned_data = pd.read_excel(\"https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx\",\n                        sheet_name = 0) #or \"EDU01A\"\ned_data.head()\n\n/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n\n\n\n  \n    \n\n\n\n\n\n\nArea_name\nSTCOU\nEDU010187F\nEDU010187D\nEDU010187N1\nEDU010187N2\nEDU010188F\nEDU010188D\nEDU010188N1\nEDU010188N2\n...\nEDU010194N1\nEDU010194N2\nEDU010195F\nEDU010195D\nEDU010195N1\nEDU010195N2\nEDU010196F\nEDU010196D\nEDU010196N1\nEDU010196N2\n\n\n\n\n0\nUNITED STATES\n0\n0\n40024299\n0\n0\n0\n39967624\n0\n0\n...\n0\n0\n0\n43993459\n0\n0\n0\n44715737\n0\n0\n\n\n1\nALABAMA\n1000\n0\n733735\n0\n0\n0\n728234\n0\n0\n...\n0\n0\n0\n727989\n0\n0\n0\n736825\n0\n0\n\n\n2\nAutauga, AL\n1001\n0\n6829\n0\n0\n0\n6900\n0\n0\n...\n0\n0\n0\n7568\n0\n0\n0\n7834\n0\n0\n\n\n3\nBaldwin, AL\n1003\n0\n16417\n0\n0\n0\n16465\n0\n0\n...\n0\n0\n0\n19961\n0\n0\n0\n20699\n0\n0\n\n\n4\nBarbour, AL\n1005\n0\n5071\n0\n0\n0\n5098\n0\n0\n...\n0\n0\n0\n5017\n0\n0\n0\n5053\n0\n0\n\n\n\n\n5 rows × 42 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nYou can read all sheets with sheet_name = None\nThis gets read into a dictionary!\n\nKeys are the sheet name\nValues are the DataFrames from each sheet\n\n\n\ned_data = pd.read_excel(\"https://www4.stat.ncsu.edu/~online/datasets/censusEd.xlsx\",\n                        sheet_name = None)\ntype(ed_data)\n\n/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n\n\ndict\n\n\n\ned_data.keys()\n\ndict_keys(['EDU01A', 'EDU01B', 'EDU01C', 'EDU01D', 'EDU01E', 'EDU01F', 'EDU01G', 'EDU01H', 'EDU01I', 'EDU01J'])\n\n\n\ned_data.get(\"EDU01A\").head() #get one DataFrame using its key!\n\n\n  \n    \n\n\n\n\n\n\nArea_name\nSTCOU\nEDU010187F\nEDU010187D\nEDU010187N1\nEDU010187N2\nEDU010188F\nEDU010188D\nEDU010188N1\nEDU010188N2\n...\nEDU010194N1\nEDU010194N2\nEDU010195F\nEDU010195D\nEDU010195N1\nEDU010195N2\nEDU010196F\nEDU010196D\nEDU010196N1\nEDU010196N2\n\n\n\n\n0\nUNITED STATES\n0\n0\n40024299\n0\n0\n0\n39967624\n0\n0\n...\n0\n0\n0\n43993459\n0\n0\n0\n44715737\n0\n0\n\n\n1\nALABAMA\n1000\n0\n733735\n0\n0\n0\n728234\n0\n0\n...\n0\n0\n0\n727989\n0\n0\n0\n736825\n0\n0\n\n\n2\nAutauga, AL\n1001\n0\n6829\n0\n0\n0\n6900\n0\n0\n...\n0\n0\n0\n7568\n0\n0\n0\n7834\n0\n0\n\n\n3\nBaldwin, AL\n1003\n0\n16417\n0\n0\n0\n16465\n0\n0\n...\n0\n0\n0\n19961\n0\n0\n0\n20699\n0\n0\n\n\n4\nBarbour, AL\n1005\n0\n5071\n0\n0\n0\n5098\n0\n0\n...\n0\n0\n0\n5017\n0\n0\n0\n5053\n0\n0\n\n\n\n\n5 rows × 42 columns",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas for Reading Raw Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-json-data",
    "href": "01_Programming_in_python/16-Pandas_For_Reading_Data.html#reading-json-data",
    "title": "Pandas for Reading Raw Data",
    "section": "Reading JSON Data",
    "text": "Reading JSON Data\n\nJSON data has a structure similar to a dictionary\n\nKey-value pairs\n\n\n{  \n  {  \n    \"name\": \"Barry Sanders\"  \n    \"games\" : 153  \n    \"position\": \"RB\"  \n  },  \n  {  \n    \"name\": \"Joe Montana\"  \n    \"games\": 192  \n    \"position\": \"QB\"  \n  }  \n}\n\nread_json() function from pandas will work!\nRead in data from URL: “https://api.exchangerate-api.com/v4/latest/USD”\n\n\nurl = \"https://api.exchangerate-api.com/v4/latest/USD\"\nusd_data = pd.read_json(url)\nusd_data.head()\n\n\n  \n    \n\n\n\n\n\n\nprovider\nWARNING_UPGRADE_TO_V6\nterms\nbase\ndate\ntime_last_updated\nrates\n\n\n\n\nUSD\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n1.00\n\n\nAED\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n3.67\n\n\nAFN\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n70.58\n\n\nALL\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n94.12\n\n\nAMD\nhttps://www.exchangerate-api.com\nhttps://www.exchangerate-api.com/docs/free\nhttps://www.exchangerate-api.com/terms\nUSD\n2025-01-03\n1735862402\n396.72\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nEvery API is different so contacting them and returning data differs depending on the website you use. This is an important way we obtain data these days!",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Pandas for Reading Raw Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#writing-functions-recap",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#writing-functions-recap",
    "title": "More on Writing Functions",
    "section": "Writing Functions Recap",
    "text": "Writing Functions Recap\n\nWriting functions is super cool!\nRecall the basic syntax\n\n\ndef func_name(args):\n    \"\"\"\n    Doc string\n    \"\"\"\n    body\n    return object\n\n\nWe saw that there were many ways to set up your function arguments and to call your function\nRemember that variables defined within the function are not generally available outside of the function\n\nThat is, a new symbol table is used when the function is called\nWe can define global variables if we really want to\n\nWe return what we want from the function with return\n\nIf we don’t return anything then None is returned!\n\n\nThe topics we’ll cover in this notebook are: - Packing and unpacking with functions + Catching extra arguments given to a function + Passing your arguments to a function from an object - lambda functions - map(), filter(), and functools.reduce()\nIn a later notebook we’ll talk about how to handle errors or exceptions!",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "More on Writing Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#packing-and-unpacking",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#packing-and-unpacking",
    "title": "More on Writing Functions",
    "section": "Packing and Unpacking",
    "text": "Packing and Unpacking\nReminder: We can pack a list by separating variables to create with commas:\nfirst, second, third = ...\nLet’s look at an example:\n\nanimals = [\"Dog\", \"Cat\", \"Horse\", \"Frog\", \"Cow\", \"Buffalo\", \"Deer\", \"Fish\", \"Bird\", \"Fox\", \"Racoon\"]\nshort_animals = animals[:3]\n\nfirst, second, third = short_animals\nprint(first + \" \" + second + \" \" + third)\n\nDog Cat Horse\n\n\nWe saw that we can pack leftover elements into a list using *variable:\n\nfirst, second, third, *other = animals\nprint(first + \" \" + second + \" \" + third)\nprint(other)\n\nDog Cat Horse\n['Frog', 'Cow', 'Buffalo', 'Deer', 'Fish', 'Bird', 'Fox', 'Racoon']\n\n\n\n\nUnlimited Positional Arguments\n\nThis idea can be used when writing a function!\nIn this case we define an argument to our function with *variable\nThis allows us to pass unlimited positional arguments to our function (variadic arguments)\nThe inputs are handled as a tuple in the function!\n\nLet’s write a silly function to print out all arguments passed via this idea\n\ndef basic_print(*args):\n  print(type(args))\n  print(args)\n  return None\n\nWe can pass this function as many things as we’d like and it will be accessible within the function body as a tuple. We can see this as the printed values are surrounded by ( and ), which implies we are printing a tuple!\n\nbasic_print(\"hi\", [\"a list\", \"how fun\"], 3, 10)\n\n&lt;class 'tuple'&gt;\n('hi', ['a list', 'how fun'], 3, 10)\n\n\nAs tuples are iterable, we can iterate across these elements via a loop!\n\ndef basic_print_elements(*args):\n  for i in args:\n    print(type(i),i)\n  return None\n\n\nbasic_print_elements(\"hi\", [\"a list\", \"how fun\"], 3, 10)\n\n&lt;class 'str'&gt; hi\n&lt;class 'list'&gt; ['a list', 'how fun']\n&lt;class 'int'&gt; 3\n&lt;class 'int'&gt; 10\n\n\nLet’s define a function that takes in as many 1D numpy arrays or pandas Series the user would like and returns the means for each input.\nWe’ll also take an argument for the number of decimal places to return for the means.\n\ndef find_means(*args, decimals = 4):\n    \"\"\"\n    Assume that args will be a bunch of numpy arrays (1D) or pandas series\n    Return the mean of each, rounded to `decimals` places\n    \"\"\"\n    means = []\n    for x in args: #iterate over the tuple values\n        means.append(np.mean(x).round(decimals))\n    return means\n\n\nCreate some data with numpy to send to this\n\n\nimport numpy as np\nfrom numpy.random import default_rng\nrng = default_rng(3) #set seed to 3\n\n#generate a few means from standard normal data\nn5 = rng.standard_normal(5)       #sample size of 5\nn25 = rng.standard_normal(25)     #sample size of 25\nn100 = rng.standard_normal(100)   #sample size of 100\nn1000 = rng.standard_normal(1000) #sample size of 1000\n\nLet’s pass these to our function!\n\nfind_means(n5, n25, n100, n1000, decimals = 2)\n\n[-0.22, 0.11, -0.01, 0.04]\n\n\nAwesome! This gives us a lot more functionality with our function writing.\n\n\n\nUnlimited Keyword Arguments\n\nYou can also pass unlimited keyword arguments if you define the arg with a **\nHandled as a dictionary in the function\n\nLet’s write a basic function to print out the keywords with their values.\n\ndef print_key_value_pairs(**kwargs):\n    \"\"\"\n    key word args can be anything\n    \"\"\"\n    print(type(kwargs), kwargs)\n    for x in kwargs:\n        print(x + \" : \" + str(kwargs.get(x))) #cast the value to a string for printing\n\nNow we pass as many named arguments as we’d like!\n\nprint_key_value_pairs(\n  name = \"Justin\",\n  job = \"Professor\",\n  phone = 9195150637)\n\n&lt;class 'dict'&gt; {'name': 'Justin', 'job': 'Professor', 'phone': 9195150637}\nname : Justin\njob : Professor\nphone : 9195150637",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "More on Writing Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#unpacking-arguments",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#unpacking-arguments",
    "title": "More on Writing Functions",
    "section": "Unpacking Arguments",
    "text": "Unpacking Arguments\n\nSuppose we want to call our function but our arguments are stored in a list or tuple\n\nWe’ll do this a bit when we do our machine learning models!\n\nWe can unpack this list or tuple to be our function arguments by calling our function in a particular way.\n\n\n#We want to call our find_means function with these arguments\ncall_args = [n5, n25, n100, n1000]\n\n\nCall the function using *call_args (unpacking)\n\n\nfind_means(*call_args, decimals = 3)\n\n[-0.223, 0.114, -0.014, 0.04]\n\n\nNice! Now we can more easily call our function too!\n\nWe can do the same thing with our keyword arguments.\nSuppose our keyword arguments are stored in a dictionary\nCan call the function using **kw_call_args (unpacking)\n\nDefine a quick function.\n\ndef print_items(name, job, number):\n  print(\"Name is: \", name)\n  print(\"Job is: \", job)\n  print(\"Phone number is: \", number)\n  return\n\nCreate a dictionary with key-value pairs corresponding to our inputs.\n\nkw_call_args = {\"name\": \"Justin Post\", \"job\": \"Professor\", \"number\": \"9195150637\"}\nkw_call_args\n\n{'name': 'Justin Post', 'job': 'Professor', 'number': '9195150637'}\n\n\nCall our function using ** with our dictionary!\n\nprint_items(**kw_call_args)\n\nName is:  Justin Post\nJob is:  Professor\nPhone number is:  9195150637\n\n\n\nPassing named and unnamed arguments can both be done at once!\nRecall our find_means function inputs: def find_means(*args, decimals = 4):\n\n\ndec_dictionary = {\"decimals\": 6}\nfind_means(*call_args, **dec_dictionary)\n\n[-0.223413, 0.114454, -0.014443, 0.039762]",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "More on Writing Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#lambda-functions",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#lambda-functions",
    "title": "More on Writing Functions",
    "section": "Lambda Functions",
    "text": "Lambda Functions\nAnother thing that comes up a lot is that we need to create a quick function for a single purpose that we don’t want to reuse for later.\nRather than define a function and storing it as an object the way we’ve been doing it, we can create a lambda function (also sometimes called an in-line function or an anonymous function)\n\nUse keyword lambda\nDefine arguments followed by a :\nGive the action for the function to perform\n\nSyntax requires a single line. Cannot use return or some other keywords\n\n\n\nsquare_it = lambda x : x**2\nsquare_it(10)\n\n100\n\n\n\nsquare_then_add = lambda x, y : x**2 + y\nsquare_then_add(10, 5)\n\n105\n\n\n\nCan still define the arguments in many ways\n\n\nmy_print = lambda x, y = \"ho\": print(x, y)\nmy_print(\"hi\")\n\nhi ho\n\n\n\nCan create the arbitrary positional arguments too (but then why would we be using lambda function!? This is just to show the functionality.)\n\n\nmy_print = lambda *x: [print(\"Input: \" + str(z)) for z in x]\nmy_print(\"hi\", \"ho\", \"off\", \"to\", \"work\", \"we\", \"go\")\n\nInput: hi\nInput: ho\nInput: off\nInput: to\nInput: work\nInput: we\nInput: go\n\n\n[None, None, None, None, None, None, None]\n\n\nNow, saving the function function in an object is really kind of counter to the point of an anonymous (lambda) function. We don’t usually save these for later use! We’ll see many uses for lambda functions. Let’s cover one of those here.\n\nmap()\nUsing lambda functions comes up a lot in the map/reduce idea. This is important for some of the legacy big data operations that we’ll do!\nMap/reduce idea: - Apply (or map) a function to each element of an iterable object - Combine (or reduce) the results where able\nExample: Counting words - Want to take a list of words and create a tuple with the word and the value 1 - Syntax for map: + map(function, object_to_apply_function_to)\n\nres = map(\n    lambda word: (word, 1),\n    [\"these\", \"are\", \"my\", \"words\", \"these\", \"words\", \"rule\"]\n    )\n\nSimilar to other functions like range or zip, we don’t get back the actual object we think we would. Instead we get a map object that can be used to find the mapped values.\n\nprint(type(res))\nres\n\n&lt;class 'map'&gt;\n\n\n&lt;map at 0x7f615a9b1990&gt;\n\n\nWe can convert the map object to a list using list()\n\nlist(res)\n\n[('these', 1),\n ('are', 1),\n ('my', 1),\n ('words', 1),\n ('these', 1),\n ('words', 1),\n ('rule', 1)]\n\n\nLet’s return the square of some values without defining a square function via map()\n\nmap(lambda r: r **2, range(0,5))\n\n&lt;map at 0x7f615a9b39d0&gt;\n\n\n\nlist(map(lambda r: r **2, range(0,5)))\n\n[0, 1, 4, 9, 16]\n\n\nNote: this can equivalently be done using a list comprehension!\n\n[r ** 2 for r in range(0,5)]\n\n[0, 1, 4, 9, 16]\n\n\nAnother example of using map with a lambda function might be to quickly uppercase a list of strings.\n\nlist(map(lambda x: x.upper(), ['cat', 'dog', 'wolf', 'bear', 'parrot']))\n\n['CAT', 'DOG', 'WOLF', 'BEAR', 'PARROT']\n\n\nAgain, this could be done with a list comprehension!\n\n[x.upper() for x in ['cat', 'dog', 'wolf', 'bear', 'parrot']] #equivalent\n\n['CAT', 'DOG', 'WOLF', 'BEAR', 'PARROT']\n\n\nOne interesting use of a lambda function is through the creation of a function generator\n\nCreate a function that generates functions!\nHere a function to raise a number to a given power\n\n\ndef raise_power(k):\n    return lambda r: r ** k\n\n\nsquare = raise_power(2) #creates a function!\nsquare(10)\n\n100\n\n\n\ncube = raise_power(3)\ncube(5)\n\n125\n\n\nWe can put this together with our packing idea and map!\n\nident, square, cube = map(raise_power, range(1,4))\nident(4)\n\n4\n\n\n\nsquare(4)\n\n16\n\n\n\ncube(4)\n\n64\n\n\n\n\n\nfilter()\n\nLambda functions can be used with filter()\n\nfilter() takes a predicate (statement to return what you want) as the first arg and an iterable as the second\nWe can give the first argument as a lambda function\n\n\nHere we want to return only vowels from a string (an iterable).\n\nfilter(lambda x: x in \"aeiou\", \"We want to return just the vowels.\")\n\n&lt;filter at 0x7f615a9b3f40&gt;\n\n\n\n#return in list form!\nlist(filter(lambda x: x in \"aeiou\", \"We want to return just the vowels.\"))\n\n['e', 'a', 'o', 'e', 'u', 'u', 'e', 'o', 'e']\n\n\nEquivalent to doing a list comprehension with an if in there!\n\n[x for x in \"We want to return just the vowels.\" if x in \"aeiou\"] #equivalent\n\n['e', 'a', 'o', 'e', 'u', 'u', 'e', 'o', 'e']\n\n\nThis time let’s use filter to only return even numbers from an iterable object.\n\nRecall the mod operator %, which returns the remainder\nA number is even if, when we divide by 2, we get 0 as the remainder\n\n\nlist(filter(lambda x: (x % 2) != 0, range(0, 10)))\n\n[1, 3, 5, 7, 9]\n\n\nEquivalent to this list comprehension with an if:\n\n[x for x in range(0, 10) if (x % 2) != 0]\n\n[1, 3, 5, 7, 9]\n\n\n\n\n\nfunctools.reduce()\nLambda functions can be used with functools.reduce()\n\nreduce() takes in a function of two variables and an iterable\nIt applies the function repetitively over the iterable, and returns the result\n\nHere, we’ll find the cumulative sum of a bunch of numbers (given as an iterable)\n\nfrom functools import reduce\nreduce(lambda x, y: x + y, range(1,11)) # sum first 10 numbers\n\n55\n\n\nHere, reduce() works like this: - Takes the first two arguments of the iterable (1 and 2) and adds them - Takes the result of that (3) and adds it to the next argument of the iterable (3) - Repeats until the iterable is exhausted\nAgain, we could do this kind of thing with a list comprehension. Here we just use the sum function on the result.\n\nsum([x for x in range(1,11)])\n\n55\n\n\nWe can also provide an initial value to reduce() to start the computation at. Here we supply 45.\n\n#add an initial value to the computation\nreduce(lambda x, y: x + y, range(1,11), 45) # sum first 10 numbers + 45\n\n100\n\n\nOk, that’s a bit silly. We can do more interesting things with this. For instance, here we write a reduce function to find the largest value in a list.\n\n#create a list of numbers to find the max of\nmy_list = [53, 13, 103, 2, 15, -10, 201, 6]\nreduce(lambda x, y: x if x &gt; y else y, my_list)\n\n201\n\n\nHow does that work? - Take x (53) and y (13), if x &gt; y take x (53), otherwise take y (13) - With the result of that as x, take y as the next value in the iterable (103) - Repeat that step. Here it would keep 103 since it is larger - Keep going until the iterable is exhausted\nThis works with a starting value as well!\n\nreduce(lambda x, y: x if x &gt; y else y, my_list, 500)\n\n500",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "More on Writing Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/18-More_Function_Writing.html#quick-video",
    "href": "01_Programming_in_python/18-More_Function_Writing.html#quick-video",
    "title": "More on Writing Functions",
    "section": "Quick Video",
    "text": "Quick Video\nThis video shows an example of writing more involved functions including the use of lambda and map(). Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=55af749d-fd9d-4b0d-b2b4-b10301614c9c&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "More on Writing Functions"
    ]
  },
  {
    "objectID": "01_Programming_in_python/20-Plotting_pandas.html#barplots-with-pandas",
    "href": "01_Programming_in_python/20-Plotting_pandas.html#barplots-with-pandas",
    "title": "Plotting with pandas",
    "section": "Barplots with pandas",
    "text": "Barplots with pandas\nWe saw the barplot for summarizing categorical data. We’ll cover two different methods to create bar plots in pandas:\n\n.plot.bar() method on a series or dataframe\n.plot() method with kind = 'bar' specified\n\n\ntable = sub_titanic_data.embarkedC.value_counts()\nprint(type(table))\ntable\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\n\ncount\n\n\nembarkedC\n\n\n\n\n\nSouthampton\n914\n\n\nCherbourg\n270\n\n\nQueenstown\n123\n\n\n\n\ndtype: int64\n\n\nNote that this is a pandas series. We can use the .plot.bar() method on this series to get a bar plot.\n\ntable.plot.bar()\n\n\n\n\n\n\n\n\nWe can then apply the matplotlib functionality to update/modify the plot (notice we already read in matplotlib.pyplot as plt.\n\ntable.plot.bar()\nplt.xticks(rotation = 0)\n\n(array([0, 1, 2]),\n [Text(0, 0, 'Southampton'),\n  Text(1, 0, 'Cherbourg'),\n  Text(2, 0, 'Queenstown')])\n\n\n\n\n\n\n\n\n\nAlternatively, we can use the slightly more flexible .plot() method on a series where we specify the kind= of the plot to create.\n\ntable.plot(kind = \"bar\", rot = 0) #can use additional arg rather than additional function call\n\n\n\n\n\n\n\n\n\nWhere we really gain is when trying to bring in a multivariate relationship\n\nFor instance, we can color the bars by another categorical variable in the DataFrame pretty easily!\n\nFirst, create the contingency table for two variables (remember this returns a DataFrame)\n\ntable = pd.crosstab(sub_titanic_data[\"embarkedC\"], sub_titanic_data[\"survivedC\"])\nprint(type(table))\ntable\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n  \n    \n\n\n\n\n\nsurvivedC\nDied\nSurvived\n\n\nembarkedC\n\n\n\n\n\n\nCherbourg\n120\n150\n\n\nQueenstown\n79\n44\n\n\nSouthampton\n610\n304\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nNow let’s use the .plot.bar() method on this DataFrame with the stacked = True argument.\n\ntable.plot.bar(stacked = True, rot = 0)\n\n\n\n\n\n\n\n\nWe can do this with the .plot() method as well.\n\ntable.plot(stacked = True, kind = \"bar\", rot = 0)\n\n\n\n\n\n\n\n\nIf we want side-by-side bar plots, we can just remove the stacked = True argument.\n\ntable.plot.bar(rot = 0)",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Plotting with `pandas`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/20-Plotting_pandas.html#plotting-numeric-variables",
    "href": "01_Programming_in_python/20-Plotting_pandas.html#plotting-numeric-variables",
    "title": "Plotting with pandas",
    "section": "Plotting Numeric Variables",
    "text": "Plotting Numeric Variables\nRecall: Numeric variable have entries that are a numerical value where math can be performed\nGoal: describe the shape, center, and spread of the distribution\n\nShape can be described well via a histogram or density plot\nBoxplots provide a good summary of the distribution as well\n\n\n\nHistogram with pandas\nHistogram - Bin data to show distribution of observations - Done via .plot.hist() or .plot(kind = \"hist\") method on a series or data frame - A .hist() method also exists!\nFirst, the .plot.hist() method on a series (we’ll also fix it up a bit using matplotlib.pyplot functionality.\n\nsub_titanic_data[\"age\"].plot.hist()\nplt.xlabel(\"Age\")\nplt.title(\"Histogram of Age for Titanic Passengers\")\n\nText(0.5, 1.0, 'Histogram of Age for Titanic Passengers')\n\n\n\n\n\n\n\n\n\n\nSpecify # of bins with bins =\nNote we also return the series in a different way here (just to show you can use either)\n\n\n#can add label/title here (xlabel doesn't seem to work as intended...)\n#instead we'll use the .set() method on the histogram to set the xlabel\nsub_titanic_data.age.plot.hist(bins = 20, title = \"Histogram of Age for Titanic Passengers\") \\\n    .set(xlabel = \"Age\")\n\n\n\n\n\n\n\n\n\nOverlaying Two Histograms\n\nTo overlay two histograms on the same graph, create two histograms and use alpha = 0-1 value. This sets the transparency.\n\nalpha = 1 is not transparent at all\nalpha = 0 is completely transparent\n\n\nLet’s create histograms of age for those that Survived and those that Died. - We should also set up the bins manually so they are the same bin widths and locations (for better comparison) - bins can be specified via the bins = argument\n\nbin_ends = 10\nbins = [i*max(sub_titanic_data.age)/bin_ends for i in range(0, bin_ends + 1)]\nprint(bins)\n\n[0.0, 8.0, 16.0, 24.0, 32.0, 40.0, 48.0, 56.0, 64.0, 72.0, 80.0]\n\n\n\nObtain subsets of data needed\n\n\nage_died = sub_titanic_data.loc[sub_titanic_data.survivedC == \"Died\", \"age\"] #series for died\nage_survived = sub_titanic_data.loc[sub_titanic_data.survivedC == \"Survived\", \"age\"] #series for survived\n\nCreate the plot using the .plot.hist() method. By creating two plots in the same cell, they will be overlayed. - Notice the use of label() to automatically create a legend (similar to what we did with matplotlib\n\nage_died.plot.hist(bins = bins, alpha = 0.5, label = \"Died\",\n                   title = \"Ages for those that survived vs those that died\") \\\n                   .set(xlabel = \"Age\")\nage_survived.plot.hist(bins = bins, alpha = 0.5, label = \"Survived\")\nplt.legend()\n\n\n\n\n\n\n\n\n\npandas will automatically overlay data from different columns of the same data frame\nThat is, if we use the plot.hist() method on a data frame with two numeric variables, it will plot both of those on the same plot\n\nTo use that here we need to make that kind of data frame…\nNeed two columns, one representing ages for those that survived and one for those that died\n\n\n\nage_died = sub_titanic_data.loc[sub_titanic_data.survivedC == \"Died\", \"age\"] #809 values\nage_survived = sub_titanic_data.loc[sub_titanic_data.survivedC == \"Survived\", \"age\"] #500 values\n\n\nNote the difference in the number of observations! This means that putting them together into a data frame isn’t super seamless.\n\n\ntemp = pd.DataFrame(zip(age_died, age_survived), columns = [\"Died\", \"Survived\"])\nprint(temp.shape)\n#only has 500 rows instead of 809!\ntemp.plot.hist(alpha = 0.5)\n\n(500, 2)\n\n\n\n\n\n\n\n\n\n\nHow do we fix that?\n\nWe can fill in NaN values for the shorter series so they end up the same length.\n\n\n\nage_survived = pd.concat([age_survived, pd.Series([np.nan for _ in range(308)])])\nage_survived\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n29.0000\n\n\n1\n0.9167\n\n\n5\n48.0000\n\n\n6\n63.0000\n\n\n8\n53.0000\n\n\n...\n...\n\n\n303\nNaN\n\n\n304\nNaN\n\n\n305\nNaN\n\n\n306\nNaN\n\n\n307\nNaN\n\n\n\n\n808 rows × 1 columns\ndtype: float64\n\n\n\nNow we can zip these together into a data frame and plot as we’d like!\n\n\nplotting_df = pd.DataFrame(zip(age_died, age_survived),\n                      columns = [\"Died\", \"Survived\"])\nprint(plotting_df.shape)\nplotting_df.plot.hist(alpha = 0.5, title = \"Ages for those that survived vs those that died\") \\\n    .set(xlabel = \"Age\")\n\n(808, 2)\n\n\n\n\n\n\n\n\n\n\n\nSide-by-side Histograms\n\nCan place two graphs next to each other with .hist() method (notice this is a different method!)\n\nSpecify a column variable and a by variable\n\nThese don’t have the same bin widths\n\n\nsub_titanic_data.hist(column = \"age\", by = \"survivedC\")\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data.hist(column = \"age\", by = \"survivedC\")\n\n\narray([&lt;Axes: title={'center': 'Died'}&gt;,\n       &lt;Axes: title={'center': 'Survived'}&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\nWe could also use the .groupby() functionality but the result is a bit subpar as it doesn’t label the graphs.\n\n\nsub_titanic_data[[\"age\", \"survivedC\"]].groupby(\"survivedC\").hist()\n\nFutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  sub_titanic_data[[\"age\", \"survivedC\"]].groupby(\"survivedC\").hist()\n\n\n\n\n\n\n\n\n\n0\n\n\nsurvivedC\n\n\n\n\n\nDied\n[[Axes(0.125,0.11;0.775x0.77)]]\n\n\nSurvived\n[[Axes(0.125,0.11;0.775x0.77)]]\n\n\n\n\ndtype: object\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKernel smoother with pandas\n\nKernel Smoother - Smoothed version of a histogram\n\n‘Kernel’ determines weight given to nearby points\n\nUse .plot.density() or plot(kind = \"density\") method\nbw_method = # specifies how ‘smooth’ you want the graph to be\n\nsmaller values imply using a smaller bandwidth (more variability)\nlarger values imply using a larger bandwidth (more smooth)\n\n\n\n\nsub_titanic_data.age.plot.density(bw_method = 0.1, label = \"bw = 0.1\",\n                                  title = \"Density Plots of Age for Titanic Passengers\")\nsub_titanic_data.age.plot.density(bw_method = 0.25, label = \"bw = 0.25\")\nsub_titanic_data.age.plot.density(bw_method = 0.5, label = \"bw = 0.5\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\nBoxplots with pandas\n\nBoxplot - Provides the five number summary in a graph\n\nMin, Q1, Median, Q3, Max\n\nOften show possible outliers as well\n\nUse .plot.box() or plot(kind = \"box\") method\nA .boxplot() method also exists!\n\n\nFirst the .plot.box() method on a series\n\nsub_titanic_data.age.plot.box()\n\n\n\n\n\n\n\n\n\nFine.. but usually we want to compare these boxplots across another variable. To do this the .boxplot() method on a data frame is very useful!\nSimilar to the .hist() method we specify a column and by variable\n\n\nsub_titanic_data.boxplot(column = [\"age\"], by = \"survivedC\")\n\n\n\n\n\n\n\n\n\n\n\nScatter Plots with pandas\n\nScatter Plot - graphs points corresponding to each observation\n\nUse .plot.scatter() or plot(kind = \"scatter\") method on a data frame with x =, and y =\n\n\n\nsub_titanic_data.plot.scatter(x = \"age\", y = \"fare\", title = \"Scatter plots rule!\")\n\n\n\n\n\n\n\n\n\nEasy to modify! Check the help for arguments (specifically the keyword arguments that get passed to .plot()) but we can specify different marker values, a title, and more!\n\n\n#c = color, marker is a matplotlib option\nsub_titanic_data.plot.scatter(x = \"age\", y = \"fare\", c = \"Red\", marker = \"v\", title = \"Oh, V's!\")\n\n\n\n\n\n\n\n\n\nWe can easily modify aspects of the plot based on a variable as well!\nThis is great as it allows us to bring a third varaible in\nHere we color by a category variable\n\n\n#s for size (should be a numeric column), cmap can be used with c for specifying color scales\nsub_titanic_data.plot.scatter(x = \"age\", y = \"fare\", c = \"survivedC\", cmap = \"viridis\", s = 10)\n\n\n\n\n\n\n\n\n\n\nMatrix of Scatter Plots\n\n.plotting.scatter_matrix() function will produce basic graphs showing relationships!\nHere we grab the numeric variables from the data frame\n\n\npd.plotting.scatter_matrix(sub_titanic_data[[\"age\", \"fare\", \"survived\", \"sibsp\"]])\n\narray([[&lt;Axes: xlabel='age', ylabel='age'&gt;,\n        &lt;Axes: xlabel='fare', ylabel='age'&gt;,\n        &lt;Axes: xlabel='survived', ylabel='age'&gt;,\n        &lt;Axes: xlabel='sibsp', ylabel='age'&gt;],\n       [&lt;Axes: xlabel='age', ylabel='fare'&gt;,\n        &lt;Axes: xlabel='fare', ylabel='fare'&gt;,\n        &lt;Axes: xlabel='survived', ylabel='fare'&gt;,\n        &lt;Axes: xlabel='sibsp', ylabel='fare'&gt;],\n       [&lt;Axes: xlabel='age', ylabel='survived'&gt;,\n        &lt;Axes: xlabel='fare', ylabel='survived'&gt;,\n        &lt;Axes: xlabel='survived', ylabel='survived'&gt;,\n        &lt;Axes: xlabel='sibsp', ylabel='survived'&gt;],\n       [&lt;Axes: xlabel='age', ylabel='sibsp'&gt;,\n        &lt;Axes: xlabel='fare', ylabel='sibsp'&gt;,\n        &lt;Axes: xlabel='survived', ylabel='sibsp'&gt;,\n        &lt;Axes: xlabel='sibsp', ylabel='sibsp'&gt;]], dtype=object)",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Plotting with `pandas`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/20-Plotting_pandas.html#quick-video",
    "href": "01_Programming_in_python/20-Plotting_pandas.html#quick-video",
    "title": "Plotting with pandas",
    "section": "Quick Video",
    "text": "Quick Video\nThis video shows an example of using pandas for plotting. Remember to pop the video out into the full player.\nThe notebook written in the video is available here.\n\nfrom IPython.display import IFrame\nIFrame(src=\"https://ncsu.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=f4e2bc16-1757-4f1e-8df9-b103016c97d2&autoplay=false&offerviewer=true&showtitle=true&showbrand=true&captions=false&interactivity=all\", height=\"405\", width=\"720\")",
    "crumbs": [
      "Home",
      "Topic 2: Dealing with Data in `python`",
      "Plotting with `pandas`"
    ]
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#basics-of-python",
    "href": "01_Programming_in_python/Learning_Python.html#basics-of-python",
    "title": "Week 1",
    "section": "Basics of python",
    "text": "Basics of python\nCreate a list!\n\n[1, \"a\", 3, 60]\n\n[1, 'a', 3, 60]\n\n\nSave this as an object!\n\nmyList = [1, \"a\", 3, 60]\nprint(myList)\nmyList\n\n[1, 'a', 3, 60]\n\n\n[1, 'a', 3, 60]\n\n\n\nmyList + [2, 4, 5]\n\n[1, 'a', 3, 60, 2, 4, 5]\n\n\n\nmyList * 4\n\n[1, 'a', 3, 60, 1, 'a', 3, 60, 1, 'a', 3, 60, 1, 'a', 3, 60]"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#markdown",
    "href": "01_Programming_in_python/Learning_Python.html#markdown",
    "title": "Week 1",
    "section": "Markdown",
    "text": "Markdown\nWe’ll add some HTML widgets and play around with that.\n\n#reading in modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\n\nLet’s generate 100 random data values using numpy.\n\nrng = np.random.default_rng(12)\nvals = rng.standard_normal(100)\nvals\n\narray([-6.82677987e-03,  1.04614329e+00,  7.41588421e-01,  7.23956542e-01,\n        1.61877622e+00, -1.20555814e+00, -6.26955471e-01, -1.32066321e+00,\n       -1.07752508e-01,  9.98763655e-01, -2.19478863e-02,  4.95880066e-01,\n       -1.91076866e+00,  1.47064166e-01, -9.06943251e-01,  1.77538939e+00,\n        8.86849076e-01,  9.49349483e-01, -5.78549625e-02,  6.12862274e-01,\n        6.57890162e-01, -3.44402666e-01, -4.97372035e-01, -1.14772783e-01,\n       -6.05452009e-01, -5.94339418e-01, -2.83375376e-01, -7.28417727e-01,\n        7.66327786e-01, -1.59608633e+00,  8.23562129e-01, -6.25566470e-01,\n       -5.45939956e-01, -1.35084714e+00, -1.44242119e-01, -2.47661509e-01,\n        1.91455831e-01, -5.33774296e-01,  9.37561793e-02,  1.81969184e+00,\n        4.08999694e-01, -5.73690037e-01,  9.53109595e-01, -1.28801134e-01,\n        5.93874468e-01,  6.12747429e-01, -3.91065717e-01, -1.93028750e+00,\n       -3.47653623e-01,  5.51455424e-01, -3.80110407e-01,  4.38969315e-01,\n        9.79540372e-01, -5.44113415e-01,  1.23135175e+00,  1.62180446e+00,\n        1.07904594e+00,  1.16546284e+00,  1.09679645e+00,  2.25453906e+00,\n        1.85860905e-01,  2.08408285e-03,  6.03107305e-01, -9.08195119e-01,\n       -1.55317676e+00, -8.82007973e-01,  3.72565350e-01,  4.73053341e-01,\n       -1.53635872e+00, -1.88346371e+00, -3.16142243e-01, -1.88056516e-01,\n       -4.92008330e-02,  6.71362447e-01,  1.22883418e+00,  2.30896430e-01,\n        6.12684704e-01, -1.09520169e+00, -1.17630801e+00,  2.36238334e-01,\n        6.64663103e-01,  7.26243456e-01,  5.92741991e-01,  7.84173709e-01,\n        8.09828030e-01, -1.75218536e+00, -7.34165769e-01,  4.55159742e-01,\n        5.96671965e-01, -1.51209241e+00,  1.17306137e+00, -4.38831924e-01,\n       -2.32424749e-01,  2.73818645e-01,  7.49635232e-01, -1.43311699e+00,\n        9.89912577e-01,  7.72934677e-02, -7.70813495e-01, -2.75273142e-01])\n\n\n\nplt.hist(vals)\n\n(array([ 8.,  6.,  6., 17., 15., 12., 24.,  7.,  4.,  1.]),\n array([-1.9302875 , -1.51180484, -1.09332219, -0.67483953, -0.25635687,\n         0.16212578,  0.58060844,  0.99909109,  1.41757375,  1.8360564 ,\n         2.25453906]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nplt.hist(vals, bins = 12)\n\n(array([ 5.,  6.,  5., 11., 13., 12., 11., 19., 11.,  2.,  4.,  1.]),\n array([-1.9302875 , -1.58155195, -1.2328164 , -0.88408086, -0.53534531,\n        -0.18660976,  0.16212578,  0.51086133,  0.85959687,  1.20833242,\n         1.55706797,  1.90580351,  2.25453906]),\n &lt;BarContainer object of 12 artists&gt;)\n\n\n\n\n\n\n\n\n\n\ndef myPlot(bins):\n    plt.hist(vals, bins)\n\nmyPlot(4)\n\n\n\n\n\n\n\n\n\nwidgets.interactive(myPlot, bins =(1,20))"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#strings-practice",
    "href": "01_Programming_in_python/Learning_Python.html#strings-practice",
    "title": "Week 1",
    "section": "Strings Practice",
    "text": "Strings Practice\nHere we’ll look at some quick string methods and functions including string formatting!\n\nString formatting\n\nOne way to do formatting is to use {} within a string and then use the .format() method. Let’s create a string that we can place values into. Something like this:\n“The probability of being less than or equal to 2 for a normal distribution with mean 4 and standard deviation 1 is …”\n\nprob_string = \"The probability of being less than or equal to {0} for a normal distribution with mean {1} and standard deviation {2} is {3}\"\n\n\nmean = 1\nstd = 2\ny = 1.55\n\n'The probability of being less than or equal to 1.55 for a normal distribution with mean 1 and standard deviation 2 is 0.6083'\n\n\nNow pull the scipy.stats stuff from above and insert the numbers using the .format() method.\n\nimport scipy.stats as stats\nstats.norm.cdf(y, mean, std)\n\n0.6083418808463948\n\n\n\nprob_string.format(y, mean, std, round(stats.norm.cdf(y, mean, std), 4))\n\n'The probability of being less than or equal to 1.55 for a normal distribution with mean 1 and standard deviation 2 is 0.6083'\n\n\n\nLet’s look at splitting up a big string of text into ‘words’. This is often a first step done when doing a basic sentiment analysis of some text.\n\nFirst we need a string of text to break up. Project Gutenberg has some classic open works that you can get the entire text from. Here we’ll pull from a cookbook called Practical Vegetarian Cookery. We’ll just grab two recipes:\n\nWINTER VEGETABLE PIE.\n\n\nPlace in baking dish, slices of cold boiled potatoes, onions, celery, and carrot, then add one scant cupful of stewed tomatoes and one half can of peas. Cover with stock, thickened to a gravy with butter and flour, cover with plain crust, and bake. A pie of this nature can be made with a great variety of ingredients; apples, boiled chestnuts, onions, and potatoes make a good combination. Rice, with a grating of cheese, celery, onion, and tomato, another variety.\n\n\nVEGETABLE HASH.\n\n\nOf cooked and chopped vegetables, use one carrot, one blood beet, two turnips, two quarts of finely sliced potatoes, one onion, and a stalk of celery; one sprig of parsley; put them in a stew pan, cover tight, and set in the oven. When thoroughly heated pour over a gravy of drawn butter and cream. Stir together and serve.\n\nLet’s create a string with these and count variables that occur.\n\nrecipe = \"\"\"\nWINTER VEGETABLE PIE.\n\nPlace in baking dish, slices of cold boiled potatoes, onions, celery,\nand carrot, then add one scant cupful of stewed tomatoes and one half\ncan of peas. Cover with stock, thickened to a gravy with butter and\nflour, cover with plain crust, and bake. A pie of this nature can be\nmade with a great variety of ingredients; apples, boiled chestnuts,\nonions, and potatoes make a good combination. Rice, with a grating of\ncheese, celery, onion, and tomato, another variety.\n\nVEGETABLE HASH.\n\nOf cooked and chopped vegetables, use one carrot, one blood beet, two\nturnips, two quarts of finely sliced potatoes, one onion, and a stalk\nof celery; one sprig of parsley; put them in a stew pan, cover tight,\nand set in the oven. When thoroughly heated pour over a gravy of drawn\nbutter and cream. Stir together and serve.\n\"\"\"\nrecipe\n\n'\\nWINTER VEGETABLE PIE.\\n\\nPlace in baking dish, slices of cold boiled potatoes, onions, celery,\\nand carrot, then add one scant cupful of stewed tomatoes and one half\\ncan of peas. Cover with stock, thickened to a gravy with butter and\\nflour, cover with plain crust, and bake. A pie of this nature can be\\nmade with a great variety of ingredients; apples, boiled chestnuts,\\nonions, and potatoes make a good combination. Rice, with a grating of\\ncheese, celery, onion, and tomato, another variety.\\n\\nVEGETABLE HASH.\\n\\nOf cooked and chopped vegetables, use one carrot, one blood beet, two\\nturnips, two quarts of finely sliced potatoes, one onion, and a stalk\\nof celery; one sprig of parsley; put them in a stew pan, cover tight,\\nand set in the oven. When thoroughly heated pour over a gravy of drawn\\nbutter and cream. Stir together and serve.\\n'\n\n\nReplace the \\n, ,, and . values with a space instead. Lowercase everything with the .lower() method and then .strip() to remove the excess whitespace. Finally, the .split() method gives us a list with each element being a word from the document.\n\nrecipe_split = recipe.replace(\"\\n\", \" \") \\\n  .replace(\".\", \" \") \\\n  .replace(\",\", \" \") \\\n  .replace(\";\", \" \") \\\n  .lower() \\\n  .strip() \\\n  .split(\" \")\n\nNow we can see how often different words come up.\n\ntype(recipe_split)\nrecipe_split.count(\"potatoes\")\n\n3"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#math-types-including-booleans",
    "href": "01_Programming_in_python/Learning_Python.html#math-types-including-booleans",
    "title": "Week 1",
    "section": "Math Types (Including Booleans)",
    "text": "Math Types (Including Booleans)\nRemember that things can’t be stored precisely so you may sometimes run into issues where two things should be equal but don’t resolve as equal.\n\nfrom math import sqrt\nsqrt(2)**2 == 2\n\nFalse\n\n\nUse the isclose() from numpy to check things like this.\n\nimport numpy as np\nnp.isclose(sqrt(2)**2, 2)\n\nTrue\n\n\nWe should have an idea about inf, -inf, and nan values in python. nan values can be returned when a mathematical function is used outside of its domain for instance. Here is the norm.ppf() function. This returns the value from the standard normal with a certain proportion of the distribution to the left of it (proportion specified by the value you feed the function). These values should of course be between 0 and 1.\n\nimport scipy.stats as stats\nstats.norm.ppf(0.5)\n\n0.0\n\n\n\n\n\nnan\n\n\nWhat happens if we give a value like -1?\n\nstats.norm.ppf(-0.1)\nnp.isnan(stats.norm.ppf(-0.1))\n\nTrue\n\n\nInfinite values can pop up too. There are lots of ways to create an infinite value via casting as well.\n\nfloat(1)\nfloat(\"Inf\")\n-float(\"Inf\")\n\n-inf\n\n\nLet’s update our string from before with digits! (Note that using floating point for z and integers for mean and standard deviation doesn’t really make sense, I’m just showing the functionality!)\n\nprob_string = \"The probability of being less than or equal to {y:.2f} for a normal distribution with mean {mean:d} and standard deviation {std:d} is {prob:.4f}\"\n\nprob_string.format(y = y, mean = mean, std = std, prob = stats.norm.cdf(y, mean, std))\n\n'The probability of being less than or equal to 1.55 for a normal distribution with mean 1 and standard deviation 2 is 0.6083'"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#user-defined-functions",
    "href": "01_Programming_in_python/Learning_Python.html#user-defined-functions",
    "title": "Week 1",
    "section": "User Defined Functions",
    "text": "User Defined Functions\nLet’s create our own plotting function that will plot a Binomial random variables probabilities with a Normal approximation overlay.\nIf you aren’t familiar, a Binomial random variable measures the number of successes in a fixed number of Success/Failure trials (denoted by \\(n\\)). These trials need to be indpendent and have the same probability of success (called \\(p\\)).\nWe denote this via \\[Y\\sim Bin(n, p)\\] As \\(Y\\) measures the number of successes in \\(n\\) trials, the values \\(Y\\) can take on are \\(y = 0, 1, 2, ..., n\\). The probabilities associated with each of these outcomes is given by the Probability Mass Function (PMF) of the Binomial distribution:\n\\[p_Y(y)= P(Y=y) = \\binom{n}{y}p^y(1-p)^{n-y}\\]\nwhere \\(\\binom{n}{y} = \\frac{n!}{y!(n-y)!}\\) is the binomial coefficient.\nWe can find these values using the scipy.stats module! Let’s read in the appropriate modules for finding these values and doing our plotting.\n\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nWhen writing a function, it is usually easier to write the code for the body of the function outside of a function definition first and then put it into a function.\nLet’s start by creating code to find the probabilities associated with the Binomial for a given \\(n\\) and \\(p\\). The scipy.stats module has a binom.pmf() function that will return probabilities from the PMF.\n\nprint(stats.binom.pmf.__doc__)\n\nProbability mass function at k of the given RV.\n\n        Parameters\n        ----------\n        k : array_like\n            Quantiles.\n        arg1, arg2, arg3,... : array_like\n            The shape parameter(s) for the distribution (see docstring of the\n            instance object for more information)\n        loc : array_like, optional\n            Location parameter (default=0).\n\n        Returns\n        -------\n        pmf : array_like\n            Probability mass function evaluated at k\n\n        \n\n\nNote: the k argument can be ‘array like’. That is, we can pass something like a list of values we want the probabilities for. As a binomial measures the number of successes in \\(n\\) trials, we need to consider all of the integers from 0 to n. This can be done easily with the range() function.\n\nstats.binom.pmf(0, n = 10, p = 0.5)\n\n0.0009765624999999989\n\n\n\nstats.binom.pmf([0,1,2], n = 10, p = 0.5)\n\narray([0.00097656, 0.00976563, 0.04394531])\n\n\nOk, we have the values of the random variable (0 to \\(n\\)) and the probabilities we want to plot. We can use matplotlib’s pyplot module which has a bar() function that can create a basic bar graph for us. It has two arguments:\n\nthe x values for the location of the bars\nthe height for the height of the bars\n\nWe just need to pass two things of the same length that correspond to the pairs of x and height.\n\nn = 30\np = 0.5\nplt.bar(x = range(n+1), height = stats.binom.pmf(range(n+1), n = n, p = p))\n\n\n\n\n\n\n\n\nGreat! Now let’s wrap that up into a function so the user can call it and just specify n and p. We’ll give default values of n=30 and p=0.5.\n\ndef binom_norm_plot(n = 30, p = 0.5):\n    plt.bar(x = range(n+1), height = stats.binom.pmf(range(n+1), n = n, p = p))\n    return None\n\n\nbinom_norm_plot()\n\n\n\n\n\n\n\n\n\nbinom_norm_plot(n= 10, p = 0.1)\n\n\n\n\n\n\n\n\nNow we are cooking! We want to add a curve to this graph corresponding to the commonly used Normal approximation to the binomial.\nThe Normal distribution has two parameters: mean and standard deviation (or variance)\nThe mean of the Binomial is \\(n*p\\) and the standard deviation is \\(\\sqrt{n*p*(1-p)}\\). We can plug these values into the norm.pdf() from the scipy.stats module (PDF stands for Probability Density Function and is the continuous analog to the PMF).\n\nprint(stats.norm.pdf.__doc__)\n\nProbability density function at x of the given RV.\n\n        Parameters\n        ----------\n        x : array_like\n            quantiles\n        arg1, arg2, arg3,... : array_like\n            The shape parameter(s) for the distribution (see docstring of the\n            instance object for more information)\n        loc : array_like, optional\n            location parameter (default=0)\n        scale : array_like, optional\n            scale parameter (default=1)\n\n        Returns\n        -------\n        pdf : ndarray\n            Probability density function evaluated at x\n\n        \n\n\n\nstats.norm.pdf(0, loc = 0, scale =1)\n\n0.3989422804014327\n\n\n\nimport math\nstats.norm.pdf(range(31), loc = n*p, scale = math.sqrt(n*p*(1-p)))\n\narray([4.45617467e-08, 3.08033680e-07, 1.86349517e-06, 9.86625663e-06,\n       4.57162515e-05, 1.85388542e-04, 6.57944456e-04, 2.04357059e-03,\n       5.55500081e-03, 1.32151677e-02, 2.75140991e-02, 5.01339573e-02,\n       7.99471056e-02, 1.11575174e-01, 1.36278225e-01, 1.45673124e-01,\n       1.36278225e-01, 1.11575174e-01, 7.99471056e-02, 5.01339573e-02,\n       2.75140991e-02, 1.32151677e-02, 5.55500081e-03, 2.04357059e-03,\n       6.57944456e-04, 1.85388542e-04, 4.57162515e-05, 9.86625663e-06,\n       1.86349517e-06, 3.08033680e-07, 4.45617467e-08])\n\n\nGreat, now let’s add those values to the plot using the plot() function from pyplot. We’ll learn about this later but if we use matplotlib and create a plot in a cell followed by another plot, it will place those on the same plot by default. Therefore, we just want to create the bar plot and then this other plot in the same cell.\n\nprint(plt.plot.__doc__)\n\nPlot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n&gt;&gt;&gt; plot(x, y)        # plot x and y using default line style and color\n&gt;&gt;&gt; plot(x, y, 'bo')  # plot x and y using blue circle markers\n&gt;&gt;&gt; plot(y)           # plot y using x as index array 0..N-1\n&gt;&gt;&gt; plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n&gt;&gt;&gt; plot(x, y, 'go--', linewidth=2, markersize=12)\n&gt;&gt;&gt; plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n&gt;&gt;&gt; plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  &gt;&gt;&gt; plot(x1, y1, 'bo')\n  &gt;&gt;&gt; plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  &gt;&gt;&gt; x = [1, 2, 3]\n  &gt;&gt;&gt; y = np.array([[1, 2], [3, 4], [5, 6]])\n  &gt;&gt;&gt; plot(x, y)\n\n  is equivalent to:\n\n  &gt;&gt;&gt; for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  &gt;&gt;&gt; plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also, this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to\n    `~.axes.Axes.autoscale_view`.\n\n**kwargs : `.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    &gt;&gt;&gt; plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    &gt;&gt;&gt; plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `.Bbox`\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gapcolor: color or None\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    mouseover: bool\n    path_effects: `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: unknown\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'&lt;'``         triangle_left marker\n``'&gt;'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).\n\n\n\nbinom_norm_plot(n = 30, p = 0.5)\nn = 30\np = 0.5\nplt.plot(range(n+1), stats.norm.pdf(range(n+1), loc = n*p, scale = math.sqrt(n*p*(1-p))))\n\n\n\n\n\n\n\n\nAdd that to the function!\n\ndef binom_norm_plot(n = 30, p = 0.5):\n    plt.bar(x = range(n+1), height = stats.binom.pmf(range(n+1), n = n, p = p))\n    plt.plot(range(n+1), stats.norm.pdf(range(n+1), loc = n*p, scale = math.sqrt(n*p*(1-p))))\n    return None\n\n\nbinom_norm_plot()\n\n\n\n\n\n\n\n\n\nbinom_norm_plot(n= 100, p = 0.2)"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#control-flow-examples",
    "href": "01_Programming_in_python/Learning_Python.html#control-flow-examples",
    "title": "Week 1",
    "section": "Control Flow Examples",
    "text": "Control Flow Examples\n\nfor loop and if/then/else\nFirst, let’s do an if/then/else logic example. Fizz buzz coding!\nThe logic for the fizz buzz programming problem: - If a number is divisible by 15 (both 3 and 5), then print fizz buzz - If a number is only divisible by 3, then print fizz - If a number is only divisible by 5, then print buzz - Otherwise print the number itself\nWe can use the % (modulus operator) to get the remainder from a division calculation.\n\n10 % 3\n\n1\n\n\n\n11 % 3\n\n2\n\n\n\n12 % 3\n\n0\n\n\nLet’s use if/then/else logic to go through and print the correct statement.\n\nvalue = 45\nif ((value % 5) == 0) and ((value % 3) == 0):\n    print(\"fizz buzz\")\nelif (value % 3) == 0:\n    print(\"fizz\")\nelif (value % 5) == 0:\n    print(\"buzz\")\nelse:\n    print(value)\n\nfizz buzz\n\n\nNow wrap it in a for loop to test all numbers between 1 and 100.\n\nfor value in range(1, 101):\n    if ((value % 5) == 0) and ((value % 3) == 0):\n        print(\"fizz buzz\")\n    elif (value % 3) == 0:\n        print(\"fizz\")\n    elif (value % 5) == 0:\n        print(\"buzz\")\n    else:\n        print(value)\n\n1\n2\nfizz\n4\nbuzz\nfizz\n7\n8\nfizz\nbuzz\n11\nfizz\n13\n14\nfizz buzz\n16\n17\nfizz\n19\nbuzz\nfizz\n22\n23\nfizz\nbuzz\n26\nfizz\n28\n29\nfizz buzz\n31\n32\nfizz\n34\nbuzz\nfizz\n37\n38\nfizz\nbuzz\n41\nfizz\n43\n44\nfizz buzz\n46\n47\nfizz\n49\nbuzz\nfizz\n52\n53\nfizz\nbuzz\n56\nfizz\n58\n59\nfizz buzz\n61\n62\nfizz\n64\nbuzz\nfizz\n67\n68\nfizz\nbuzz\n71\nfizz\n73\n74\nfizz buzz\n76\n77\nfizz\n79\nbuzz\nfizz\n82\n83\nfizz\nbuzz\n86\nfizz\n88\n89\nfizz buzz\n91\n92\nfizz\n94\nbuzz\nfizz\n97\n98\nfizz\nbuzz\n\n\n\n\nwhile loop Example\nLet’s do a quick interactive number guessing game. We’ll use the input function to ask the user for an input.\nA while loop allows us to keep executing until the user guesses correctly and we update the condition.\n\ninput(\"give me a value \")\n\ngive me a value 514kjdaf\n\n\n'514kjdaf'\n\n\n\ntruth = 18\ncorrect = False\nwhile not correct:\n    guess = int(input(\"Give me an integer value between 1 and 20: \"))\n    if guess == truth:\n        print(\"Way to go!\")\n        correct = True\n    elif guess &lt; truth:\n        print(\"Something larger\")\n    elif guess &gt; truth:\n        print(\"something smaller\")\n\nGive me an integer value between 1 and 20: 1\nSomething larger\nGive me an integer value between 1 and 20: 20\nsomething smaller\nGive me an integer value between 1 and 20: 19\nsomething smaller\nGive me an integer value between 1 and 20: 18\nWay to go!"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#lists-and-tuples-practice",
    "href": "01_Programming_in_python/Learning_Python.html#lists-and-tuples-practice",
    "title": "Week 1",
    "section": "Lists and Tuples Practice",
    "text": "Lists and Tuples Practice\nList comprehensions can be confusing at first. Let’s do a little practice. First, let’s write a list comprehension to return a list with values 0, 1, …, 9.\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n[x for x in range(10)]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nOk, that is a bit silly but we can do more with this. For instance, we can return \\(x^2\\) instead of just \\(x\\).\n\n[x**2 for x in range(10)]\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nWe can also return more than one value if we return a list! Let’s return \\(x\\), \\(x+1\\), and \\(x+2\\).\n\n[[x, x+1, x+2] for x in range(10)]\n\n[[0, 1, 2],\n [1, 2, 3],\n [2, 3, 4],\n [3, 4, 5],\n [4, 5, 6],\n [5, 6, 7],\n [6, 7, 8],\n [7, 8, 9],\n [8, 9, 10],\n [9, 10, 11]]\n\n\n:Next we’ll consider the map() function, which allows us to apply a function to every element of list (along with other things we’ll look at later!).\nConsider applying the sum() function to the first (0th) and second (1st) elements of our list. This should sum the subsequent lists.\n\ny = [[x, x+1, x+2] for x in range(10)]\n\n\ny[0]\n\n[0, 1, 2]\n\n\n\nsum(y[0])\n\n3\n\n\nTo do this to every list element would be tedious. Instead we can map() the sum() function to each element.\n\nmap(sum, y)\n\n&lt;map at 0x7d5be5260b20&gt;\n\n\nThis doesn’t actually return the values! Instead this is an iterator that allows us to find the values when needed.\nAn easy way to get at these is to just wrap the map in a list().\n\nz = list(map(sum, y))\nz\n\n[3, 6, 9, 12, 15, 18, 21, 24, 27, 30]\n\n\nNext we consider the zip() function, which is quite useful for looking at multiple lists (and other data structures) at once. Here we have two lists of the same length, which we can check with the code below.\n\nprint((len(z), len(y)))\nzip(z, y)\n\n(10, 10)\n\n\n&lt;zip at 0x7d5be521bb80&gt;\n\n\nNow we can write a for loop to iterate over the 0th (first) element of each list at the same time, then the 1st (second) element, …, etc.\n\nfor i, j in zip(z, y):\n    print(i, j)\n\n3 [0, 1, 2]\n6 [1, 2, 3]\n9 [2, 3, 4]\n12 [3, 4, 5]\n15 [4, 5, 6]\n18 [5, 6, 7]\n21 [6, 7, 8]\n24 [7, 8, 9]\n27 [8, 9, 10]\n30 [9, 10, 11]\n\n\nThis can be done via a list comprehension as well!\n\n[[i,j] for i, j in zip(z,y)]\n\n[[3, [0, 1, 2]],\n [6, [1, 2, 3]],\n [9, [2, 3, 4]],\n [12, [3, 4, 5]],\n [15, [4, 5, 6]],\n [18, [5, 6, 7]],\n [21, [6, 7, 8]],\n [24, [7, 8, 9]],\n [27, [8, 9, 10]],\n [30, [9, 10, 11]]]\n\n\n\n[[i,j] for i, j in zip(z,y) if i &gt;12]\n\n[[15, [4, 5, 6]],\n [18, [5, 6, 7]],\n [21, [6, 7, 8]],\n [24, [7, 8, 9]],\n [27, [8, 9, 10]],\n [30, [9, 10, 11]]]"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#dictionaries",
    "href": "01_Programming_in_python/Learning_Python.html#dictionaries",
    "title": "Week 1",
    "section": "Dictionaries",
    "text": "Dictionaries\nLet’s construct a dictionary from a tuple and a list. The tuple will act as the keys for our dictionary and the list as the values corresponding to those keys. (We could use lists for both!)\n\nkeys = tuple(x for x in \"abcdef\")\nkeys\n\n('a', 'b', 'c', 'd', 'e', 'f')\n\n\nNow we’ll use another list comprehension to return a list where each element is a list itself of length four based on different mathematical functions.\n\nimport math\nvalues = [[x, pow(x, 3), math.exp(x), x + 10] for x in range(10, 16)]\nvalues\n\n[[10, 1000, 22026.465794806718, 20],\n [11, 1331, 59874.14171519782, 21],\n [12, 1728, 162754.79141900392, 22],\n [13, 2197, 442413.3920089205, 23],\n [14, 2744, 1202604.2841647768, 24],\n [15, 3375, 3269017.3724721107, 25]]\n\n\nUsing the zip() function on the tuple an dlist, we can easily create a dictionary using the dict() function.\n\nmy_dict = dict(zip(keys, values))\nmy_dict\n\n{'a': [10, 1000, 22026.465794806718, 20],\n 'b': [11, 1331, 59874.14171519782, 21],\n 'c': [12, 1728, 162754.79141900392, 22],\n 'd': [13, 2197, 442413.3920089205, 23],\n 'e': [14, 2744, 1202604.2841647768, 24],\n 'f': [15, 3375, 3269017.3724721107, 25]}\n\n\n\nmy_dict[\"a\"]\n\n[10, 1000, 22026.465794806718, 20]\n\n\nWhat happens if we convert this dictionary into a list?\n\nlist(my_dict)\n\n['a', 'b', 'c', 'd', 'e', 'f']\n\n\n\nlist(my_dict.values())\n\n[[10, 1000, 22026.465794806718, 20],\n [11, 1331, 59874.14171519782, 21],\n [12, 1728, 162754.79141900392, 22],\n [13, 2197, 442413.3920089205, 23],\n [14, 2744, 1202604.2841647768, 24],\n [15, 3375, 3269017.3724721107, 25]]\n\n\n\nlist(zip(list(my_dict), list(my_dict.values())))\n\n[('a', [10, 1000, 22026.465794806718, 20]),\n ('b', [11, 1331, 59874.14171519782, 21]),\n ('c', [12, 1728, 162754.79141900392, 22]),\n ('d', [13, 2197, 442413.3920089205, 23]),\n ('e', [14, 2744, 1202604.2841647768, 24]),\n ('f', [15, 3375, 3269017.3724721107, 25])]\n\n\nWe can use any immutable object as our keys. Below we’ll use a tuple as one of our keys and see how to reference that key.\n\nmy_dict[(1, \"this\")] = \"this is a string\"\n\n\nmy_dict\n\n{'a': [10, 1000, 22026.465794806718, 20],\n 'b': [11, 1331, 59874.14171519782, 21],\n 'c': [12, 1728, 162754.79141900392, 22],\n 'd': [13, 2197, 442413.3920089205, 23],\n 'e': [14, 2744, 1202604.2841647768, 24],\n 'f': [15, 3375, 3269017.3724721107, 25],\n (1, 'this'): 'this is a string'}\n\n\nA really useful operator is the in operator. This easily allows us to see if a key exists in a dictionary.\n\n\"a\" in my_dict\n\nTrue\n\n\n\n\"o\" in my_dict\n\nFalse\n\n\n\n\"o\" not in my_dict\n\nTrue"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#numpy",
    "href": "01_Programming_in_python/Learning_Python.html#numpy",
    "title": "Week 1",
    "section": "NumPy",
    "text": "NumPy\nWe’ll load in the numpy module and create a two-dimensional array using random values from a standard normal distribution.\n\nimport numpy as np\nrng = np.random.default_rng(10)\nvalues = rng.standard_normal(30)\nvalues\n\narray([-1.10333845, -0.72502464, -0.78180526,  0.26697586, -0.24858073,\n        0.12648305,  0.84304257,  0.85793655,  0.47518364, -0.4507686 ,\n       -0.75493228, -0.81481411, -0.34385486, -0.05138009, -0.97227368,\n       -1.13448753,  0.30570522, -1.85168503, -0.17705351,  0.42582567,\n       -0.98535561, -1.11295413, -0.76062603,  0.64802459, -0.12983136,\n       -1.86959723, -0.42334911,  1.0138968 ,  0.98371534,  0.63004195])\n\n\n\nvalues.shape\n\n(30,)\n\n\n\nvalues.reshape((15, 2))\n\narray([[-1.10333845, -0.72502464],\n       [-0.78180526,  0.26697586],\n       [-0.24858073,  0.12648305],\n       [ 0.84304257,  0.85793655],\n       [ 0.47518364, -0.4507686 ],\n       [-0.75493228, -0.81481411],\n       [-0.34385486, -0.05138009],\n       [-0.97227368, -1.13448753],\n       [ 0.30570522, -1.85168503],\n       [-0.17705351,  0.42582567],\n       [-0.98535561, -1.11295413],\n       [-0.76062603,  0.64802459],\n       [-0.12983136, -1.86959723],\n       [-0.42334911,  1.0138968 ],\n       [ 0.98371534,  0.63004195]])\n\n\n\nvalues.reshape((15, 2)).shape\n\n(15, 2)\n\n\n\nmy_array = values.reshape((15,2))\nmy_array.base\n\narray([-1.10333845, -0.72502464, -0.78180526,  0.26697586, -0.24858073,\n        0.12648305,  0.84304257,  0.85793655,  0.47518364, -0.4507686 ,\n       -0.75493228, -0.81481411, -0.34385486, -0.05138009, -0.97227368,\n       -1.13448753,  0.30570522, -1.85168503, -0.17705351,  0.42582567,\n       -0.98535561, -1.11295413, -0.76062603,  0.64802459, -0.12983136,\n       -1.86959723, -0.42334911,  1.0138968 ,  0.98371534,  0.63004195])\n\n\nWe can iterate over values in an array using nditer(). Be careful here: the iteration doesn’t always go through the values in the same pattern as it relies on the memory layout of the array. You can change this behavior though. See https://numpy.org/doc/stable/reference/arrays.nditer.html#arrays-nditer for a discussion and information on how to specify the order.\n\nfor i in np.nditer(my_array):\n    print(math.exp(i))\n\n0.33176166344167046\n0.4843126352266208\n0.45757921701143867\n1.306008914185862\n0.7799068968474736\n1.1348302173565885\n2.3234254197779323\n2.3582894560408887\n1.6083095234626505\n0.6371382601360868\n0.4700424440698853\n0.44272161841215146\n0.709031831025442\n0.9499175441491969\n0.3782221046899628\n0.3215868817920055\n1.3575820587101297\n0.15697244006051864\n0.8377349576565134\n1.5308538739450661\n0.37330645316600336\n0.32858683744892436\n0.4673737442809796\n1.911760583275141\n0.8782435285725788\n0.1541857502671811\n0.6548499786487098\n2.7563209456441498\n2.6743740273453316\n1.877689348050752\n\n\nMany of the functions we’ll apply to arrays are ufuncs or universal functions. These act on an array without requiring the use of explicit looping. They tend to be computationally superior to looping so their use is encouraged! Here we apply the \\(e\\) function to each element of our array y.\n\nnp.exp(my_array)\n\narray([[0.33176166, 0.48431264],\n       [0.45757922, 1.30600891],\n       [0.7799069 , 1.13483022],\n       [2.32342542, 2.35828946],\n       [1.60830952, 0.63713826],\n       [0.47004244, 0.44272162],\n       [0.70903183, 0.94991754],\n       [0.3782221 , 0.32158688],\n       [1.35758206, 0.15697244],\n       [0.83773496, 1.53085387],\n       [0.37330645, 0.32858684],\n       [0.46737374, 1.91176058],\n       [0.87824353, 0.15418575],\n       [0.65484998, 2.75632095],\n       [2.67437403, 1.87768935]])\n\n\nIf you haven’t seen matrices and the matrix representation of the linear model, you can ignore all of this and just see how we are doing some array computations below.\nLet’s fit a simple linear regression model! Suppose we have a 1D response \\(y\\) and a single predictor (call the vector \\(x\\)). Then we can find the usual least squares regression line using arrays!\nDefine \\[y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_n\\end{pmatrix}\\mbox{ our response}\\] \\[x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{pmatrix}\\mbox{ our predictor}\\] We can consider the simple linear regression model: \\[y_i = \\beta_0+\\beta_1 x_i + E_i\\] which can be fit by minimizing the sum of squared residuals. Let \\(\\hat{y}_i = \\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) denote the prediction for the \\(i^{th}\\) observation. Then we want to find the \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize \\[SS(E) = \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\\] This can be minimized by considering the matrix view of the model: Define \\[X = \\begin{pmatrix} 1 & x_1\\\\ 1 & x_2 \\\\ ... \\\\ 1 & x_n\\end{pmatrix}\\] and \\[\\beta = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1\\end{pmatrix}\\] Then using matrix multiplication \\[X\\beta = \\begin{pmatrix} \\beta_0*1 + \\beta_1*x_1\\\\\\beta_0*1 + \\beta_1*x_2\\\\...\\\\\\beta_0*1+\\beta_1*x_n\\end{pmatrix}\\] Our model can be written as\n\\[y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ ... \\\\ y_n\\end{pmatrix} = \\begin{pmatrix} \\beta_0*1 + \\beta_1*x_1\\\\\\beta_0*1 + \\beta_1*x_2\\\\...\\\\\\beta_0*1+\\beta_1*x_n\\end{pmatrix} + \\begin{pmatrix} E_1 \\\\ E_2 \\\\ ... \\\\ E_n\\end{pmatrix}\\]\nor \\[y=X\\beta + E\\]\nThen minimizing the \\(SS(E)\\) can be equivalently written as minimizing over the \\(\\beta\\) vector in \\[(y-X\\beta)^T(y-X\\beta)\\] The solution to this equation is \\[\\hat{\\beta} = \\begin{pmatrix}\\hat{\\beta_0} \\\\\\hat{\\beta_1} \\end{pmatrix} = \\left(X^TX\\right)^{-1}X^Ty\\]\n\nx = np.array([3, 2, 6, 10, 11.2])     #x values\ny = -2 + 4*x + rng.standard_normal(5) #create SLR model based response\ny\n\narray([ 9.76194119,  4.15506012, 22.16957773, 37.82402224, 42.87679986])\n\n\nOk, we want to now fit an SLR model between \\(y\\) and \\(x\\) and find estimates of the slope and intercept (true values are -2 and 4).\nLet’s create our ‘design matrix’ by zip 1’s (for the intercept) together with our x values.\n\nX = np.array(list(zip(np.ones(5), x)))\nX\n\narray([[ 1. ,  3. ],\n       [ 1. ,  2. ],\n       [ 1. ,  6. ],\n       [ 1. , 10. ],\n       [ 1. , 11.2]])\n\n\nWe’ll need the transpose of this matrix.\n\nX.transpose()\n\narray([[ 1. ,  1. ,  1. ,  1. ,  1. ],\n       [ 3. ,  2. ,  6. , 10. , 11.2]])\n\n\nAnd we need to multiply these matrices together.\n\nnp.matmul(X.transpose(), X)\n\narray([[  5.  ,  32.2 ],\n       [ 32.2 , 274.44]])\n\n\nLastly, we need to calculate an matrix inverse. We can do that with the np.linalg.inv() function.\n\nXTXinv = np.linalg.inv(np.matmul(X.transpose(), X))\nXTXinv\n\narray([[ 0.81834447, -0.09601622],\n       [-0.09601622,  0.01490935]])\n\n\nFinally, let’s get the least squares solution!\n\nnp.matmul(np.matmul(XTXinv, X.transpose()), y)\n\narray([-3.23545354,  4.12933754])\n\n\nPretty close to the actual values!"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#pandas-data-frames",
    "href": "01_Programming_in_python/Learning_Python.html#pandas-data-frames",
    "title": "Week 1",
    "section": "Pandas Data Frames",
    "text": "Pandas Data Frames\nWe’ll learn how to read in data using pandas shortly, but here we read in a data file to create a data frame to work with.\nThere is a dataset on the light measurement quality of cheese at: https://www4.stat.ncsu.edu/~online/datasets/cheese.csv\n\nimport pandas as pd\nimport numpy as np\ncheese = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/cheese.csv\")\n\nWe can use the .head() and .info() methods to inspect the data.\n\ncheese.head()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\n\n\n1\n26\n2\n51.52\n6.18\n17.09\n\n\n2\n26\n3\n52.69\n6.09\n17.59\n\n\n3\n26\n4\n52.06\n6.36\n17.50\n\n\n4\n26\n5\n51.63\n6.13\n17.19\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ncheese.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 24 entries, 0 to 23\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   syrup   24 non-null     int64  \n 1   rep     24 non-null     int64  \n 2   L       24 non-null     float64\n 3   a       24 non-null     float64\n 4   b       24 non-null     float64\ndtypes: float64(3), int64(2)\nmemory usage: 1.1 KB\n\n\nWe can also easily add a variable by simply creating a new column reference. Here the value specified is replicated the appropriate amount of times to fill the column. We’ll look at this in more detail later.\n\ncheese[\"dummy\"] = \"no\"\n\n\ncheese.head()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nno\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nno\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nno\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nno\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can reorder the rows of the data frame using the .sort_values() method. Simply specify the column name (or names as a list).\n\ncheese.sort_values(\"a\")\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nno\n\n\n5\n26\n6\n52.73\n6.12\n17.50\nno\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nno\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nno\n\n\n10\n42\n5\n48.64\n6.30\n16.21\nno\n\n\n23\n62\n6\n47.88\n6.34\n15.64\nno\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nno\n\n\n7\n42\n2\n48.57\n6.42\n15.91\nno\n\n\n20\n62\n3\n47.35\n6.49\n15.70\nno\n\n\n22\n62\n5\n46.77\n6.66\n15.91\nno\n\n\n19\n62\n2\n46.66\n6.66\n16.30\nno\n\n\n8\n42\n3\n47.57\n6.84\n16.04\nno\n\n\n18\n62\n1\n45.99\n6.84\n15.68\nno\n\n\n11\n42\n6\n47.49\n6.91\n15.91\nno\n\n\n21\n62\n4\n45.83\n6.96\n15.61\nno\n\n\n9\n42\n4\n46.85\n6.97\n15.85\nno\n\n\n6\n42\n1\n47.21\n7.02\n16.00\nno\n\n\n17\n55\n6\n42.65\n7.55\n14.40\nno\n\n\n16\n55\n5\n42.12\n7.56\n14.03\nno\n\n\n13\n55\n2\n42.31\n7.59\n13.98\nno\n\n\n14\n55\n3\n42.31\n7.63\n14.42\nno\n\n\n15\n55\n4\n41.49\n7.66\n13.58\nno\n\n\n12\n55\n1\n41.43\n7.71\n13.74\nno\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ncheese.sort_values([\"syrup\", \"a\"])\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nno\n\n\n5\n26\n6\n52.73\n6.12\n17.50\nno\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nno\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nno\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nno\n\n\n10\n42\n5\n48.64\n6.30\n16.21\nno\n\n\n7\n42\n2\n48.57\n6.42\n15.91\nno\n\n\n8\n42\n3\n47.57\n6.84\n16.04\nno\n\n\n11\n42\n6\n47.49\n6.91\n15.91\nno\n\n\n9\n42\n4\n46.85\n6.97\n15.85\nno\n\n\n6\n42\n1\n47.21\n7.02\n16.00\nno\n\n\n17\n55\n6\n42.65\n7.55\n14.40\nno\n\n\n16\n55\n5\n42.12\n7.56\n14.03\nno\n\n\n13\n55\n2\n42.31\n7.59\n13.98\nno\n\n\n14\n55\n3\n42.31\n7.63\n14.42\nno\n\n\n15\n55\n4\n41.49\n7.66\n13.58\nno\n\n\n12\n55\n1\n41.43\n7.71\n13.74\nno\n\n\n23\n62\n6\n47.88\n6.34\n15.64\nno\n\n\n20\n62\n3\n47.35\n6.49\n15.70\nno\n\n\n19\n62\n2\n46.66\n6.66\n16.30\nno\n\n\n22\n62\n5\n46.77\n6.66\n15.91\nno\n\n\n18\n62\n1\n45.99\n6.84\n15.68\nno\n\n\n21\n62\n4\n45.83\n6.96\n15.61\nno\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can add some NaN or not a number values. These represent missing data values. For a discussion of why this isn’t None, see https://stackoverflow.com/questions/17534106/what-is-the-difference-between-nan-and-none\n\ncheese.loc[0:3, \"dummy\"] = np.nan\ncheese.head()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nNaN\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nNaN\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nNaN\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nNaN\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can then drop any rows that contain a NaN value (in any column) with .dropna(). This doesn’t overwrite the object.\n\ncheese.dropna()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n5\n26\n6\n52.73\n6.12\n17.50\nno\n\n\n6\n42\n1\n47.21\n7.02\n16.00\nno\n\n\n7\n42\n2\n48.57\n6.42\n15.91\nno\n\n\n8\n42\n3\n47.57\n6.84\n16.04\nno\n\n\n9\n42\n4\n46.85\n6.97\n15.85\nno\n\n\n10\n42\n5\n48.64\n6.30\n16.21\nno\n\n\n11\n42\n6\n47.49\n6.91\n15.91\nno\n\n\n12\n55\n1\n41.43\n7.71\n13.74\nno\n\n\n13\n55\n2\n42.31\n7.59\n13.98\nno\n\n\n14\n55\n3\n42.31\n7.63\n14.42\nno\n\n\n15\n55\n4\n41.49\n7.66\n13.58\nno\n\n\n16\n55\n5\n42.12\n7.56\n14.03\nno\n\n\n17\n55\n6\n42.65\n7.55\n14.40\nno\n\n\n18\n62\n1\n45.99\n6.84\n15.68\nno\n\n\n19\n62\n2\n46.66\n6.66\n16.30\nno\n\n\n20\n62\n3\n47.35\n6.49\n15.70\nno\n\n\n21\n62\n4\n45.83\n6.96\n15.61\nno\n\n\n22\n62\n5\n46.77\n6.66\n15.91\nno\n\n\n23\n62\n6\n47.88\n6.34\n15.64\nno\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\ncheese.head()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nNaN\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nNaN\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nNaN\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nNaN\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can also fill in any NaN values using .fillna(). This also doesn’t overwrite the object.\n\ncheese.fillna(\"yes\")\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\ndummy\n\n\n\n\n0\n26\n1\n51.89\n6.22\n17.43\nyes\n\n\n1\n26\n2\n51.52\n6.18\n17.09\nyes\n\n\n2\n26\n3\n52.69\n6.09\n17.59\nyes\n\n\n3\n26\n4\n52.06\n6.36\n17.50\nyes\n\n\n4\n26\n5\n51.63\n6.13\n17.19\nno\n\n\n5\n26\n6\n52.73\n6.12\n17.50\nno\n\n\n6\n42\n1\n47.21\n7.02\n16.00\nno\n\n\n7\n42\n2\n48.57\n6.42\n15.91\nno\n\n\n8\n42\n3\n47.57\n6.84\n16.04\nno\n\n\n9\n42\n4\n46.85\n6.97\n15.85\nno\n\n\n10\n42\n5\n48.64\n6.30\n16.21\nno\n\n\n11\n42\n6\n47.49\n6.91\n15.91\nno\n\n\n12\n55\n1\n41.43\n7.71\n13.74\nno\n\n\n13\n55\n2\n42.31\n7.59\n13.98\nno\n\n\n14\n55\n3\n42.31\n7.63\n14.42\nno\n\n\n15\n55\n4\n41.49\n7.66\n13.58\nno\n\n\n16\n55\n5\n42.12\n7.56\n14.03\nno\n\n\n17\n55\n6\n42.65\n7.55\n14.40\nno\n\n\n18\n62\n1\n45.99\n6.84\n15.68\nno\n\n\n19\n62\n2\n46.66\n6.66\n16.30\nno\n\n\n20\n62\n3\n47.35\n6.49\n15.70\nno\n\n\n21\n62\n4\n45.83\n6.96\n15.61\nno\n\n\n22\n62\n5\n46.77\n6.66\n15.91\nno\n\n\n23\n62\n6\n47.88\n6.34\n15.64\nno\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can get a lot of summary statistics easily from a data frame using the .describe() method.\n\ncheese.describe()\n\n\n  \n    \n\n\n\n\n\n\nsyrup\nrep\nL\na\nb\n\n\n\n\ncount\n24.000000\n24.000000\n24.000000\n24.000000\n24.000000\n\n\nmean\n46.250000\n3.500000\n47.151667\n6.800417\n15.800417\n\n\nstd\n14.013193\n1.744557\n3.691321\n0.559895\n1.239907\n\n\nmin\n26.000000\n1.000000\n41.430000\n6.090000\n13.580000\n\n\n25%\n38.000000\n2.000000\n45.035000\n6.330000\n15.312500\n\n\n50%\n48.500000\n3.500000\n47.280000\n6.750000\n15.910000\n\n\n75%\n56.750000\n5.000000\n49.360000\n7.152500\n16.497500\n\n\nmax\n62.000000\n6.000000\n52.730000\n7.710000\n17.590000"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#reading-raw-data-with-pandas",
    "href": "01_Programming_in_python/Learning_Python.html#reading-raw-data-with-pandas",
    "title": "Week 1",
    "section": "Reading Raw Data with Pandas",
    "text": "Reading Raw Data with Pandas\nBelow we’ll check our working directory and then read in a SAS file using read_sas() from pandas.\n\nimport os\nos.getcwd()\n\n'/content'\n\n\n\npd.read_sas(\"air.sas7bdat\")\n\n\n  \n    \n\n\n\n\n\n\nDATE\nAIR\n\n\n\n\n0\n1949-01-01\n112.0\n\n\n1\n1949-02-01\n118.0\n\n\n2\n1949-03-01\n132.0\n\n\n3\n1949-04-01\n129.0\n\n\n4\n1949-05-01\n121.0\n\n\n...\n...\n...\n\n\n139\n1960-08-01\n606.0\n\n\n140\n1960-09-01\n508.0\n\n\n141\n1960-10-01\n461.0\n\n\n142\n1960-11-01\n390.0\n\n\n143\n1960-12-01\n432.0\n\n\n\n\n144 rows × 2 columns"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#numerical-summaries",
    "href": "01_Programming_in_python/Learning_Python.html#numerical-summaries",
    "title": "Week 1",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nContingency Tables\nWe’ll import pandas and read in a data set about NFL games. The data has information about games from 2002 to 2014. Each row represents one game and each column a variable measured on that game.\nAvailable here: https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\n\nimport pandas as pd\nscores = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\")\nscores.head()\n\n\n  \n    \n\n\n\n\n\n\nweek\ndate\nday\nseason\nawayTeam\nAQ1\nAQ2\nAQ3\nAQ4\nAOT\n...\nhomeFumLost\nhomeNumPen\nhomePenYds\nhome3rdConv\nhome3rdAtt\nhome4thConv\nhome4thAtt\nhomeTOP\nHminusAScore\nhomeSpread\n\n\n\n\n0\n1\n5-Sep\nThu\n2002\nSan Francisco 49ers\n3\n0\n7\n6\n-1\n...\n0\n10\n80\n4\n8\n0\n1\n32.47\n-3\n-4.0\n\n\n1\n1\n8-Sep\nSun\n2002\nMinnesota Vikings\n3\n17\n0\n3\n-1\n...\n1\n4\n33\n2\n6\n0\n0\n28.48\n4\n4.5\n\n\n2\n1\n8-Sep\nSun\n2002\nNew Orleans Saints\n6\n7\n7\n0\n6\n...\n0\n8\n85\n1\n6\n0\n1\n31.48\n-6\n6.0\n\n\n3\n1\n8-Sep\nSun\n2002\nNew York Jets\n0\n17\n3\n11\n6\n...\n1\n10\n82\n4\n8\n2\n2\n39.13\n-6\n-3.0\n\n\n4\n1\n8-Sep\nSun\n2002\nArizona Cardinals\n10\n3\n3\n7\n-1\n...\n0\n7\n56\n6\n10\n1\n2\n34.40\n8\n6.0\n\n\n\n\n5 rows × 82 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can use the pd.cut() function to take a numeric variable and chop it up into groups. Note: generally splitting a numeric variable into categories isn’t a great idea. You lose information!\n\npd.cut(scores.AtotalYds, bins = [0, 150, 250, 400, 700])\n\n0       (250, 400]\n1       (250, 400]\n2       (250, 400]\n3       (250, 400]\n4       (250, 400]\n           ...    \n3466    (250, 400]\n3467    (250, 400]\n3468    (250, 400]\n3469    (150, 250]\n3470    (250, 400]\nName: AtotalYds, Length: 3471, dtype: category\nCategories (4, interval[int64, right]): [(0, 150] &lt; (150, 250] &lt; (250, 400] &lt; (400, 700]]\n\n\nGreat, but we can improve the categories being produced for the category variable. We just need to specify the labels argument and have the same number of labels as the number of categories produced.\n\nscores[\"AtotalYdsCut\"] = pd.cut(scores.AtotalYds, bins = [0, 150, 250, 400, 700], labels = [\"Poor\", \"Ok\", \"Good\", \"Great\"])\n\nNow we can easily create a one-way table to display the number of occurrences of each category using the .value_counts() method.\n\nscores.AtotalYdsCut.value_counts()\n\nGood     2082\nGreat     706\nOk        611\nPoor       72\nName: AtotalYdsCut, dtype: int64\n\n\nLet’s create a similar variable for the away team total yards and create a two-table using the pd.crosstab() function.\n\nscores[\"HtotalYdsCut\"] = pd.cut(scores.HtotalYds, bins = [0, 150, 250, 400, 700], labels = [\"Poor\", \"Ok\", \"Good\", \"Great\"])\npd.crosstab(scores.HtotalYdsCut, scores.AtotalYdsCut)\n\n\n  \n    \n\n\n\n\n\nAtotalYdsCut\nPoor\nOk\nGood\nGreat\n\n\nHtotalYdsCut\n\n\n\n\n\n\n\n\nPoor\n0\n5\n20\n10\n\n\nOk\n4\n56\n282\n101\n\n\nGood\n43\n399\n1354\n396\n\n\nGreat\n25\n151\n426\n199\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nNumeric Variable Summaries with pd.pivot_table()\nIn addition to the methods/functions used in the notes, we can also create summary stats across groups using the pivot_table() function and the .agg() method.\nFirst let’s look at the pivot_table() function. We need to supply a data frame, the columns we want to summarize (as values), the grouping variable(s) (as index), and the aggregation function(s) (as aggfunc).\n\npd.pivot_table(scores, values = \"AtotalYds\", index = \"season\")\n\n\n  \n    \n\n\n\n\n\n\nAtotalYds\n\n\nseason\n\n\n\n\n\n2002\n324.161049\n\n\n2003\n308.247191\n\n\n2004\n321.161049\n\n\n2005\n308.378277\n\n\n2006\n316.449438\n\n\n2007\n321.906367\n\n\n2008\n318.760300\n\n\n2009\n323.977528\n\n\n2010\n329.734082\n\n\n2011\n341.655431\n\n\n2012\n347.089888\n\n\n2013\n340.685393\n\n\n2014\n343.310861\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can give more than one function if we specify it as a list.\n\npd.pivot_table(scores, values = \"AtotalYds\", index = \"season\", aggfunc = [\"mean\", \"median\", \"std\"])\n\n\n  \n    \n\n\n\n\n\n\nmean\nmedian\nstd\n\n\n\nAtotalYds\nAtotalYds\nAtotalYds\n\n\nseason\n\n\n\n\n\n\n\n2002\n324.161049\n327\n90.314948\n\n\n2003\n308.247191\n311\n88.807591\n\n\n2004\n321.161049\n318\n90.912791\n\n\n2005\n308.378277\n304\n82.750663\n\n\n2006\n316.449438\n323\n79.343217\n\n\n2007\n321.906367\n327\n82.208671\n\n\n2008\n318.760300\n317\n86.567603\n\n\n2009\n323.977528\n323\n86.360668\n\n\n2010\n329.734082\n327\n88.964809\n\n\n2011\n341.655431\n336\n86.608624\n\n\n2012\n347.089888\n352\n88.788882\n\n\n2013\n340.685393\n341\n83.003251\n\n\n2014\n343.310861\n343\n83.797760\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe can also group by more than one variable by specify the variables as a list.\n\npd.pivot_table(scores, values = \"AtotalYds\", index = [\"season\", \"week\"], aggfunc = [\"mean\", \"median\", \"std\"])\n\n\n  \n    \n\n\n\n\n\n\n\nmean\nmedian\nstd\n\n\n\n\nAtotalYds\nAtotalYds\nAtotalYds\n\n\nseason\nweek\n\n\n\n\n\n\n\n2002\n1\n310.000000\n286.0\n70.386552\n\n\n10\n352.857143\n367.0\n76.175766\n\n\n11\n299.125000\n332.5\n98.892450\n\n\n12\n357.750000\n348.0\n97.722396\n\n\n13\n314.062500\n302.0\n104.683949\n\n\n...\n...\n...\n...\n...\n\n\n2014\n9\n344.769231\n364.0\n96.695531\n\n\nConfChamp\n257.500000\n257.5\n68.589358\n\n\nDivision\n367.250000\n363.0\n46.399533\n\n\nSuperBowl\n396.000000\n396.0\nNaN\n\n\nWildCard\n257.000000\n276.5\n133.434129\n\n\n\n\n273 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n.agg() method\nThe .agg() method gives us another way to create summary stats. We can create a variable and pass a tuple containing the column to summarize and the function to use when summarizing.\n\nscores.agg(max_AtotalYds = (\"AtotalYds\", \"max\"))\n\n\n  \n    \n\n\n\n\n\n\nAtotalYds\n\n\n\n\nmax_AtotalYds\n622\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n    \n  \n\n\nWe can create multiple summaries easily by specifying more than one variable. These summaries also respect the groupby() groupings!\n\nscores.groupby(\"season\").agg(max_AtotalYds = (\"AtotalYds\", \"max\"))\n\n\n  \n    \n\n\n\n\n\n\nmax_AtotalYds\n\n\nseason\n\n\n\n\n\n2002\n591\n\n\n2003\n548\n\n\n2004\n605\n\n\n2005\n494\n\n\n2006\n536\n\n\n2007\n531\n\n\n2008\n564\n\n\n2009\n524\n\n\n2010\n592\n\n\n2011\n622\n\n\n2012\n583\n\n\n2013\n542\n\n\n2014\n596\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nscores \\\n.groupby(\"season\") \\\n.agg(max_AtotalYds = (\"AtotalYds\", \"max\"),\n     min_AtotalYds = (\"AtotalYds\", \"min\"))\n\n\n  \n    \n\n\n\n\n\n\nmax_AtotalYds\nmin_AtotalYds\n\n\nseason\n\n\n\n\n\n\n2002\n591\n47\n\n\n2003\n548\n96\n\n\n2004\n605\n26\n\n\n2005\n494\n113\n\n\n2006\n536\n104\n\n\n2007\n531\n104\n\n\n2008\n564\n125\n\n\n2009\n524\n72\n\n\n2010\n592\n67\n\n\n2011\n622\n137\n\n\n2012\n583\n119\n\n\n2013\n542\n103\n\n\n2014\n596\n78"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#more-function-writing",
    "href": "01_Programming_in_python/Learning_Python.html#more-function-writing",
    "title": "Week 1",
    "section": "More Function Writing",
    "text": "More Function Writing\nLet’s look at the use of a lambda function as the key to the sorted() function. We’ll use a toy data set we create with an id column that we want to sort by.\nWe can use list comprehensions to generate ids of the form ‘id#’ and some cost values we just make up. Then we’ll put those into a data frame to inspect.\n\nimport pandas as pd\nnums = [4, 10, 22, 5, 12, 1]\nids = [\"id\" + str(x) for x in nums]\ncost = [21, 32, 12, 0, 23, 43]\nprint(ids, cost)\n\n['id4', 'id10', 'id22', 'id5', 'id12', 'id1'] [21, 32, 12, 0, 23, 43]\n\n\nPut those into a data frame.\n\nmy_df = pd.DataFrame(cost, index = ids, columns = [\"cost\"])\nmy_df\n\n\n  \n    \n\n\n\n\n\n\ncost\n\n\n\n\nid4\n21\n\n\nid10\n32\n\n\nid22\n12\n\n\nid5\n0\n\n\nid12\n23\n\n\nid1\n43\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow let’s see how the sorted function acts on the ids column.\n\nsorted(my_df.index)\n\n['id1', 'id10', 'id12', 'id22', 'id4', 'id5']\n\n\nNot quite what we want! We can sort with a key. A lambda function can be used to make a quick function that will take the id column’s value and look at the 2nd element on, converting that to an int.\nPassing this as the function to use for the key will give us what we want!\n\nsorted(my_df.index, key = lambda x: int(x[2:]))\n\n['id1', 'id4', 'id5', 'id10', 'id12', 'id22']\n\n\nNow we can sort the data frame using this sorting.\n\nmy_df.loc[sorted(my_df.index, key = lambda x: int(x[2:]))]\n\n\n  \n    \n\n\n\n\n\n\ncost\n\n\n\n\nid1\n43\n\n\nid4\n21\n\n\nid5\n0\n\n\nid10\n32\n\n\nid12\n23\n\n\nid22\n12\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nInvestigating the LLN\nLet’s use our find_means() function to see the behavior of the sample mean when the sample size gets large samples.\nIn this case, we are generating data from a standard normal distribution. This is the population distribution. Since we are generating the data ourselves, we know that the actual mean of the population here is 0. In statistics we often try to estimate these population quantities. Our usual estimator of the population mean is the sample mean. Ideally our estimator should be observed really close to the truth (population value) when our sample size grows!\nA statistical theory called the Law of Large Numbers (LLN) says that the behavior of sample average type statistics should get closer to the population value as the sample size grows.\nWe can demonstrate this behavior by using our find_means() function for increasingly large random samples from the standard normal distribution! We know the true value is 0 so we can see how close it gets using our simulations.\nFirst, let’s create our find_means() function and bring in numpy as needed.\n\nimport numpy as np\nfrom numpy.random import default_rng\nrng = default_rng(3)\n\ndef find_means(*args, decimals = 4):\n    \"\"\"\n    Assume that args will be a bunch of numpy arrays (1D) or pandas series\n    \"\"\"\n    means = []\n    for x in args: #iterate over the tuple values\n        means.append(np.mean(x).round(decimals))\n    return means\n\nNow we can use map() to create samples of varying sizes easily!\n\nstandard_normal_data = map(rng.standard_normal, range(1, 51))\n\nNow we can pass that list using * to unpack it. The elements will be read into the unlimited positional argument we defined, args.\n\nfind_means(*standard_normal_data)\n\n[2.0409,\n -1.0688,\n -0.412,\n 0.0515,\n -0.4263,\n 0.1058,\n 0.5156,\n 0.2256,\n -0.5504,\n 0.3226,\n -0.5999,\n 0.0015,\n -0.0263,\n -0.0402,\n 0.1694,\n 0.3517,\n -0.0175,\n -0.0879,\n 0.5763,\n -0.0086,\n -0.3138,\n 0.0313,\n 0.3178,\n 0.1171,\n -0.2855,\n 0.0906,\n 0.0179,\n 0.1475,\n 0.3238,\n -0.0359,\n 0.091,\n 0.241,\n 0.1158,\n 0.0413,\n 0.0656,\n -0.0811,\n 0.2771,\n -0.1547,\n -0.1708,\n -0.151,\n -0.0666,\n 0.0119,\n 0.0339,\n 0.1983,\n -0.1047,\n -0.0413,\n 0.1733,\n 0.0081,\n 0.1325,\n -0.0272]"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#plotting-matplotlib",
    "href": "01_Programming_in_python/Learning_Python.html#plotting-matplotlib",
    "title": "Week 1",
    "section": "Plotting matplotlib",
    "text": "Plotting matplotlib\nLet’s take our function that was able to output many means from normal random samples and use it to make a plot! This will visualize the result we were seeing.\nLet’s first manually create the plot, then put the plot functionality into a function! We really have a sample size associated with each of these means. Let’s first create a data frame that has the sample size and the calculated mean.\n\nstandard_normal_data = map(rng.standard_normal, range(1, 51))\nmy_means = find_means(*standard_normal_data)\nmean_df = pd.DataFrame(zip(my_means, range(1,51)), columns = [\"means\", \"n\"])\nmean_df.head()\n\n\n  \n    \n\n\n\n\n\n\nmeans\nn\n\n\n\n\n0\n-1.4405\n1\n\n\n1\n0.2436\n2\n\n\n2\n0.1613\n3\n\n\n3\n-0.4510\n4\n\n\n4\n-0.3320\n5\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow import matplotlib.pyplot and let’s make a plot!\n\nimport matplotlib.pyplot as plt\nplt.scatter(mean_df.n, mean_df.means)\nplt.axhline(y = 0, color = 'r', linestyle = '-')\n\n\n\n\n\n\n\n\nGreat, now let’s make a function that does this for us! The function will just take in a sample size and then produce the plot from that. Our find_means() function will be used as a helper function.\n\ndef plot_means(n = 50):\n  standard_normal_data = map(rng.standard_normal, range(1, n+1))\n  my_means = find_means(*standard_normal_data)\n  mean_df = pd.DataFrame(zip(my_means, range(1, n+1)), columns = [\"means\", \"n\"])\n  plt.scatter(mean_df.n, mean_df.means)\n  plt.axhline(y = 0, color = 'r', linestyle = '-')\n\n\nplot_means(10)\n\n\n\n\n\n\n\n\n\nplot_means(1000)"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#plotting-with-pandas",
    "href": "01_Programming_in_python/Learning_Python.html#plotting-with-pandas",
    "title": "Week 1",
    "section": "Plotting with pandas",
    "text": "Plotting with pandas\nLet’s revisit our NFL data example from previous. We’ll then look at line plots.\n\nimport pandas as pd\nscores = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/scoresFull.csv\")\nscores.head()\n\n\n  \n    \n\n\n\n\n\n\nweek\ndate\nday\nseason\nawayTeam\nAQ1\nAQ2\nAQ3\nAQ4\nAOT\n...\nhomeFumLost\nhomeNumPen\nhomePenYds\nhome3rdConv\nhome3rdAtt\nhome4thConv\nhome4thAtt\nhomeTOP\nHminusAScore\nhomeSpread\n\n\n\n\n0\n1\n5-Sep\nThu\n2002\nSan Francisco 49ers\n3\n0\n7\n6\n-1\n...\n0\n10\n80\n4\n8\n0\n1\n32.47\n-3\n-4.0\n\n\n1\n1\n8-Sep\nSun\n2002\nMinnesota Vikings\n3\n17\n0\n3\n-1\n...\n1\n4\n33\n2\n6\n0\n0\n28.48\n4\n4.5\n\n\n2\n1\n8-Sep\nSun\n2002\nNew Orleans Saints\n6\n7\n7\n0\n6\n...\n0\n8\n85\n1\n6\n0\n1\n31.48\n-6\n6.0\n\n\n3\n1\n8-Sep\nSun\n2002\nNew York Jets\n0\n17\n3\n11\n6\n...\n1\n10\n82\n4\n8\n2\n2\n39.13\n-6\n-3.0\n\n\n4\n1\n8-Sep\nSun\n2002\nArizona Cardinals\n10\n3\n3\n7\n-1\n...\n0\n7\n56\n6\n10\n1\n2\n34.40\n8\n6.0\n\n\n\n\n5 rows × 82 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nWe saw we could get summaries of the data in a number of ways. One was the .agg() method.\n\nscores \\\n.groupby(\"season\") \\\n.agg(max_AtotalYds = (\"AtotalYds\", \"max\"),\n     min_AtotalYds = (\"AtotalYds\", \"min\"))\n\n\n  \n    \n\n\n\n\n\n\nmax_AtotalYds\nmin_AtotalYds\n\n\nseason\n\n\n\n\n\n\n2002\n591\n47\n\n\n2003\n548\n96\n\n\n2004\n605\n26\n\n\n2005\n494\n113\n\n\n2006\n536\n104\n\n\n2007\n531\n104\n\n\n2008\n564\n125\n\n\n2009\n524\n72\n\n\n2010\n592\n67\n\n\n2011\n622\n137\n\n\n2012\n583\n119\n\n\n2013\n542\n103\n\n\n2014\n596\n78\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nLine graphs are especially useful when you have data over time. For instance, the summary above is over season. We can put season on the x-axis and see how our summaries change over time. The plot.line() method is great for creating these types of plots. It prefers data where each column will be plotted and the index represents the time. That is exactly what we have outputted from above!\nLet’s add a mean and median variable for the AtotalYds and then create a line plot for these four variables.\n\nsummaries = scores \\\n.groupby(\"season\") \\\n.agg(max_AtotalYds = (\"AtotalYds\", \"max\"),\n     min_AtotalYds = (\"AtotalYds\", \"min\"),\n     mean_AtotalYds = (\"AtotalYds\", \"mean\"),\n     median_AtotalYds = (\"AtotalYds\", \"median\"))\nsummaries.head()\n\n\n  \n    \n\n\n\n\n\n\nmax_AtotalYds\nmin_AtotalYds\nmean_AtotalYds\nmedian_AtotalYds\n\n\nseason\n\n\n\n\n\n\n\n\n2002\n591\n47\n324.161049\n327.0\n\n\n2003\n548\n96\n308.247191\n311.0\n\n\n2004\n605\n26\n321.161049\n318.0\n\n\n2005\n494\n113\n308.378277\n304.0\n\n\n2006\n536\n104\n316.449438\n323.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nsummaries.plot()"
  },
  {
    "objectID": "01_Programming_in_python/Learning_Python.html#error-handling",
    "href": "01_Programming_in_python/Learning_Python.html#error-handling",
    "title": "Week 1",
    "section": "Error Handling",
    "text": "Error Handling\nLet’s use our find_means() function and add a try block to keep processing if one of our arguments passed isn’t appropriate.\n\ndef find_means(*args, decimals = 4):\n    \"\"\"\n    Assume that args will be a bunch of numpy arrays (1D) or pandas series\n    \"\"\"\n    means = []\n    for x in args: #iterate over the tuple values\n        means.append(np.mean(x).round(decimals))\n    return means\n\n\nfind_means(np.array([1,2,3,6, \"hi\"]))\n\n\n---------------------------------------------------------------------------\nUFuncTypeError                            Traceback (most recent call last)\n&lt;ipython-input-83-a4c06acf4d92&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 find_means(np.array([1,2,3,6, \"hi\"]))\n\n&lt;ipython-input-82-06de86f374b0&gt; in find_means(decimals, *args)\n      5     means = []\n      6     for x in args: #iterate over the tuple values\n----&gt; 7         means.append(np.mean(x).round(decimals))\n      8     return means\n\n/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py in mean(*args, **kwargs)\n\n/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py in mean(a, axis, dtype, out, keepdims, where)\n   3430             return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n   3431 \n-&gt; 3432     return _methods._mean(a, axis=axis, dtype=dtype,\n   3433                           out=out, **kwargs)\n   3434 \n\n/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py in _mean(a, axis, dtype, out, keepdims, where)\n    178             is_float16_result = True\n    179 \n--&gt; 180     ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n    181     if isinstance(ret, mu.ndarray):\n    182         ret = um.true_divide(\n\nUFuncTypeError: ufunc 'add' did not contain a loop with signature matching types (dtype('&lt;U21'), dtype('&lt;U21')) -&gt; None\n\n\n\n\ndef find_means(*args, decimals = 4):\n    \"\"\"\n    Assume that args will be a bunch of numpy arrays (1D) or pandas series\n    \"\"\"\n    means = []\n    for x in args: #iterate over the tuple values\n        try:\n            means.append(np.mean(x).round(decimals))\n        except TypeError:\n            print(\"It appears that the data type is wrong!\")\n            means.append(np.nan)\n    return means\n\n\nfind_means(np.array([1,2,3,6, \"hi\"]))\n\nIt appears that the data type is wrong!\n\n\n[nan]\n\n\n\nfind_means(np.array([424,13,13]), np.array([1,2,3,6, \"hi\"]), np.array([\"yo\"]))\n\nIt appears that the data type is wrong!\nIt appears that the data type is wrong!\n\n\n[150.0, nan, nan]"
  },
  {
    "objectID": "02_Big_Data_Management/SQL_Joins_Notebook.html",
    "href": "02_Big_Data_Management/SQL_Joins_Notebook.html",
    "title": "SQL Joins on chinook",
    "section": "",
    "text": "SQL Joins on chinook\nNow let’s do some joins on the chinook database. We’ll reread in the packages are make our connection to the database.\n\nimport sqlite3\nimport pandas as pd\ncon = sqlite3.connect(\"chinook.db\")\n\n\nUsing pandas we can check out the tables returned as a data frame.\n\npd.read_sql(\"SELECT * FROM sqlite_schema WHERE type = 'table';\", con)\n\n\n  \n    \n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nalbums\nalbums\n2\nCREATE TABLE \"albums\"\\r\\n(\\r\\n [AlbumId] IN...\n\n\n1\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n2\ntable\nartists\nartists\n4\nCREATE TABLE \"artists\"\\r\\n(\\r\\n [ArtistId] ...\n\n\n3\ntable\ncustomers\ncustomers\n5\nCREATE TABLE \"customers\"\\r\\n(\\r\\n [Customer...\n\n\n4\ntable\nemployees\nemployees\n8\nCREATE TABLE \"employees\"\\r\\n(\\r\\n [Employee...\n\n\n5\ntable\ngenres\ngenres\n10\nCREATE TABLE \"genres\"\\r\\n(\\r\\n [GenreId] IN...\n\n\n6\ntable\ninvoices\ninvoices\n11\nCREATE TABLE \"invoices\"\\r\\n(\\r\\n [InvoiceId...\n\n\n7\ntable\ninvoice_items\ninvoice_items\n13\nCREATE TABLE \"invoice_items\"\\r\\n(\\r\\n [Invo...\n\n\n8\ntable\nmedia_types\nmedia_types\n15\nCREATE TABLE \"media_types\"\\r\\n(\\r\\n [MediaT...\n\n\n9\ntable\nplaylists\nplaylists\n16\nCREATE TABLE \"playlists\"\\r\\n(\\r\\n [Playlist...\n\n\n10\ntable\nplaylist_track\nplaylist_track\n17\nCREATE TABLE \"playlist_track\"\\r\\n(\\r\\n [Pla...\n\n\n11\ntable\ntracks\ntracks\n20\nCREATE TABLE \"tracks\"\\r\\n(\\r\\n [TrackId] IN...\n\n\n12\ntable\nsqlite_stat1\nsqlite_stat1\n864\nCREATE TABLE sqlite_stat1(tbl,idx,stat)\n\n\n13\ntable\njustin_music\njustin_music\n865\nCREATE TABLE justin_music (\\n album TEX...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nThe tracks and albums tables each share a variable/column/key of AlbumId. Let’s do an inner join on those using pd.merge(). Remember that we can pull the full tables into pandas using pd.read_sql() and then use pandas to do the join!\n\ntracks_albums = pd.merge(left = pd.read_sql(\"SELECT * FROM tracks\", con), right = pd.read_sql(\"SELECT * FROM albums\", con),\n         how = \"inner\",\n         on = \"AlbumId\")\ntracks_albums\n\n\n  \n    \n\n\n\n\n\n\nTrackId\nName\nAlbumId\nMediaTypeId\nGenreId\nComposer\nMilliseconds\nBytes\nUnitPrice\nTitle\nArtistId\n\n\n\n\n0\n1\nFor Those About To Rock (We Salute You)\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n343719\n11170334\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n1\n6\nPut The Finger On You\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n205662\n6713451\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n2\n7\nLet's Get It Up\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n233926\n7636561\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n3\n8\nInject The Venom\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n210834\n6852860\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n4\n9\nSnowballed\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n203102\n6599424\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3498\n3499\nPini Di Roma (Pinien Von Rom) \\ I Pini Della V...\n343\n2\n24\nNone\n286741\n4718950\n0.99\nRespighi:Pines of Rome\n226\n\n\n3499\n3500\nString Quartet No. 12 in C Minor, D. 703 \"Quar...\n344\n2\n24\nFranz Schubert\n139200\n2283131\n0.99\nSchubert: The Late String Quartets & String Qu...\n272\n\n\n3500\n3501\nL'orfeo, Act 3, Sinfonia (Orchestra)\n345\n2\n24\nClaudio Monteverdi\n66639\n1189062\n0.99\nMonteverdi: L'Orfeo\n273\n\n\n3501\n3502\nQuintet for Horn, Violin, 2 Violas, and Cello ...\n346\n2\n24\nWolfgang Amadeus Mozart\n221331\n3665114\n0.99\nMozart: Chamber Music\n274\n\n\n3502\n3503\nKoyaanisqatsi\n347\n2\n10\nPhilip Glass\n206005\n3305164\n0.99\nKoyaanisqatsi (Soundtrack from the Motion Pict...\n275\n\n\n\n\n3503 rows × 11 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nNow we’ll join that table we just made with another table, invoice_items and do a left join (treating invoice_items as the left table). We’ll join on the key that is shared, trackId.\n\nnext = pd.merge(left = pd.read_sql(\"SELECT * FROM invoice_items\", con),\n         right= tracks_albums,\n         how = \"left\",\n         on = \"TrackId\")\nnext\n\n\n  \n    \n\n\n\n\n\n\nInvoiceLineId\nInvoiceId\nTrackId\nUnitPrice_x\nQuantity\nName\nAlbumId\nMediaTypeId\nGenreId\nComposer\nMilliseconds\nBytes\nUnitPrice_y\nTitle\nArtistId\n\n\n\n\n0\n1\n1\n2\n0.99\n1\nBalls to the Wall\n2\n2\n1\nNone\n342562\n5510424\n0.99\nBalls to the Wall\n2\n\n\n1\n2\n1\n4\n0.99\n1\nRestless and Wild\n3\n2\n1\nF. Baltes, R.A. Smith-Diesel, S. Kaufman, U. D...\n252051\n4331779\n0.99\nRestless and Wild\n2\n\n\n2\n3\n2\n6\n0.99\n1\nPut The Finger On You\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n205662\n6713451\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n3\n4\n2\n8\n0.99\n1\nInject The Venom\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n210834\n6852860\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n4\n5\n2\n10\n0.99\n1\nEvil Walks\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n263497\n8611245\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2235\n2236\n411\n3136\n0.99\n1\nLooking For Love\n141\n1\n3\nSykes\n391941\n12769847\n0.99\nGreatest Hits\n100\n\n\n2236\n2237\n411\n3145\n0.99\n1\nSweet Lady Luck\n141\n1\n3\nVandenberg\n273737\n8919163\n0.99\nGreatest Hits\n100\n\n\n2237\n2238\n411\n3154\n0.99\n1\nFeirinha da Pavuna/Luz do Repente/Bagaço da La...\n248\n1\n7\nArlindo Cruz/Franco/Marquinhos PQD/Negro, Jove...\n107206\n3593684\n0.99\nAo Vivo [IMPORT]\n155\n\n\n2238\n2239\n411\n3163\n0.99\n1\nSamba pras moças\n248\n1\n7\nGrazielle/Roque Ferreira\n152816\n5121366\n0.99\nAo Vivo [IMPORT]\n155\n\n\n2239\n2240\n412\n3177\n1.99\n1\nHot Girl\n249\n3\n19\nNone\n1325458\n267836576\n1.99\nThe Office, Season 1\n156\n\n\n\n\n2240 rows × 15 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nLastly, we’ll now combine that table with the invoices table using an outer join on the key InvoiceId. Note that you can actually do all of these joins in SQL using one call (which is likely much more efficient!). Please see the additional readings for topic 2 for more information.\n\npd.merge(left = pd.read_sql(\"SELECT * FROM invoices\", con),\n         right = next,\n         how = \"outer\",\n         on = \"InvoiceId\")\n\n\n  \n    \n\n\n\n\n\n\nInvoiceId\nCustomerId\nInvoiceDate\nBillingAddress\nBillingCity\nBillingState\nBillingCountry\nBillingPostalCode\nTotal\nInvoiceLineId\n...\nName\nAlbumId\nMediaTypeId\nGenreId\nComposer\nMilliseconds\nBytes\nUnitPrice_y\nTitle\nArtistId\n\n\n\n\n0\n1\n2\n2009-01-01 00:00:00\nTheodor-Heuss-Straße 34\nStuttgart\nNone\nGermany\n70174\n1.98\n1\n...\nBalls to the Wall\n2\n2\n1\nNone\n342562\n5510424\n0.99\nBalls to the Wall\n2\n\n\n1\n1\n2\n2009-01-01 00:00:00\nTheodor-Heuss-Straße 34\nStuttgart\nNone\nGermany\n70174\n1.98\n2\n...\nRestless and Wild\n3\n2\n1\nF. Baltes, R.A. Smith-Diesel, S. Kaufman, U. D...\n252051\n4331779\n0.99\nRestless and Wild\n2\n\n\n2\n2\n4\n2009-01-02 00:00:00\nUllevålsveien 14\nOslo\nNone\nNorway\n0171\n3.96\n3\n...\nPut The Finger On You\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n205662\n6713451\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n3\n2\n4\n2009-01-02 00:00:00\nUllevålsveien 14\nOslo\nNone\nNorway\n0171\n3.96\n4\n...\nInject The Venom\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n210834\n6852860\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n4\n2\n4\n2009-01-02 00:00:00\nUllevålsveien 14\nOslo\nNone\nNorway\n0171\n3.96\n5\n...\nEvil Walks\n1\n1\n1\nAngus Young, Malcolm Young, Brian Johnson\n263497\n8611245\n0.99\nFor Those About To Rock We Salute You\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2235\n411\n44\n2013-12-14 00:00:00\nPorthaninkatu 9\nHelsinki\nNone\nFinland\n00530\n13.86\n2236\n...\nLooking For Love\n141\n1\n3\nSykes\n391941\n12769847\n0.99\nGreatest Hits\n100\n\n\n2236\n411\n44\n2013-12-14 00:00:00\nPorthaninkatu 9\nHelsinki\nNone\nFinland\n00530\n13.86\n2237\n...\nSweet Lady Luck\n141\n1\n3\nVandenberg\n273737\n8919163\n0.99\nGreatest Hits\n100\n\n\n2237\n411\n44\n2013-12-14 00:00:00\nPorthaninkatu 9\nHelsinki\nNone\nFinland\n00530\n13.86\n2238\n...\nFeirinha da Pavuna/Luz do Repente/Bagaço da La...\n248\n1\n7\nArlindo Cruz/Franco/Marquinhos PQD/Negro, Jove...\n107206\n3593684\n0.99\nAo Vivo [IMPORT]\n155\n\n\n2238\n411\n44\n2013-12-14 00:00:00\nPorthaninkatu 9\nHelsinki\nNone\nFinland\n00530\n13.86\n2239\n...\nSamba pras moças\n248\n1\n7\nGrazielle/Roque Ferreira\n152816\n5121366\n0.99\nAo Vivo [IMPORT]\n155\n\n\n2239\n412\n58\n2013-12-22 00:00:00\n12,Community Centre\nDelhi\nNone\nIndia\n110017\n1.99\n2240\n...\nHot Girl\n249\n3\n19\nNone\n1325458\n267836576\n1.99\nThe Office, Season 1\n156\n\n\n\n\n2240 rows × 23 columns"
  },
  {
    "objectID": "03_Modeling_Data/02-Modeling_Example.html#read-in-the-fat-data-set",
    "href": "03_Modeling_Data/02-Modeling_Example.html#read-in-the-fat-data-set",
    "title": "Putting it all together",
    "section": "1. Read in the fat data set",
    "text": "1. Read in the fat data set\n\nDescription\nAge, weight, height, and 10 body circumference measurements are recorded for 252 men. Each man’s percentage of body fat was accurately estimated by an underwater weighing technique.\n\nbrozek, Percent body fat using Brozek’s equation, 457/Density - 414.2\nsiri, Percent body fat using Siri’s equation, 495/Density - 450\ndensity, Density (gm/\\(cm^3\\))\nage, Age (yrs)\nweight, Weight (lbs)\nheight, Height (inches)\nadipos, Adiposity index = Weight/Height\\(^2\\) (kg/\\(m^2\\))\nfree, Fat Free Weight = (1 - fraction of body fat) * Weight, using Brozek’s formula (lbs)\nneck, Neck circumference (cm)\nchest, Chest circumference (cm)\nabdom, Abdomen circumference (cm) at the umbilicus and level with the iliac crest\nhip, Hip circumference (cm)\nthigh, Thigh circumference (cm)\nknee, Knee circumference (cm)\nankle, Ankle circumference (cm)\nbiceps, Extended biceps circumference (cm)\nforearm, Forearm circumference (cm)\nwrist, Wrist circumference (cm) distal to the styloid processes\n\nSource Johnson R. Journal of Statistics Education v.4, n.1 (1996)\nWe’ll attempt to model the percent body fat (Brozek masure used as the gold standard) using easily obtainable measurements.\n\nimport pandas as pd\nimport numpy as np\nfat_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/fat.csv\")\n\n\nfat_data.columns\n\nIndex(['Unnamed: 0', 'brozek', 'siri', 'density', 'age', 'weight', 'height',\n       'adipos', 'free', 'neck', 'chest', 'abdom', 'hip', 'thigh', 'knee',\n       'ankle', 'biceps', 'forearm', 'wrist'],\n      dtype='object')\n\n\n\nfat_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 252 entries, 0 to 251\nData columns (total 19 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Unnamed: 0  252 non-null    int64  \n 1   brozek      252 non-null    float64\n 2   siri        252 non-null    float64\n 3   density     252 non-null    float64\n 4   age         252 non-null    int64  \n 5   weight      252 non-null    float64\n 6   height      252 non-null    float64\n 7   adipos      252 non-null    float64\n 8   free        252 non-null    float64\n 9   neck        252 non-null    float64\n 10  chest       252 non-null    float64\n 11  abdom       252 non-null    float64\n 12  hip         252 non-null    float64\n 13  thigh       252 non-null    float64\n 14  knee        252 non-null    float64\n 15  ankle       252 non-null    float64\n 16  biceps      252 non-null    float64\n 17  forearm     252 non-null    float64\n 18  wrist       252 non-null    float64\ndtypes: float64(17), int64(2)\nmemory usage: 37.5 KB\n\n\n\nfat_data.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\nbrozek\nsiri\ndensity\nage\nweight\nheight\nadipos\nfree\nneck\nchest\nabdom\nhip\nthigh\nknee\nankle\nbiceps\nforearm\nwrist\n\n\n\n\n0\n1\n12.6\n12.3\n1.0708\n23\n154.25\n67.75\n23.7\n134.9\n36.2\n93.1\n85.2\n94.5\n59.0\n37.3\n21.9\n32.0\n27.4\n17.1\n\n\n1\n2\n6.9\n6.1\n1.0853\n22\n173.25\n72.25\n23.4\n161.3\n38.5\n93.6\n83.0\n98.7\n58.7\n37.3\n23.4\n30.5\n28.9\n18.2\n\n\n2\n3\n24.6\n25.3\n1.0414\n22\n154.00\n66.25\n24.7\n116.0\n34.0\n95.8\n87.9\n99.2\n59.6\n38.9\n24.0\n28.8\n25.2\n16.6\n\n\n3\n4\n10.9\n10.4\n1.0751\n26\n184.75\n72.25\n24.9\n164.7\n37.4\n101.8\n86.4\n101.2\n60.1\n37.3\n22.8\n32.4\n29.4\n18.2\n\n\n4\n5\n27.8\n28.7\n1.0340\n24\n184.25\n71.25\n25.6\n133.1\n34.4\n97.3\n100.0\n101.9\n63.2\n42.2\n24.0\n32.2\n27.7\n17.7\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nDrop the observations where we have missing values for simplicity (and only select the columns we are going to use).\n\nmod_fat_data = fat_data.drop([\"Unnamed: 0\", \"siri\", \"density\"], axis = 1).dropna()\nmod_fat_data.head()\n\n\n  \n    \n\n\n\n\n\n\nbrozek\nage\nweight\nheight\nadipos\nfree\nneck\nchest\nabdom\nhip\nthigh\nknee\nankle\nbiceps\nforearm\nwrist\n\n\n\n\n0\n12.6\n23\n154.25\n67.75\n23.7\n134.9\n36.2\n93.1\n85.2\n94.5\n59.0\n37.3\n21.9\n32.0\n27.4\n17.1\n\n\n1\n6.9\n22\n173.25\n72.25\n23.4\n161.3\n38.5\n93.6\n83.0\n98.7\n58.7\n37.3\n23.4\n30.5\n28.9\n18.2\n\n\n2\n24.6\n22\n154.00\n66.25\n24.7\n116.0\n34.0\n95.8\n87.9\n99.2\n59.6\n38.9\n24.0\n28.8\n25.2\n16.6\n\n\n3\n10.9\n26\n184.75\n72.25\n24.9\n164.7\n37.4\n101.8\n86.4\n101.2\n60.1\n37.3\n22.8\n32.4\n29.4\n18.2\n\n\n4\n27.8\n24\n184.25\n71.25\n25.6\n133.1\n34.4\n97.3\n100.0\n101.9\n63.2\n42.2\n24.0\n32.2\n27.7\n17.7\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThere aren’t any categorical predictors so we don’t need to create any dummy variables.\nWe do however want to standardize our predictors when using the LASSO generally. If we have predictors on vastly different scales (say one takes on values from 0 to 1 and the other from 0 to 10000), this can affect our fitting and penalization. Instead we usually standardize our numerical predictors. We do so by subtracting off the mean and dividing by the standard deviation.\nFor example, if we had \\(x\\) with values 0, 3, 6, 10, 11, we would create a new column, say \\(x_{std}\\) that takes each observation and subtracts off the mean (\\(\\frac{0+3+6+10+11}{5} = 6\\)) and divides each observation by the standard deviation (\\(\\sqrt{\\frac{1}{5-1}\\left((0-6)^2+(3-6)^2+(6-6)^2+(10-6)^2+(11-6)^2\\right)} = 4.64\\)). This would yield a new set of values given by -1.447, -0.7234, 0, 0.964, 1.206.\n\nfrom math import sqrt\ntemp = np.array([0, 3, 6, 10, 11])\nprint(temp.mean(), temp.std(ddof = 1))\n(temp-temp.mean())/temp.std(ddof = 1)\n\n6.0 4.636809247747852\n\n\narray([-1.29399328, -0.64699664,  0.        ,  0.86266219,  1.07832773])\n\n\nHowever, we need to split our data before we standardize! We want to standardize only the training set values using the training set means and standard deviations. We’ll then use those same values to standardize the test set before we test our models!"
  },
  {
    "objectID": "03_Modeling_Data/02-Modeling_Example.html#training-and-test-split",
    "href": "03_Modeling_Data/02-Modeling_Example.html#training-and-test-split",
    "title": "Putting it all together",
    "section": "2. Training and Test Split",
    "text": "2. Training and Test Split\nFirst, let’s just read in all the functions we’ll need from sklearn\n\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, LassoCV, Lasso\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n  mod_fat_data.drop(\"brozek\", axis = 1),\n  mod_fat_data[\"brozek\"],\n  test_size=0.20,\n  random_state=41)"
  },
  {
    "objectID": "03_Modeling_Data/02-Modeling_Example.html#fit-and-select-models-on-training-data",
    "href": "03_Modeling_Data/02-Modeling_Example.html#fit-and-select-models-on-training-data",
    "title": "Putting it all together",
    "section": "3. Fit and Select Models on Training Data",
    "text": "3. Fit and Select Models on Training Data\nFirst, we can standardize our variables. We’ll save the values used for standardization so we can standardize the test set with those as well.\n\nmeans = X_train.apply(np.mean, axis = 0)\nmeans\n\n\n\n\n\n\n\n\n0\n\n\n\n\nage\n45.079602\n\n\nweight\n178.316667\n\n\nheight\n70.036070\n\n\nadipos\n25.403483\n\n\nfree\n143.059204\n\n\nneck\n37.876617\n\n\nchest\n100.808458\n\n\nabdom\n92.559701\n\n\nhip\n99.855224\n\n\nthigh\n59.358706\n\n\nknee\n38.607960\n\n\nankle\n23.018408\n\n\nbiceps\n32.155721\n\n\nforearm\n28.671642\n\n\nwrist\n18.196517\n\n\n\n\ndtype: float64\n\n\n\nstds = X_train.apply(np.std, axis = 0)\nstds\n\n\n\n\n\n\n\n\n0\n\n\n\n\nage\n12.807167\n\n\nweight\n27.765423\n\n\nheight\n3.868759\n\n\nadipos\n3.421550\n\n\nfree\n17.150096\n\n\nneck\n2.274781\n\n\nchest\n8.342636\n\n\nabdom\n10.502178\n\n\nhip\n6.760037\n\n\nthigh\n5.063393\n\n\nknee\n2.403715\n\n\nankle\n1.560810\n\n\nbiceps\n2.856846\n\n\nforearm\n2.046680\n\n\nwrist\n0.938050\n\n\n\n\ndtype: float64\n\n\n\nX_train = X_train.apply(lambda x: (x-np.mean(x))/np.std(x), axis = 0)\nX_train\n\n\n  \n    \n\n\n\n\n\n\nage\nweight\nheight\nadipos\nfree\nneck\nchest\nabdom\nhip\nthigh\nknee\nankle\nbiceps\nforearm\nwrist\n\n\n\n\n120\n0.540354\n1.015051\n1.153840\n0.232794\n0.445525\n1.285127\n0.418518\n0.632278\n0.627922\n-0.169591\n0.287904\n1.013315\n0.610561\n1.235346\n1.389566\n\n\n133\n0.384191\n-0.767741\n-0.849386\n-0.176377\n-1.507817\n-0.033681\n-0.048960\n-0.300862\n-1.117631\n-0.643582\n-1.251380\n-1.613526\n0.505550\n0.307013\n-0.955724\n\n\n207\n0.149947\n0.600867\n0.636879\n0.203568\n-0.551554\n1.021366\n0.226732\n0.832237\n0.272894\n0.264900\n0.329506\n0.180414\n1.590663\n1.430785\n0.216921\n\n\n49\n0.149947\n-1.830214\n-0.849386\n-1.520797\n-1.274582\n-1.704172\n-2.086685\n-2.110010\n-1.872064\n-1.729810\n-1.750607\n-0.716556\n-1.874697\n-1.403073\n-1.488745\n\n\n25\n-1.411679\n-0.686705\n0.378398\n-1.023946\n0.515495\n-0.956847\n-1.343515\n-1.224480\n-0.496332\n-0.860827\n-0.793755\n-0.332140\n-0.789584\n-0.230442\n-0.529308\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n80\n1.711573\n-0.524633\n-0.590905\n-0.088697\n-1.676912\n0.230081\n-0.372599\n0.308536\n-0.407575\n-0.900326\n-0.169721\n0.436691\n-0.964603\n-0.719039\n0.856545\n\n\n226\n0.774597\n-0.317541\n-0.461665\n0.057435\n0.066518\n-0.297443\n0.106866\n-0.138990\n-0.407575\n-0.544834\n-0.044914\n-0.268071\n0.435543\n0.307013\n0.643337\n\n\n140\n-0.396622\n-0.056425\n0.249158\n-0.234830\n-0.539892\n-0.209522\n-0.264719\n0.051446\n0.258102\n-0.051094\n0.412711\n-0.908764\n-0.474552\n-0.377021\n-1.701953\n\n\n163\n-0.865110\n-1.380014\n0.119917\n-1.637703\n-1.414523\n-0.824966\n-1.391462\n-0.872172\n-1.517037\n-1.374317\n-1.251380\n-1.677595\n-1.349642\n-1.207635\n-1.808557\n\n\n192\n-0.240459\n0.546843\n0.119917\n0.583513\n1.139399\n1.241167\n0.490438\n0.146665\n0.420823\n0.245151\n-0.003312\n1.077384\n0.645564\n0.697890\n0.536733\n\n\n\n\n201 rows × 15 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n#now each column has mean 0 and sd 1\nX_train.apply(np.mean, axis = 0)\nX_train.apply(np.std, axis = 0)\n\n\n\n\n\n\n\n\n0\n\n\n\n\nage\n1.0\n\n\nweight\n1.0\n\n\nheight\n1.0\n\n\nadipos\n1.0\n\n\nfree\n1.0\n\n\nneck\n1.0\n\n\nchest\n1.0\n\n\nabdom\n1.0\n\n\nhip\n1.0\n\n\nthigh\n1.0\n\n\nknee\n1.0\n\n\nankle\n1.0\n\n\nbiceps\n1.0\n\n\nforearm\n1.0\n\n\nwrist\n1.0\n\n\n\n\ndtype: float64\n\n\n\nMLR Models\nBelow I’ll consider some basic MLR models to compare. Even though we don’t need to use CV on those to determine a tuning parameter, we can use CV on those to determine the best MLR model (using the training set alone). Similarly, we’ll then find a ‘best’ LASSO model with tuning parameter selected by CV. Then we’ll compare the two ‘best’ models on the test set to see the overall winner!\n\n# Full model\ncv_full_model = cross_validate(\n    LinearRegression(),\n    X_train,\n    y_train,\n    cv = 5,\n    scoring = \"neg_mean_squared_error\")\ncv_mlr1= cross_validate(\n    LinearRegression(),\n    X_train[[\"weight\", \"wrist\", \"abdom\"]],\n    y_train,\n    cv = 5,\n    scoring = \"neg_mean_squared_error\")\ncv_mlr2 = cross_validate(\n    LinearRegression(),\n    X_train[[\"weight\", \"height\", \"neck\", \"chest\", \"abdom\", \"biceps\", \"forearm\", \"wrist\"]],\n    y_train,\n    cv = 5,\n    scoring = \"neg_mean_squared_error\")\n\n\nprint(np.sqrt(-sum(cv_full_model['test_score'])),\n      np.sqrt(-sum(cv_mlr1['test_score'])),\n      np.sqrt(-sum(cv_mlr2['test_score'])))\n\n6.101775440661614 9.044740234075288 9.680673752851906\n\n\nLooks like the full model is the best here!\n\nmlr_best = LinearRegression().fit(X_train, y_train)\nprint(mlr_best.intercept_)\nprint(np.array(list(zip(X_train.columns, mlr_best.coef_))))\n\n19.045273631840807\n[['age' '0.1338989120135575']\n ['weight' '9.34842058078133']\n ['height' '0.19894030597107948']\n ['adipos' '-0.9776707934156488']\n ['free' '-8.827474152930117']\n ['neck' '0.11485215326542098']\n ['chest' '0.5751829606181673']\n ['abdom' '1.2206574175508305']\n ['hip' '0.18340049047065232']\n ['thigh' '0.8379939074493237']\n ['knee' '0.16205656779585353']\n ['ankle' '0.2436973560143484']\n ['biceps' '0.17243823308972106']\n ['forearm' '0.30301942051892716']\n ['wrist' '-0.013949037814783072']]\n\n\n\n\nLASSO model\n\nlasso_mod = LassoCV(cv=5, random_state=0) \\\n    .fit(X_train,\n         y_train)\n\nJust to look at the tuning parameters and CV errors:\n\nnp.set_printoptions(suppress = True)\nfit_info = np.array(list(zip(lasso_mod.alphas_, np.mean(lasso_mod.mse_path_, axis = 1))))\nfit_info[fit_info[:,1].argsort()][0:10,].round(4)\n\narray([[0.0683, 2.1017],\n       [0.0732, 2.106 ],\n       [0.0785, 2.1139],\n       [0.0842, 2.1278],\n       [0.0637, 2.1427],\n       [0.0903, 2.1461],\n       [0.0968, 2.17  ],\n       [0.1038, 2.2005],\n       [0.1113, 2.229 ],\n       [0.0594, 2.233 ]])\n\n\n\nprint(lasso_mod.alpha_)\nprint(lasso_mod.intercept_)\nprint(np.array(list(zip(X_train.columns, lasso_mod.coef_))))\n\n0.06827844720988437\n19.045273631840807\n[['age' '0.03520622302650939']\n ['weight' '8.676685171578885']\n ['height' '0.18524483241596834']\n ['adipos' '0.0']\n ['free' '-8.098358879411064']\n ['neck' '0.0']\n ['chest' '0.1012312495726051']\n ['abdom' '1.5227501560784638']\n ['hip' '0.0']\n ['thigh' '0.536843692590691']\n ['knee' '0.2410290965097099']\n ['ankle' '0.0997282321269406']\n ['biceps' '0.12439075979914176']\n ['forearm' '0.20197553831501108']\n ['wrist' '0.0']]\n\n\nFit that best model for comparison on the test set.\n\nlasso_best = Lasso(lasso_mod.alpha_).fit(X_train,y_train)"
  },
  {
    "objectID": "03_Modeling_Data/02-Modeling_Example.html#compare-on-the-test-set",
    "href": "03_Modeling_Data/02-Modeling_Example.html#compare-on-the-test-set",
    "title": "Putting it all together",
    "section": "4. Compare on the Test Set",
    "text": "4. Compare on the Test Set\nWe need to standardize the test set using our training values. Then we can find the test set predictions and RMSE!\nWe’ll see how to have python do this for us later. For now, let’s do it manually!\n\n#quick function to standardize based off of a supplied mean and std\ndef my_std_fun(x, means, stds):\n    return(x-means)/stds\n#loop through the columns and use the function on each\nfor x in X_test.columns:\n    X_test[x] = my_std_fun(X_test[x], means[x], stds[x])\nX_test.head()\n\n\n  \n    \n\n\n\n\n\n\nage\nweight\nheight\nadipos\nfree\nneck\nchest\nabdom\nhip\nthigh\nknee\nankle\nbiceps\nforearm\nwrist\n\n\n\n\n107\n0.540354\n0.897999\n1.089220\n0.174341\n1.355141\n1.812651\n1.101755\n0.860802\n0.124966\n-0.702830\n0.038291\n0.244483\n1.030604\n0.453592\n0.963150\n\n\n143\n-1.724004\n-0.668697\n0.572259\n-1.111626\n0.049026\n-1.044767\n-1.043850\n-1.472047\n-0.880946\n-0.643582\n-1.043368\n-0.204002\n-0.579563\n-0.719039\n0.003713\n\n\n167\n-0.787028\n1.672344\n0.572259\n1.431082\n2.136478\n2.647896\n0.885996\n0.746540\n1.027328\n0.778390\n1.286359\n1.013315\n1.765681\n2.163679\n1.709378\n\n\n29\n-1.255516\n-0.632681\n-0.267804\n-0.468642\n0.153981\n-0.517244\n-0.408559\n-0.862650\n-0.170890\n-0.090593\n-1.376186\n-0.268071\n-0.719577\n-0.963337\n-0.635912\n\n\n30\n-1.021272\n0.132659\n0.959980\n-0.527095\n0.970303\n0.361962\n-0.036974\n-0.367514\n-0.008169\n-0.367087\n0.038291\n6.971758\n0.120510\n-0.474741\n0.216921\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nNow find our predictions and test set RMSE.\n\nmlr_pred = mlr_best.predict(X_test)\nlasso_pred = lasso_best.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, mlr_pred)), np.sqrt(mean_squared_error(y_test, lasso_pred)))\n\n1.7547240606525405 1.9916053642246054"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website houses the learning materials for ST 554 - Big Data Analysis at NC State."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis of Big Data",
    "section": "",
    "text": "Welcome to ST 554 - Analysis of Big Data (with python)\nIn this course we’ll look at common issues, analysis, and software used with big data. We’ll discuss the major aspects with the commonly cited ‘5 V’s of Big Data’:\nVolume, Variety, Velocity, Veracity (Variability), and Value\nThe course covers\nUsing python as our programming language we’ll learn about using Jupyter notebooks to share and document our work. We’ll use pyspark as our interface to the Spark software, which is a matrue software and commonly used to handle big data.",
    "crumbs": [
      "Home",
      "Analysis of Big Data"
    ]
  },
  {
    "objectID": "index.html#course-learning-outcomes",
    "href": "index.html#course-learning-outcomes",
    "title": "Analysis of Big Data",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\nAt the end of this course students will be able to\n\nexplain the steps and purpose of python programs (CO 1)\nefficiently read in, combine, and manipulate data in python (CO 2)\nutilize help and other resources to customize programs (CO 3)\nwrite programs using good programming practices (CO 4)\nexplore, manage, and solve common problems with big data (CO 5)",
    "crumbs": [
      "Home",
      "Analysis of Big Data"
    ]
  },
  {
    "objectID": "index.html#weekly-to-do-list",
    "href": "index.html#weekly-to-do-list",
    "title": "Analysis of Big Data",
    "section": "Weekly To-do List",
    "text": "Weekly To-do List\nGenerally speaking, each week will have a few videos to watch and readings to do. There are then corresponding homework assignments to complete (see the syllabus on Moodle for homework policies).\n\nThere will be two exams and the exam windows (days when you can take the exams) are available on the syllabus and course schedule.\nThere will be three projects, the third of which will count as the final for the course. These will require a reasonably substantial time commitment.",
    "crumbs": [
      "Home",
      "Analysis of Big Data"
    ]
  },
  {
    "objectID": "index.html#getting-help",
    "href": "index.html#getting-help",
    "title": "Analysis of Big Data",
    "section": "Getting Help!",
    "text": "Getting Help!\nTo obtain course help there are a number of options:\n\nDiscussion Forum on Moodle - This should be used for any question you feel comfortable asking and having others view. The TA, other students, and I will answer questions on this board. This will be the fastest way to receive a response!\nE-mail - If there is a question that you don’t feel comfortable asking the whole class you can use e-mail. The TA and I will be checking daily (during the regular work week).\nZoom Office Hour Sessions - These sessions can be used to share screens and have multiple users. You can do text chat, voice, and video. They are great for a class like this!",
    "crumbs": [
      "Home",
      "Analysis of Big Data"
    ]
  },
  {
    "objectID": "index.html#spring-2026-course-schedule",
    "href": "index.html#spring-2026-course-schedule",
    "title": "Analysis of Big Data",
    "section": "Spring 2026 Course Schedule",
    "text": "Spring 2026 Course Schedule\n\n\n\n\n\n\n\n\nTopic/Week\nLearning Materials\nAssignments\n\n\n\n\nWeek 1\n1/12-1/17\n01 - Course Goals & Other Resources\n02 - Basic Use of Python\n03 - Modules\n04 - JupyterLab Notebooks & Markdown\n05 - List Basics and Strings\n06 - Numeric Types and Booleans\n07 - Common Uses for Data\nHW 1 due W, 1/15\n\n\nWeek 2\n1/20-1/23 (Off M)\n08 - User Defined Functions\n09 - Control Flow\n10 - Lists and Tuples\n11 - Dictionaries\n12 - Numpy\nHW 2 due W, 1/22\n\n\nWeek 3\n1/26-1/30\n13 - Exploratory Data Analysis Concepts\n14 - Pandas Series\n15 - Pandas DataFrames\n16 - Pandas for Reading Data\n17 - Numeric Summaries\nHW 3 due W, 1/29\n\n\nWeek 4\n2/2-2/6\n18 - More Function Writing\n19 - Plotting with Matplotlib\n20 - Plotting with pandas\n21 - Error Handling\nHW 4 due W, 2/5\n\n\nWeek 5\n2/9-2/13\n22 - Big Recap!\n23 - Fitting and Evaluating SLR Models\n24 - Prediction and Training/Test Set Ideas\n25 - Cross-Validation\n26 - Multiple Linear Regression\n27 - LASSO\nExam 1 Th/F 2/6-2/7 - covers weeks 1-4\nProject 1 due W, 2/19\n\n\nWeek 6\n2/16-2/20 (Off T)\nNo new material. Project work time!\n\n\n\nWeek 7\n2/23-2/27\n01 - Big Data Basics\n02 - The Role of Statistics in Big Data\n03 - Databases & SQL\n04 - SQL Joins\n05 - SQL Resources\n06 - Data Pipelines & Storage\nHW 5 due W, 2/26\n\n\nWeek 8\n3/2-3/6\n07 - Legacy Software: HDFS\n08 - Connecting to our JupyterHub Environment\n09 - Spark for Big Data\nHW 6 due W, 3/5\n\n\nWeek 9\n3/9-3/13\n10 - pyspark: RDDs\n11 - pyspark: pandas-on-Spark\n12 - pyspark: Spark SQL\nProject 2 due W, 3/19\n\n\nWeek 10\n3/16-3/20\nNo new material - spring break\n\n\n\nWeek 11\n3/23-3/27\n01 - Modeling Recap\n02 - Modeling Example\n03 - Logistic Regression Basics\n04 - Logistic Regression Extensions\n05 - Regularized Regression\nHW 7 due W, 3/26\n\n\nWeek 12\n3/30-4/3\n06 - Loss Functions & Model Performance\n07 - Classification & Regression Trees\n08 - Bagging Trees & Random Forests\n09 - kNN\nHW 8 due W, 4/2\n\n\nWeek 13\n4/6-4/10\n10 - Spark MLlib Basics\n11 - Model Pipelines in MLlib\n12 - MLflow\n13 - MLOps\nHW 9 due W, 4/9\n\n\nWeek 14\n4/13-4/17\n01 - Streaming Data Concepts\n02 - Common Streaming Tasks\n03 - Spark Structured Streaming\n04 - Reading & Writing Streams with Spark Structured Streaming\nExam 2 Th/F 4/10-4/11 - covers weeks 1-13\n(emphasis on 5-13)\n\n\nWeek 15\n4/20-4/24\n05 - Transformations, Windowing, & Aggregations\n06 - Streaming Joins\nFinal Project due M, 4/28\n\n\nWeek 16\n4/27-4/28\nNo new material.",
    "crumbs": [
      "Home",
      "Analysis of Big Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/07-Common_Uses_For_Data_Landing.html",
    "href": "01_Programming_in_python/07-Common_Uses_For_Data_Landing.html",
    "title": "Common Uses for Data",
    "section": "",
    "text": "The video below discusses the common ways we use data. We discuss the general idea of statistical learning and using data for descriptive purposes, for predictive purposes, in order to make statistical inferences, and to find patterns in data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Common Uses for Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/07-Common_Uses_For_Data_Landing.html#notes",
    "href": "01_Programming_in_python/07-Common_Uses_For_Data_Landing.html#notes",
    "title": "Common Uses for Data",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThis wraps up the content for week 1. You should head back to our Moodle site to check out your homework assignment for this week.\nOtherwise, use the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 1: Learning `python`",
      "Common Uses for Data"
    ]
  },
  {
    "objectID": "01_Programming_in_python/22-Big_Recap_Landing.html",
    "href": "01_Programming_in_python/22-Big_Recap_Landing.html",
    "title": "Big Recap!",
    "section": "",
    "text": "The video below attempts to help place us in our goals for the course. We discuss what we’ve done so far and where we are going. I highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Big Recap!"
    ]
  },
  {
    "objectID": "01_Programming_in_python/22-Big_Recap_Landing.html#notes",
    "href": "01_Programming_in_python/22-Big_Recap_Landing.html#notes",
    "title": "Big Recap!",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Big Recap!"
    ]
  },
  {
    "objectID": "01_Programming_in_python/22-Big_Recap_Landing.html#additional-readings-for-week-4",
    "href": "01_Programming_in_python/22-Big_Recap_Landing.html#additional-readings-for-week-4",
    "title": "Big Recap!",
    "section": "Additional Readings for Week 4",
    "text": "Additional Readings for Week 4\nHere are some useful resources for learning about modeling, metrics, etc.\n\nISLR book: Read sections\n\n(Big picture ideas) 2.1, 2.2\n(Basic regression) 3.1, 3.2, 3.3\n\n5.1\n\n(Regularized models) 6.2\n\nGoogle’s open course materials have information about these models, loss functions, metrics, etc:\nLASSO basics in python\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Big Recap!"
    ]
  },
  {
    "objectID": "01_Programming_in_python/24-Prediction_Testing_Training_Landing.html",
    "href": "01_Programming_in_python/24-Prediction_Testing_Training_Landing.html",
    "title": "Prediction and Training/Test Set Ideas",
    "section": "",
    "text": "The video below elaborates on the ways that we evaluate predictive models. We ideally want our models to perform well on data it isn’t trained on. In order to understand that behavior, we can split our data into a training and a test set. These ideas are vital for understanding how well our model works and applying more complicated methods like cross validation.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Prediction and Training/Test Set Ideas"
    ]
  },
  {
    "objectID": "01_Programming_in_python/24-Prediction_Testing_Training_Landing.html#notes",
    "href": "01_Programming_in_python/24-Prediction_Testing_Training_Landing.html#notes",
    "title": "Prediction and Training/Test Set Ideas",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Prediction and Training/Test Set Ideas"
    ]
  },
  {
    "objectID": "01_Programming_in_python/26-Multiple_Linear_Regression_Landing.html",
    "href": "01_Programming_in_python/26-Multiple_Linear_Regression_Landing.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The video below goes into the ideas of mutliple linear regression (MLR). This is the extension of the SLR model that allows for multiple predictors, qualitative predictors, interactions, and more!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "01_Programming_in_python/26-Multiple_Linear_Regression_Landing.html#notes",
    "href": "01_Programming_in_python/26-Multiple_Linear_Regression_Landing.html#notes",
    "title": "Multiple Linear Regression",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 3: Basics of Predictive Modeling",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html",
    "href": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html",
    "title": "Big Data Basics",
    "section": "",
    "text": "The video below gives a definition of big data and discusses common issues that arise when dealing with big data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Big Data Basics"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html#notes",
    "href": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html#notes",
    "title": "Big Data Basics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Big Data Basics"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html#additional-readings-for-week-7",
    "href": "02_Big_Data_Management/01-Big_Data_Basics_Landing.html#additional-readings-for-week-7",
    "title": "Big Data Basics",
    "section": "Additional Readings for Week 7",
    "text": "Additional Readings for Week 7\n\nThe Big Data Paradigm\n\nAcademic overview\nOverview 2 (Suse)\nOverview 3 (Oracle)\n\n\n\nDatabases\n\nBasics of Databases(SQL and NoSQL)\nWhat is a database?, what is a relational database? (Oracle)\nPython SQL Libraries (realpython - a nice site but some stuff goes out of date!)\nSQLite in python (tutorial from pynative.com)\nSQLite schema table (sqlite.org documentation - a lot of other useful stuff there)\nThree plus table joins (learnsql.com)\n\n\n\nData Storage\n\nThe first answer here is useful to read\nData Warehouses: Article 1 (Oracle), Article 2 (Amazon)\nDatabases and Data Warehouses: Article 1, Article 2\nData Marts (IBM)\nMDM (SAS)\nDatabases, Data Warehouses, and Data Lake: Article 1, Article 2, Article 3\nData Lakes: Article 1, Article 2\nLake House (databricks - they actually have a lot of useful training resources as well!)\n\n\n\nBig Data Storage\n\nData pipelines: Article 1, Article 2\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Big Data Basics"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/04-SQL_Joins_Landing.html",
    "href": "02_Big_Data_Management/04-SQL_Joins_Landing.html",
    "title": "SQL Joins",
    "section": "",
    "text": "The video below discusses SQL joins. These actions are used to combine data across two or more tables in a database. These are extremely important and useful in practice!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe notebook written in the video is available here.",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "SQL Joins"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/04-SQL_Joins_Landing.html#notes",
    "href": "02_Big_Data_Management/04-SQL_Joins_Landing.html#notes",
    "title": "SQL Joins",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "SQL Joins"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html",
    "href": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html",
    "title": "Legacy Big Data Software: Hadoop Distributed File System",
    "section": "",
    "text": "The video below discusses a legacy big data software called Hadoop along with the common Map-Reduce actions that Hadoop allows one to perform on big data. While not a widely used software at this point, understanding the language and ideas of what made Hadoop popular are important.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe Charles Dickens text analyzed can be found here.",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Legacy Big Data Software: Hadoop Distributed File System"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html#notes",
    "href": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html#notes",
    "title": "Legacy Big Data Software: Hadoop Distributed File System",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Legacy Big Data Software: Hadoop Distributed File System"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html#additional-readings-for-week-8",
    "href": "02_Big_Data_Management/07-Legacy_Software_HDFS_Landing.html#additional-readings-for-week-8",
    "title": "Legacy Big Data Software: Hadoop Distributed File System",
    "section": "Additional Readings for Week 8",
    "text": "Additional Readings for Week 8\n\nHadoop\n\nHadoop Tutorial, Hadoop Architecture, Wikipedia article on Hadoop, What is Hadoop?, Another Intro to Hadoop\nHadoop YARN: Docs, Article\nHDFS & the Cloud: Article 1, Book (not free though)\nS3 storage\nAzure Blob Storage\n\n\n\nMapReduce\n\nMapReduce Examples: Article 1, Article 2\nA good intro through an example using PySpark\nHadoop vs Spark (IBM)\n\n\n\nSpark\n\nWhat is spark?\nBasic Spark Tutorial\nSpark APIs (databricks)\nMore on DAGs\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "Legacy Big Data Software: Hadoop Distributed File System"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/09-Spark_for_Big_Data_Landing.html",
    "href": "02_Big_Data_Management/09-Spark_for_Big_Data_Landing.html",
    "title": "spark: Big Data Software",
    "section": "",
    "text": "The video below introduces the spark software for handling big data. This is a commonly used software to summarize, fit models, and handle streaming big data.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`spark`: Big Data Software"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/09-Spark_for_Big_Data_Landing.html#notes",
    "href": "02_Big_Data_Management/09-Spark_for_Big_Data_Landing.html#notes",
    "title": "spark: Big Data Software",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`spark`: Big Data Software"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/11-pyspark_pandas_on_Spark_Landing.html",
    "href": "02_Big_Data_Management/11-pyspark_pandas_on_Spark_Landing.html",
    "title": "pyspark: Pandas-on-Spark",
    "section": "",
    "text": "The video below discusses the use of pyspark and the pandas-on-spark framework. This utilizes a pandas style data frame and functionality.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe pyspark code used in the notes and the example done at the end of the notes is available in this notebook. You’ll need to download this .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.\nIf you are not an NC State student, you can download docker and gain access to Spark with a Jupyter notebook interface reasonably quickly!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`pyspark`: Pandas-on-Spark"
    ]
  },
  {
    "objectID": "02_Big_Data_Management/11-pyspark_pandas_on_Spark_Landing.html#notes",
    "href": "02_Big_Data_Management/11-pyspark_pandas_on_Spark_Landing.html#notes",
    "title": "pyspark: Pandas-on-Spark",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 4: Big Data Management",
      "`pyspark`: Pandas-on-Spark"
    ]
  },
  {
    "objectID": "03_Modeling_Data/01-Modeling_Data_Recap_Landing.html",
    "href": "03_Modeling_Data/01-Modeling_Data_Recap_Landing.html",
    "title": "Modeling Data Recap",
    "section": "",
    "text": "The video below recaps the big ideas involved in predictive modeling.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Modeling Data Recap"
    ]
  },
  {
    "objectID": "03_Modeling_Data/01-Modeling_Data_Recap_Landing.html#notes",
    "href": "03_Modeling_Data/01-Modeling_Data_Recap_Landing.html#notes",
    "title": "Modeling Data Recap",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Modeling Data Recap"
    ]
  },
  {
    "objectID": "03_Modeling_Data/01-Modeling_Data_Recap_Landing.html#additional-readings-for-week-11",
    "href": "03_Modeling_Data/01-Modeling_Data_Recap_Landing.html#additional-readings-for-week-11",
    "title": "Modeling Data Recap",
    "section": "Additional Readings for Week 11",
    "text": "Additional Readings for Week 11\n\nScikit-Learn Documentation\n\nAPI documentation\n\n\n\nModeling Readings\n\nIntrouction to Statistical Learning with Python\n\nChapter 3: sections 1, 2, and 3\nChapter 4: sections 1, 2, and 3\n\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Modeling Data Recap"
    ]
  },
  {
    "objectID": "03_Modeling_Data/03-Logistic_Regression_Basics_Landing.html",
    "href": "03_Modeling_Data/03-Logistic_Regression_Basics_Landing.html",
    "title": "Logistic Regression Basics",
    "section": "",
    "text": "The video below describes the basics of the logistic regression model and why we prefer it over linear regression for classification problems.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nNote: Depending on which computer I was on (and hence which version of sklearn I had) the notation for two things differed:",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Logistic Regression Basics"
    ]
  },
  {
    "objectID": "03_Modeling_Data/03-Logistic_Regression_Basics_Landing.html#notes",
    "href": "03_Modeling_Data/03-Logistic_Regression_Basics_Landing.html#notes",
    "title": "Logistic Regression Basics",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Logistic Regression Basics"
    ]
  },
  {
    "objectID": "03_Modeling_Data/05-Regularized_Regression_Landing.html",
    "href": "03_Modeling_Data/05-Regularized_Regression_Landing.html",
    "title": "Regularized Regression",
    "section": "",
    "text": "The video below goes further into the ideas of using regularization when fitting our linear regression and logistic regression models.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Regularized Regression"
    ]
  },
  {
    "objectID": "03_Modeling_Data/05-Regularized_Regression_Landing.html#notes",
    "href": "03_Modeling_Data/05-Regularized_Regression_Landing.html#notes",
    "title": "Regularized Regression",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThis wraps up week 11! Head to the Moodle site to get started on your next homework assignment or use the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Regularized Regression"
    ]
  },
  {
    "objectID": "03_Modeling_Data/07-Classification_And_Regression_Trees_Landing.html",
    "href": "03_Modeling_Data/07-Classification_And_Regression_Trees_Landing.html",
    "title": "Classification & Regression Trees",
    "section": "",
    "text": "The video below introduces tree based models.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Classification & Regression Trees"
    ]
  },
  {
    "objectID": "03_Modeling_Data/07-Classification_And_Regression_Trees_Landing.html#notes",
    "href": "03_Modeling_Data/07-Classification_And_Regression_Trees_Landing.html#notes",
    "title": "Classification & Regression Trees",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "Classification & Regression Trees"
    ]
  },
  {
    "objectID": "03_Modeling_Data/09-kNN_Landing.html",
    "href": "03_Modeling_Data/09-kNN_Landing.html",
    "title": "k Nearest Neighbors",
    "section": "",
    "text": "The video below looks at the non-parametric model called k Nearest Neighbors (kNN). This isn’t a model that we’ll be able to implement in spark but is useful to learn about.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "k Nearest Neighbors"
    ]
  },
  {
    "objectID": "03_Modeling_Data/09-kNN_Landing.html#notes",
    "href": "03_Modeling_Data/09-kNN_Landing.html#notes",
    "title": "k Nearest Neighbors",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\nLink to the shiny app used in the notes\n\nThis wraps up week 12! Head to the Moodle site to get started on your next homework assignment or use the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "k Nearest Neighbors"
    ]
  },
  {
    "objectID": "03_Modeling_Data/11-MLPipelines_Landing.html",
    "href": "03_Modeling_Data/11-MLPipelines_Landing.html",
    "title": "MLPipelines",
    "section": "",
    "text": "The video below discusses the use of pipelines in modeling.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe pyspark code used in the notes is available in this notebook and the data set is also available online. You’ll need to download the .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "MLPipelines"
    ]
  },
  {
    "objectID": "03_Modeling_Data/11-MLPipelines_Landing.html#notes",
    "href": "03_Modeling_Data/11-MLPipelines_Landing.html#notes",
    "title": "MLPipelines",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "MLPipelines"
    ]
  },
  {
    "objectID": "03_Modeling_Data/13-MLOps_Landing.html",
    "href": "03_Modeling_Data/13-MLOps_Landing.html",
    "title": "MLOps",
    "section": "",
    "text": "The video below gives a rough overview of the entire MLOps framework. This video and material are optional!\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "MLOps"
    ]
  },
  {
    "objectID": "03_Modeling_Data/13-MLOps_Landing.html#notes",
    "href": "03_Modeling_Data/13-MLOps_Landing.html#notes",
    "title": "MLOps",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nThis wraps up week 13! Head to the Moodle site to get started on your next homework assignment or use the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 5: Modeling Data",
      "MLOps"
    ]
  },
  {
    "objectID": "04_Streaming_Data/03-Spark_Structured_Streaming_Landing.html",
    "href": "04_Streaming_Data/03-Spark_Structured_Streaming_Landing.html",
    "title": "Spark Structured Streaming",
    "section": "",
    "text": "The video below gives our first example of working with Spark Structured Streaming. We go through an example where we read in text data coming into a folder and count the number of occurrences of each word.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe notebook used in the video is available here. You’ll need to download this .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Spark Structured Streaming"
    ]
  },
  {
    "objectID": "04_Streaming_Data/03-Spark_Structured_Streaming_Landing.html#notes",
    "href": "04_Streaming_Data/03-Spark_Structured_Streaming_Landing.html#notes",
    "title": "Spark Structured Streaming",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Spark Structured Streaming"
    ]
  },
  {
    "objectID": "04_Streaming_Data/05-Transformations_Windowing_Aggregations_Landing.html",
    "href": "04_Streaming_Data/05-Transformations_Windowing_Aggregations_Landing.html",
    "title": "Streaming Transformations, Windowing, & Aggregations",
    "section": "",
    "text": "The video below discusses how we can take streaming sources and transform them, group the observations and aggregate them, and how windowing is used to allow flexibility in how we do this.\nI highly recommend watching the video using the ‘full’ Panopto player. There is a ‘pop out’ button in the bottom right of the video to enter this viewer.\nThe notebook used in the video is available here. You’ll need to download this .ipynb file and upload it to your JupyterHub environment. Make sure that the kernel used to run the notebook is a pyspark kernel!\nRemember, if you are off campus you should log in to the VPN and then you can access our JupyterHub.",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Streaming Transformations, Windowing, & Aggregations"
    ]
  },
  {
    "objectID": "04_Streaming_Data/05-Transformations_Windowing_Aggregations_Landing.html#notes",
    "href": "04_Streaming_Data/05-Transformations_Windowing_Aggregations_Landing.html#notes",
    "title": "Streaming Transformations, Windowing, & Aggregations",
    "section": "Notes",
    "text": "Notes\n\nHTML version\nPDF version\n\nUse the table of contents on the left or the arrows at the bottom of this page to navigate to the next learning material!",
    "crumbs": [
      "Home",
      "Topic 6: Streaming Data",
      "Streaming Transformations, Windowing, & Aggregations"
    ]
  }
]