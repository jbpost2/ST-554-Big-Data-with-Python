{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203c08a2-aadd-4215-8cb7-96f3645f85bb",
   "metadata": {},
   "source": [
    "# Notebook Corresponding to `pyspark` Notes\n",
    "\n",
    "This notebook has the code and examples from the three sets of notes:\n",
    "- `pyspark`: RDDs\n",
    "- `pyspark`: pandas-on-Spark\n",
    "- `pyspark`: Spark SQL\n",
    "\n",
    "Each section should be able to be run without running the cells from the other sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e5e3e-1761-4c8a-8076-13c4dc23ff83",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "First, the code from the notes is given below. Make sure that the kernel chosen to run these notes is the `pyspark` kernel. See the top right of the notebook!\n",
    "\n",
    "Note: We can still run python code and load in libraries as normal.\n",
    "\n",
    "Now, we start by creating our spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb4cb43-2095-4aed-bf0d-031c45386f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/02 10:35:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640b518-82cb-4bb2-9028-58012d9d1c7e",
   "metadata": {},
   "source": [
    "Let's populate a list with tuples and explicitly create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9050c48-fa70-4d5c-89a4-a4e8a4d49f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate a list with tuples\n",
    "quick_cat = lambda x: \"a\" if x < 20 else \"b\"\n",
    "my_data = [(quick_cat(x), x) for x in range(1,51)]\n",
    "my_data[:3]\n",
    "#create the RDD\n",
    "my_rdd = spark.sparkContext.parallelize(my_data)\n",
    "my_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149b23a-dec4-4d81-bb70-0f2f67a1b6da",
   "metadata": {},
   "source": [
    "This is an object stored (likely) over multiple partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d58fc-7d7a-41ae-bcae-cf3ad9ee4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810eb13-04d1-458b-b53b-54a8a6b14ba8",
   "metadata": {},
   "source": [
    "We see that `my_rdd` doesn't actually print out the data when we look at the object. This is because there may be a ton of data and it doesn't want to show it to you by default. Instead we can perform an action like the `.take()` to actually have some data returned to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d264e-1bf0-4b4d-98f6-d19ccefd3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c218ba-3a06-4a5e-8ad6-59265dfb22de",
   "metadata": {},
   "source": [
    "When we have tuple type object passed in as the data, the first value represents the `key` and the second the associated `values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2dd10-cc29-4f5b-bfec-d33bf45f6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.keys().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f261db4-7596-485b-a66e-5f01de8d2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.values().take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54943f-d411-4e24-9050-9c2c60b584f9",
   "metadata": {},
   "source": [
    "This allows us to do operations by key if we'd like! Note that `.count()` and `.countByKey()` are actions and so they return the value locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa254c12-77c2-465e-a892-dbf98a975c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407508a-1ee0-4fa2-92eb-ed62a6811fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bb5e2-4ad7-4708-a68a-8855c6080b97",
   "metadata": {},
   "source": [
    "If instead we wanted to use the result of this counting operation as a new RDD, we could instead use something like the `mapValues()` method. This returns an RDD rather than a value and so we need to use `.collect()` to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce765d-8b2b-40da-a78c-ac6ed9b2c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(len) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692320f-3775-4727-a2c3-15322d8bdb44",
   "metadata": {},
   "source": [
    "With this, we could do some other transformation on the resulting object (say using `.map()`, which can apply a function to each element of our RDD). For instance, creating a log transformed value as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620c6c5-500f-402f-835b-718cd2a363ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "my_rdd \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(len) \\\n",
    "    .map(lambda x: (x[0], x[1], log(x[1]))) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f5d99-ae89-40c4-b719-c7851d22c40a",
   "metadata": {},
   "source": [
    "RDD functions are hard to use though! We might want to find the total sum of the values for each key. We can use `.groupByKey()` and `mapValues()` for this but the documentation says it is better to use `aggregateByKey()`. But this function requires some confusing arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d6567-eda5-47f8-97bd-c47391e7d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(sum) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a482d9-396b-4d4c-8691-f045d7bcfdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd \\\n",
    "  .aggregateByKey(0, #initial value for each partition\n",
    "                  lambda within_1, within_2: within_1 + within_2, #how to combine values on the same partition, next function is how to combine across partitions\n",
    "                  lambda across_1, across_2: across_1 + across_2) \\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06344b7-4e5a-4c60-b8bd-4179946cdbe0",
   "metadata": {},
   "source": [
    "We can use `.map()` instead if we wanted to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea137c95-8c1c-478b-be5d-524e1ccf66c6",
   "metadata": {},
   "source": [
    "### MapReduce Example Done Explicity Using RDDs\n",
    "\n",
    "Recall that in the Hadoop section, we did a MapReduce algorithm to count the number of words in Oliver Twist.  We can redo that example using Spark!  It will actually be parallelized and all that automatically across our machine too!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318405cf-b290-4a78-83ad-c611e641feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/02 10:36:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#create a spark session object (simplified, defaults to local)\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1b761-8f60-4dd3-98f1-2f69d074367a",
   "metadata": {},
   "source": [
    "Now let's read in our 53 chapters as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e6acba-b51a-4866-abdb-fe89b8673588",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap = []\n",
    "for i in range(1, 54):\n",
    "    with open('dickens/chap' + str(i) + '.txt', 'r') as f:\n",
    "        my_chap.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e147c7-6f99-46fd-89b9-2d968b16d133",
   "metadata": {},
   "source": [
    "We want spark to handle this using RDDs explicitly. We can do that is via the `sparkContext.parallelize()` method.  This just tells spark to take our list and distribute it/prepare it for parallel computations.\n",
    "\n",
    "Let's create some RDDs! We don't care about the chapters themselves, we just want the final word counts. This means we can start with an RDD without keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b9214a-d13e-417d-9a6e-7bcc5a186fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chap_rdd = spark.sparkContext.parallelize(my_chap)\n",
    "type(my_chap_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6143f6d4-cedf-4bcf-9435-8dbc51e7be29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chapter i  treats of the place where oliver twist was born and of the circumstances attending his birth  among other public buildings in a certain town which for many reasons it will be prudent to refrain from mentioning and to which i will assign no fictitious name there is one anciently common to most towns great or small to wit a workhouse and in this workhouse was born on a day and date which i need not trouble myself to repeat inasmuch as it can be of no possible consequence to the reader in this stage of the business at all events the item of mortality whose name is prefixed to the head of this chapter  for a long time after it was ushered into this world of sorrow and trouble by the parish surgeon it remained a matter of considerable doubt whether the child would survive to bear any name at all in which case it is somewhat more than probable that these memoirs would never have appeared or if they had that being comprised within a couple of pages they would have possessed the inestimable merit of being the most concise and faithful specimen of biography extant in the literature of any age or country  although i am not disposed to maintain that the being born in a workhouse is in itself the most fortunate and enviable circumstance that can possibly befall a human being i do mean to say that in this particular instance it was the best thing for oliver twist that could by possibility have occurred  the fact is that there was considerable difficulty in inducing oliver to take upon himself the office of respirationa troublesome practice but one which custom has rendered necessary to our easy existence and for some time he lay gasping on a little flock mattress rather unequally poised between this world and the next the balance being decidedly in favour of the latter  now if during this brief period oliver had been surrounded by careful grandmothers anxious aunts experienced nurses and doctors of profound wisdom he would most inevitably and indubitably have been killed in no time  there being nobody by however but a pauper old woman who was rendered rather misty by an unwonted allowance of beer and a parish surgeon who did such matters by contract oliver and nature fought out the point between them  the result was that after a few struggles oliver breathed sneezed and proceeded to advertise to the inmates of the workhouse the fact of a new burden having been imposed  upon the parish by setting up as loud a cry as could reasonably have been expected from a male infant who had not been possessed of that very useful appendage a voice for a much longer space of time than three minutes and a quarter  as oliver gave this first proof of the free and proper action of his lungs the patchwork coverlet which was carelessly flung over the iron bedstead rustled the pale face of a young woman was raised feebly from the pillow and a faint voice imperfectly articulated the words let me see the child and die  the surgeon had been sitting with his face turned towards the fire giving the palms of his hands a warm and a rub alternately  as the young woman spoke he rose and advancing to the beds head said with more kindness than might have been expected of him  oh you must not talk about dying yet  lor bless her dear heart no interposed the nurse hastily depositing in her pocket a green glass bottle the contents of which she had been tasting in a corner with evident satisfaction  lor bless her dear heart when she has lived as long as i have sir and had thirteen children of her own and all on em dead except two and them in the wurkus with me shell know better than to take on in that way bless her dear heart  think what it is to be a mother theres a dear young lamb do  apparently this consolatory perspective of a mothers prospects failed in producing its due effect  the patient shook her head and stretched out her hand towards the child  the surgeon deposited it in her arms  she imprinted her cold white lips passionately on its forehead passed her hands over her face gazed wildly round shuddered fell backand died they chafed her breast hands and temples but the blood had stopped forever  they talked of hope and comfort they had been strangers too long  its all over mrs thingummy said the surgeon at last  ah poor dear so it is said the nurse picking up the cork of the green bottle which had fallen out on the pillow as she stooped to take up the child  poor dear  you neednt mind sending up to me if the child cries nurse said the surgeon putting on his gloves with great deliberation its very likely it will be troublesome give it a little gruel if it is  he put on his hat and pausing by the bedside on his way to the door added she was a goodlooking girl too where did she come from  she was brought here last night replied the old woman by the overseers order  she was found lying in the street  she had walked some distance for her shoes were worn to pieces but where she came from or where she was going to nobody knows  the surgeon leaned over the body and raised the left hand  the old story he said shaking his head no weddingring i see ah goodnight  the medical gentleman walked away to dinner and the nurse having once more applied herself to the green bottle sat down on a low chair before the fire and proceeded to dress the infant  what an excellent example of the power of dress young oliver twist was  wrapped in the blanket which had hitherto formed his only covering he might have been the child of a nobleman or a beggar it would have been hard for the haughtiest stranger to have assigned him his proper station in society  but now that he was enveloped in the old calico robes which had grown yellow in the same service he was badged and ticketed and fell into his place at oncea parish childthe orphan of a workhousethe humble halfstarved drudgeto be cuffed and buffeted through the worlddespised by all and pitied by none  oliver cried lustily if he could have known that he was an orphan left to the tender mercies of churchwardens and overseers perhaps he would have cried the louder    ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chap_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e09e31-e941-4ff4-9358-d7e9ea201a9e",
   "metadata": {},
   "source": [
    "Great, now we want to take these different values (each chapter is a value) and split those strings by spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe13e9e3-27ef-452d-bf24-f2f7c4575e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'i',\n",
       " '',\n",
       " 'treats',\n",
       " 'of',\n",
       " 'the',\n",
       " 'place',\n",
       " 'where',\n",
       " 'oliver',\n",
       " 'twist']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2decf08-9890-4f84-9393-bc67ca3b5232",
   "metadata": {},
   "source": [
    "Now we have an RDD whose elements are each word (again no keys). First let's filter to remove any empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35dec7d2-a7ac-48c0-b972-17a52614254e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'i',\n",
       " 'treats',\n",
       " 'of',\n",
       " 'the',\n",
       " 'place',\n",
       " 'where',\n",
       " 'oliver',\n",
       " 'twist',\n",
       " 'was']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320e16f-eb5a-4470-b37b-89cec026f7c8",
   "metadata": {},
   "source": [
    "Let's do a transform on this where we make each word a key and assign it a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382519fb-4ffd-439c-aec0-fea968f25532",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09146e-6b56-4607-b2b9-2d99b0dd64bf",
   "metadata": {},
   "source": [
    "Nice! Now we just need to reduce this by key and we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70be36-fdcf-4f64-b8ac-a1ab0da2d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5b577-8863-427f-903a-0612104282c5",
   "metadata": {},
   "source": [
    "Let's sort this by the keys. We can sort descending by simply sorting on the negative of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3cfe85-78c6-49ec-afb0-0dcbeb2d27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: -x[1]) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51aa436-cdbe-4d9c-912d-093449597b4a",
   "metadata": {},
   "source": [
    "Let's collect all of the data and turn it into a regular pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4a2ba-d62b-4f8a-a31e-ab4e049db4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: -x[1]) \\\n",
    "    .collect()\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results, columns = [\"word\", \"count\"])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6e482-98bc-4881-87cb-671cc21734ba",
   "metadata": {},
   "source": [
    "## pandas-on-Spark\n",
    "\n",
    "Below is the code from the notes on pandas-on-Spark.\n",
    "\n",
    "First let's import our modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a36f306-0a70-4f04-a677-427e0edeefb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51847416-7026-4751-932d-b178a0e85abc",
   "metadata": {},
   "source": [
    "Create a pandas-on-Spark series via `ps.Series()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09dbbe0a-ca73-4dae-9959-bf8b4f72aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/02 10:39:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    3.0\n",
       "2    5.0\n",
       "3    NaN\n",
       "4    6.0\n",
       "5    8.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.Series([1, 3, 5, np.nan, 6, 8]) #ignore the warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdca4f2-bd47-4110-ac3b-547f5e83baf0",
   "metadata": {},
   "source": [
    "Create a pandas-on-Spark DataFrame via `ps.DataFrame()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bedeef8a-47da-434a-abfc-1d520b863925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b      c\n",
       "10  1  100    one\n",
       "20  2  200    two\n",
       "30  3  300  three\n",
       "40  4  400   four\n",
       "50  5  500   five\n",
       "60  6  600    six"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140622b6-5a2c-4e47-8bc4-17f0286a1e24",
   "metadata": {},
   "source": [
    "Convert from a pandas DataFrame to a pandas-on-spark easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc188b2-a151-49ce-8c6a-a588182fcde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  alcohol  quality\n",
       "0            7.4              0.70         0.00             1.9      0.076                 11.0                  34.0   0.9978  3.51       0.56      9.4        5\n",
       "1            7.8              0.88         0.00             2.6      0.098                 25.0                  67.0   0.9968  3.20       0.68      9.8        5\n",
       "2            7.8              0.76         0.04             2.3      0.092                 15.0                  54.0   0.9970  3.26       0.65      9.8        5\n",
       "3           11.2              0.28         0.56             1.9      0.075                 17.0                  60.0   0.9980  3.16       0.58      9.8        6\n",
       "4            7.4              0.70         0.00             1.9      0.076                 11.0                  34.0   0.9978  3.51       0.56      9.4        5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/red-wine.csv\", delimiter = \";\")\n",
    "psdf = ps.from_pandas(pdf)\n",
    "psdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53941f79-d7e0-490d-9549-6c73c0d39ace",
   "metadata": {},
   "source": [
    "Can subset the data using things we know like the `.loc[]` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4279000c-02cd-4efa-8193-9fc315d720bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  quality\n",
       "3       9.8        6\n",
       "7      10.0        7\n",
       "8       9.5        7\n",
       "16     10.5        7\n",
       "19      9.2        6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psdf.loc[psdf.quality > 5, [\"alcohol\", \"quality\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a14665-2640-4260-a275-b55c75735486",
   "metadata": {},
   "source": [
    "Can also read data directly into a pandas-on-spark data frame using the `ps.read_csv()` function (can't read from a URL though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433835c0-2a31-44f5-adf4-56bc202017db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    809\n",
       "1    500\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_ps = ps.read_csv(\"data/titanic.csv\") #data uploaded to jhub in data folder\n",
    "titanic_ps[\"survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a5fc6-2976-409a-a061-6295299abfee",
   "metadata": {},
   "source": [
    "Can now do our usual summarizations using the `.groupby()` method along with a summarization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20862a3d-7601-4ef7-b89c-9bbe58e5b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/groupby.py:649: FutureWarning: Default value of `numeric_only` will be changed to `False` instead of `True` in 4.0.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.962000</td>\n",
       "      <td>28.918228</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>49.361184</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.500618</td>\n",
       "      <td>30.545369</td>\n",
       "      <td>0.521632</td>\n",
       "      <td>0.328801</td>\n",
       "      <td>23.353831</td>\n",
       "      <td>160.809917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pclass        age     sibsp     parch       fare        body\n",
       "survived                                                                \n",
       "1         1.962000  28.918228  0.462000  0.476000  49.361184         NaN\n",
       "0         2.500618  30.545369  0.521632  0.328801  23.353831  160.809917"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_ps.groupby(\"survived\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b997d96-884d-46ff-b8f1-3160d8e09165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 10:39:16 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1046.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1308.000000</td>\n",
       "      <td>121.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.294882</td>\n",
       "      <td>0.381971</td>\n",
       "      <td>29.881135</td>\n",
       "      <td>0.498854</td>\n",
       "      <td>0.385027</td>\n",
       "      <td>33.295479</td>\n",
       "      <td>160.809917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.837836</td>\n",
       "      <td>0.486055</td>\n",
       "      <td>14.413500</td>\n",
       "      <td>1.041658</td>\n",
       "      <td>0.865560</td>\n",
       "      <td>51.758668</td>\n",
       "      <td>97.696922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.275000</td>\n",
       "      <td>256.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>328.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pclass     survived          age        sibsp        parch         fare        body\n",
       "count  1309.000000  1309.000000  1046.000000  1309.000000  1309.000000  1308.000000  121.000000\n",
       "mean      2.294882     0.381971    29.881135     0.498854     0.385027    33.295479  160.809917\n",
       "std       0.837836     0.486055    14.413500     1.041658     0.865560    51.758668   97.696922\n",
       "min       1.000000     0.000000     0.166700     0.000000     0.000000     0.000000    1.000000\n",
       "25%       2.000000     0.000000    21.000000     0.000000     0.000000     7.895800   72.000000\n",
       "50%       3.000000     0.000000    28.000000     0.000000     0.000000    14.454200  155.000000\n",
       "75%       3.000000     1.000000    39.000000     1.000000     0.000000    31.275000  256.000000\n",
       "max       3.000000     1.000000    80.000000     8.000000     9.000000   512.329200  328.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_ps.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce400c9-84ab-4751-82b1-9d48c24ea4b3",
   "metadata": {},
   "source": [
    "We can also use the `.transform()` and `.apply()` methods (also used in regular pandas) to perform other common operations.\n",
    "\n",
    "First we can transform the values in our columns (say center and scale them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e564ab-5fd4-4a87-b7ca-5ab7b4315840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(pser) -> ps.Series[np.float64]:\n",
    "     return (pser + pser.mean())/pser.std()  # should always return the same length as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99fde9c-8979-4a1a-84dc-8b4cb1a7388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o_age</th>\n",
       "      <th>o_fare</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>4.085138</td>\n",
       "      <td>4.726416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>2.136735</td>\n",
       "      <td>3.571295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>2.211894</td>\n",
       "      <td>3.571295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>4.154517</td>\n",
       "      <td>3.571295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>3.807620</td>\n",
       "      <td>3.571295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     o_age    o_fare       age      fare\n",
       "0  29.0000  211.3375  4.085138  4.726416\n",
       "1   0.9167  151.5500  2.136735  3.571295\n",
       "2   2.0000  151.5500  2.211894  3.571295\n",
       "3  30.0000  151.5500  4.154517  3.571295\n",
       "4  25.0000  151.5500  3.807620  3.571295"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_res = titanic_ps[[\"age\", \"fare\"]] \\\n",
    "    .rename(columns = {\"age\": \"o_age\", \"fare\": \"o_fare\"}) \\\n",
    "    .join(titanic_ps[[\"age\", \"fare\"]]\n",
    "              .transform(standardize))\n",
    "std_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46271c3-3ba4-48d9-a059-2d52236295c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1310, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db65dda-c6ab-4a3d-8579-b3ba72890f84",
   "metadata": {},
   "source": [
    "Can use `.apply()` to possible return something shorter than the original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "269a0bf6-6a15-46be-8f0b-703efbeaf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_positives(pser) -> ps.Series[np.float64]:\n",
    "     return (pser[pser>30] + pser[pser>30].mean())/pser[pser>30].std()\n",
    "# can return something short than input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86e68a9b-84fa-4c28-a388-4e6573a21a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.135889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.636052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.235791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.635943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.436139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age\n",
       "0   9.135889\n",
       "1  10.636052\n",
       "2   8.235791\n",
       "3   9.635943\n",
       "4  11.436139"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_pos = titanic_ps[[\"age\"]].apply(standardize_positives)\n",
    "std_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "270d1325-9321-4575-b3b4-1c46c3ec00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(437, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c7090-c1a8-4ae9-8ac2-31b6da2cdfcb",
   "metadata": {},
   "source": [
    "### MapReduce Example Done via pandas-on-Spark\n",
    "\n",
    "Let's repeat our map reduce example but, you know, do it more easily :)\n",
    "\n",
    "Recall the `my_chap` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f00be5-f432-420c-8814-55a05d1c3393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter i  treats of the place where oliver twist was born and of the circumstances attending his birth  among other public buildings in a certain town which for many reasons it will be prudent to refrain from mentioning and to which i will assign no fictitious name there is one anciently common to most towns great or small to wit a workhouse and in this workhouse was born on a day and date which i need not trouble myself to repeat inasmuch as it can be of no possible consequence to the reader in this stage of the business at all events the item of mortality whose name is prefixed to the head of this chapter  for a long time after it was ushered into this world of sorrow and trouble by the parish surgeon it remained a matter of considerable doubt whether the child would survive to bear any name at all in which case it is somewhat more than probable that these memoirs would never have appeared or if they had that being comprised within a couple of pages they would have possessed the inestimable merit of being the most concise and faithful specimen of biography extant in the literature of any age or country  although i am not disposed to maintain that the being born in a workhouse is in itself the most fortunate and enviable circumstance that can possibly befall a human being i do mean to say that in this particular instance it was the best thing for oliver twist that could by possibility have occurred  the fact is that there was considerable difficulty in inducing oliver to take upon himself the office of respirationa troublesome practice but one which custom has rendered necessary to our easy existence and for some time he lay gasping on a little flock mattress rather unequally poised between this world and the next the balance being decidedly in favour of the latter  now if during this brief period oliver had been surrounded by careful grandmothers anxious aunts experienced nurses and doctors of profound wisdom he would most inevitably and indubitably have been killed in no time  there being nobody by however but a pauper old woman who was rendered rather misty by an unwonted allowance of beer and a parish surgeon who did such matters by contract oliver and nature fought out the point between them  the result was that after a few struggles oliver breathed sneezed and proceeded to advertise to the inmates of the workhouse the fact of a new burden having been imposed  upon the parish by setting up as loud a cry as could reasonably have been expected from a male infant who had not been possessed of that very useful appendage a voice for a much longer space of time than three minutes and a quarter  as oliver gave this first proof of the free and proper action of his lungs the patchwork coverlet which was carelessly flung over the iron bedstead rustled the pale face of a young woman was raised feebly from the pillow and a faint voice imperfectly articulated the words let me see the child and die  the surgeon had been sitting with his face turned towards the fire giving the palms of his hands a warm and a rub alternately  as the young woman spoke he rose and advancing to the beds head said with more kindness than might have been expected of him  oh you must not talk about dying yet  lor bless her dear heart no interposed the nurse hastily depositing in her pocket a green glass bottle the contents of which she had been tasting in a corner with evident satisfaction  lor bless her dear heart when she has lived as long as i have sir and had thirteen children of her own and all on em dead except two and them in the wurkus with me shell know better than to take on in that way bless her dear heart  think what it is to be a mother theres a dear young lamb do  apparently this consolatory perspective of a mothers prospects failed in producing its due effect  the patient shook her head and stretched out her hand towards the child  the surgeon deposited it in her arms  she imprinted her cold white lips passionately on its forehead passed her hands over her face gazed wildly round shuddered fell backand died they chafed her breast hands and temples but the blood had stopped forever  they talked of hope and comfort they had been strangers too long  its all over mrs thingummy said the surgeon at last  ah poor dear so it is said the nurse picking up the cork of the green bottle which had fallen out on the pillow as she stooped to take up the child  poor dear  you neednt mind sending up to me if the child cries nurse said the surgeon putting on his gloves with great deliberation its very likely it will be troublesome give it a little gruel if it is  he put on his hat and pausing by the bedside on his way to the door added she was a goodlooking girl too where did she come from  she was brought here last night replied the old woman by the overseers order  she was found lying in the street  she had walked some distance for her shoes were worn to pieces but where she came from or where she was going to nobody knows  the surgeon leaned over the body and raised the left hand  the old story he said shaking his head no weddingring i see ah goodnight  the medical gentleman walked away to dinner and the nurse having once more applied herself to the green bottle sat down on a low chair before the fire and proceeded to dress the infant  what an excellent example of the power of dress young oliver twist was  wrapped in the blanket which had hitherto formed his only covering he might have been the child of a nobleman or a beggar it would have been hard for the haughtiest stranger to have assigned him his proper station in society  but now that he was enveloped in the old calico robes which had grown yellow in the same service he was badged and ticketed and fell into his place at oncea parish childthe orphan of a workhousethe humble halfstarved drudgeto be cuffed and buffeted through the worlddespised by all and pitied by none  oliver cried lustily if he could have known that he was an orphan left to the tender mercies of churchwardens and overseers perhaps he would have cried the louder    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "#read in each chapter to a list element\n",
    "my_chap = []\n",
    "for i in range(1, 54):\n",
    "    with open('dickens/chap' + str(i) + '.txt', 'r') as f:\n",
    "        my_chap.append(f.read())\n",
    "        \n",
    "my_chap[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b633507-518d-47fb-822d-3dd2d961988f",
   "metadata": {},
   "source": [
    "Let's put this into a pandas-on-Spark series and manipulate from there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d479ebc3-9369-458e-b00a-469b7329437e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    chapter i  treats of the place where oliver tw...\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine the list elements into one large string\n",
    "from functools import reduce\n",
    "big_string = reduce(lambda x, y: x + y, my_chap)\n",
    "\n",
    "#create a series with the big string\n",
    "chap_pss = ps.Series(big_string)\n",
    "chap_pss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c22055-fe0b-4c6d-97a0-d3e66bd54834",
   "metadata": {},
   "source": [
    "Now use the `.str.split()` method on a pandas-on-Spark series to create a series with a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b5182-6f5c-47d3-8c87-79341df01c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de95a3a-427a-4727-a156-401254e8be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a56749a-d961-471d-ac52-ccb16eaa211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                       this is a regular sentence\n",
       "1    https://docs.python.org/3/tutorial/index.html\n",
       "2                                             None\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ps.Series([\"this is a regular sentence\",\n",
    "               \"https://docs.python.org/3/tutorial/index.html\",\n",
    "               np.nan])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa8c79fb-59f6-4254-9d5a-ca7cc0aa1188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 11:03:28 ERROR Executor: Exception in task 60.0 in stage 30.0 (TID 844)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/03/02 11:03:28 ERROR TaskSetManager: Task 60 in stage 30.0 failed 1 times; aborting job\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 5.0 in stage 30.0 (TID 789) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 50.0 in stage 30.0 (TID 834) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 48.0 in stage 30.0 (TID 832) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 9.0 in stage 30.0 (TID 793) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 16.0 in stage 30.0 (TID 800) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 36.0 in stage 30.0 (TID 820) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 20.0 in stage 30.0 (TID 804) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 49.0 in stage 30.0 (TID 833) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 29.0 in stage 30.0 (TID 813) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 7.0 in stage 30.0 (TID 791) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 71.0 in stage 30.0 (TID 855) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 63.0 in stage 30.0 (TID 847) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 70.0 in stage 30.0 (TID 854) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 44.0 in stage 30.0 (TID 828) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 8.0 in stage 30.0 (TID 792) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 17.0 in stage 30.0 (TID 801) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 11:03:28 WARN TaskSetManager: Lost task 56.0 in stage 30.0 (TID 840) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 60 in stage 30.0 failed 1 times, most recent failure: Lost task 60.0 in stage 30.0 (TID 844) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n",
      "  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n",
      "  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n",
      "  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\n",
      "TypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n    data = sanitize_array(data, index, dtype, copy)\n  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n    subarr = maybe_infer_to_datetimelike(data)\n  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n    return lib.maybe_convert_objects(  # type: ignore[return-value]\n  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\nTypeError: Cannot convert numpy.ndarray to numpy.ndarray\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/series.py:7342\u001b[0m, in \u001b[0;36mSeries.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_display_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   7338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_internal_pandas()\u001b[38;5;241m.\u001b[39mto_string(\n\u001b[1;32m   7339\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   7340\u001b[0m     )\n\u001b[0;32m-> 7342\u001b[0m pser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_psdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_repr_pandas_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_display_count\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname]\n\u001b[1;32m   7343\u001b[0m pser_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pser)\n\u001b[1;32m   7344\u001b[0m pser \u001b[38;5;241m=\u001b[39m pser\u001b[38;5;241m.\u001b[39miloc[:max_display_count]\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/frame.py:13393\u001b[0m, in \u001b[0;36mDataFrame._get_or_create_repr_pandas_cache\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m  13390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_or_create_repr_pandas_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries]:\n\u001b[1;32m  13391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache:\n\u001b[1;32m  13392\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\n\u001b[0;32m> 13393\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {n: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m  13394\u001b[0m         )\n\u001b[1;32m  13395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache[n]\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/frame.py:13388\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_internal_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m  13383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  13384\u001b[0m \u001b[38;5;124;03m    Return a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m  13385\u001b[0m \n\u001b[1;32m  13386\u001b[0m \u001b[38;5;124;03m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m  13387\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 13388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/utils.py:600\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_lazy_property\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[0;32m--> 600\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/internal.py:1115\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1115\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1117\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m   1118\u001b[0m         {field\u001b[38;5;241m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m sdf\u001b[38;5;241m.\u001b[39mschema}\n\u001b[1;32m   1119\u001b[0m     )\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"pyarrow/array.pxi\", line 883, in pyarrow.lib._PandasConvertible.to_pandas\n  File \"pyarrow/table.pxi\", line 475, in pyarrow.lib.ChunkedArray._to_pandas\n  File \"pyarrow/array.pxi\", line 1871, in pyarrow.lib._array_like_to_pandas\n  File \"pyarrow/pandas-shim.pxi\", line 115, in pyarrow.lib._PandasAPIShim.series\n  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n    data = sanitize_array(data, index, dtype, copy)\n  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n    subarr = maybe_infer_to_datetimelike(data)\n  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1189, in maybe_infer_to_datetimelike\n    return lib.maybe_convert_objects(  # type: ignore[return-value]\n  File \"lib.pyx\", line 2538, in pandas._libs.lib.maybe_convert_objects\nTypeError: Cannot convert numpy.ndarray to numpy.ndarray\n"
     ]
    }
   ],
   "source": [
    "s.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb7178c0-69f9-4eed-af0c-b9a692082270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/02 10:45:19 ERROR Executor: Exception in task 2.0 in stage 78.0 (TID 1426)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/strings.py\", line 2030, in pudf\n",
      "    return s.str.split(pat, n)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/strings/accessor.py\", line 137, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "TypeError: split() takes from 1 to 2 positional arguments but 3 were given\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/02 10:45:19 WARN TaskSetManager: Lost task 2.0 in stage 78.0 (TID 1426) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/strings.py\", line 2030, in pudf\n",
      "    return s.str.split(pat, n)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/strings/accessor.py\", line 137, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "TypeError: split() takes from 1 to 2 positional arguments but 3 were given\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "25/03/02 10:45:19 ERROR TaskSetManager: Task 2 in stage 78.0 failed 1 times; aborting job\n",
      "25/03/02 10:45:19 WARN TaskSetManager: Lost task 0.0 in stage 78.0 (TID 1424) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 78.0 failed 1 times, most recent failure: Lost task 2.0 in stage 78.0 (TID 1426) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/strings.py\", line 2030, in pudf\n",
      "    return s.str.split(pat, n)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/strings/accessor.py\", line 137, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "TypeError: split() takes from 1 to 2 positional arguments but 3 were given\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/02 10:45:19 WARN TaskSetManager: Lost task 1.0 in stage 78.0 (TID 1425) (jhub.cos.ncsu.edu executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 78.0 failed 1 times, most recent failure: Lost task 2.0 in stage 78.0 (TID 1426) (jhub.cos.ncsu.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/strings.py\", line 2030, in pudf\n",
      "    return s.str.split(pat, n)\n",
      "  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/strings/accessor.py\", line 137, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "TypeError: split() takes from 1 to 2 positional arguments but 3 were given\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/strings.py\", line 2030, in pudf\n    return s.str.split(pat, n)\n  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/strings/accessor.py\", line 137, in wrapper\n    return func(self, *args, **kwargs)\nTypeError: split() takes from 1 to 2 positional arguments but 3 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/series.py:7342\u001b[0m, in \u001b[0;36mSeries.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_display_count \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   7338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_internal_pandas()\u001b[38;5;241m.\u001b[39mto_string(\n\u001b[1;32m   7339\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   7340\u001b[0m     )\n\u001b[0;32m-> 7342\u001b[0m pser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_psdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_repr_pandas_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_display_count\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname]\n\u001b[1;32m   7343\u001b[0m pser_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pser)\n\u001b[1;32m   7344\u001b[0m pser \u001b[38;5;241m=\u001b[39m pser\u001b[38;5;241m.\u001b[39miloc[:max_display_count]\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/frame.py:13393\u001b[0m, in \u001b[0;36mDataFrame._get_or_create_repr_pandas_cache\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m  13390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_or_create_repr_pandas_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pd\u001b[38;5;241m.\u001b[39mSeries]:\n\u001b[1;32m  13391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache:\n\u001b[1;32m  13392\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\n\u001b[0;32m> 13393\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_repr_pandas_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {n: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m  13394\u001b[0m         )\n\u001b[1;32m  13395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_pandas_cache[n]\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/frame.py:13388\u001b[0m, in \u001b[0;36mDataFrame._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_internal_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m  13383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  13384\u001b[0m \u001b[38;5;124;03m    Return a pandas DataFrame directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m  13385\u001b[0m \n\u001b[1;32m  13386\u001b[0m \u001b[38;5;124;03m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m  13387\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 13388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/utils.py:600\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_lazy_property\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[0;32m--> 600\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/internal.py:1115\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1115\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1117\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m   1118\u001b[0m         {field\u001b[38;5;241m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m sdf\u001b[38;5;241m.\u001b[39mschema}\n\u001b[1;32m   1119\u001b[0m     )\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1257\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1257\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/strings.py\", line 2030, in pudf\n    return s.str.split(pat, n)\n  File \"/home/jupyter-jbpost2@ncsu.edu/.local/lib/python3.9/site-packages/pandas/core/strings/accessor.py\", line 137, in wrapper\n    return func(self, *args, **kwargs)\nTypeError: split() takes from 1 to 2 positional arguments but 3 were given\n"
     ]
    }
   ],
   "source": [
    "chap_pss.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c8964-8ee4-4a41-a6ff-cef974cb4441",
   "metadata": {},
   "source": [
    "What we want is to have each word be in a column with the count associated as another column. Let's convert the list stored in the series to a data frame. Then remove the empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7503e81-53b1-450c-b11c-7bfa6c610cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = ps.DataFrame(chap_pss.str.split(\" \")[0], columns = [\"word\"])\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc1ba8-522b-4174-97ad-f2c56bb1c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = word_df.loc[word_df.word != \"\"]\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f0d62-2366-4b4b-a6b0-ae5a9c8a6d77",
   "metadata": {},
   "source": [
    "At first, we can just assign each value to a 1 and then use our usual `.groupby()` to get our desired result. Add a count of 1 for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b3d07-581a-461f-95f6-f7789e2ba04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df[\"count\"] = 1\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a6803-10d3-47a7-ba64-f3829396d590",
   "metadata": {},
   "source": [
    "Awesome! Now just group by the word and sum it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81a84a-6fc9-40cc-aa98-3b1ec456ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df \\\n",
    "    .groupby(\"word\") \\\n",
    "    .sum() \\\n",
    "    .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7b57a-6dfc-4a36-8658-fbcc57de408b",
   "metadata": {},
   "source": [
    "Sort it so we can compare to our previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1161fc9-a82d-4f3b-9e8a-8da98b760c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df \\\n",
    "    .groupby(\"word\") \\\n",
    "    .sum() \\\n",
    "    .sort_values(by = \"count\", ascending = False) \\\n",
    "    .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40603ba7-0cb0-49ce-9cda-e20fcc09ea24",
   "metadata": {},
   "source": [
    "Woot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f8068-3584-4ce7-9c06-1b92aa659312",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "Below is the code from the Spark SQL notes.\n",
    "\n",
    "Start with creation a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cd4cd-28b9-44d7-a84d-a05f8b608cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad3820-391a-4574-a3f1-55a738dbbced",
   "metadata": {},
   "source": [
    "Now let's look at a few ways to create a Spark SQL Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930dc49-2323-43a5-b9df-56d4863fa40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime, date\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520ae75-ac10-48fd-8f06-c362adb289f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da387b2-8937-4c4f-ab46-97bc8c507c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"neuralgia.csv\",\n",
    "                     format=\"csv\", \n",
    "                     sep=\",\", \n",
    "                     inferSchema=\"true\", \n",
    "                     header=\"true\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c45c96-2232-4b5e-b1b2-02c4d19b545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaca10d-4db9-4b53-8dd8-b1ac49515c07",
   "metadata": {},
   "source": [
    "Conveniently, we can go back and forth between Spark SQL style Data Frames and pandas-on-Spark style Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790b4d3-3910-471e-879a-741ca4d7848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.load(\"neuralgia.csv\",\n",
    "                     format=\"csv\", \n",
    "                     sep=\",\", \n",
    "                     inferSchema=\"true\", \n",
    "                     header=\"true\")\n",
    "type(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5c320-7ad7-47f4-9c0a-c2288602bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfps = sdf.to_pandas_on_spark()\n",
    "type(dfps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80812be7-6791-4500-adf5-8bd9d80dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = dfps.to_spark()\n",
    "type(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a4614-e6a9-4113-bde6-8c1d7c05b760",
   "metadata": {},
   "source": [
    "Schema and column names are important to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26441a4f-bdf8-44a3-bc93-f2831380cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"neuralgia.csv\",\n",
    "                     format=\"csv\", \n",
    "                     sep=\",\", \n",
    "                     inferSchema=\"true\", \n",
    "                     header=\"true\")\n",
    "df.printSchema()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d909c-8b21-4dc4-ad06-c8fc8073f6a6",
   "metadata": {},
   "source": [
    "We can return the data using `.take()`, `.show()`. and `.collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e7847-204d-4b55-859f-8286c75a4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e11661-7142-4bf9-818a-549f66d2bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dac2b9-03e1-4e44-97d3-bfe82835069d",
   "metadata": {},
   "source": [
    "Next, we'll look at common transformations. Starting with column operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba3de5-6cf1-43d6-940e-9eae8f7f5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select columns you want\n",
    "df.select(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e45b88-fb9c-4d3f-bd14-6418ea176290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995804a-8656-49ec-ba69-d8d699219fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Age\", \"Pain\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82228857-1d7b-43cc-91a7-47a3cef9d3b8",
   "metadata": {},
   "source": [
    "`.withColumn()` can be used to create new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e135622-68ff-4e6d-837f-57f663e06798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"Current_Age\", df.Age + 2).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21696f40-604f-40e9-9e44-417e22101653",
   "metadata": {},
   "source": [
    "Can also rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf1c73-83b7-456d-998c-8d3cf5cd3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df \\\n",
    "  .withColumnRenamed('Age', 'Former_Age') \\\n",
    "  .withColumn(\"Current_Age\", col(\"Former_Age\") + 2) \\\n",
    "  .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fed8e8-b6fa-4f20-b80b-4b9cec687173",
   "metadata": {},
   "source": [
    "We can using conditional logic with `when()` from `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ecaeb-b5f8-41e2-96bd-30ca7fc0ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.withColumn(\"Age_cat\", \n",
    "               when(df.Age>75, \"75+\")\n",
    "              .when(df.Age>=70, \"70-75\")\n",
    "              .otherwise(\"<70\")) \\\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452dab69-5694-4ed0-864c-2bc494baa22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"Age_cat\", \n",
    "               when(df.Age>75, \"75+\")\n",
    "              .when(df.Age>=70, \"70-75\")\n",
    "              .otherwise(\"<70\")) \\\n",
    "   .withColumn(\"ln_Duration\", log(df.Duration)) \\\n",
    "   .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6432a0-04e7-48c7-b690-bac844bf1d1f",
   "metadata": {},
   "source": [
    "We can also create our own functions with `udf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52874983-9e4c-42e5-9932-2536b78f3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_trt = udf(lambda x: \"P Trt\" if x == \"P\" else \"Other\")\n",
    "df.withColumn('my_trt', code_trt('Treatment')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7236f-75e8-4cb9-8881-63711330ea14",
   "metadata": {},
   "source": [
    "We can do the common operations on rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b304757-0343-4d3e-a527-97505a204593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.Duration).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794738b1-f27c-4127-9a89-11f063d42ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.Duration, ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a2d4d-a568-4c8d-bcf2-d0970e72547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.Age < 65).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac8769-fbf5-4be6-85ca-dd9727876501",
   "metadata": {},
   "source": [
    "We can do basic summaries including grouped summaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188de337-90be-4eca-88d4-cc73d0ed0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Age\", \"Pain\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1e268-df08-44f3-be8c-2a872f2205b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .select([\"Duration\", \"Age\", \"Treatment\"]) \\\n",
    "    .agg(sum(\"Duration\"), avg(\"Age\"), count(\"Treatment\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ed745-6c6c-440c-9473-1c94e5722070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\"Duration\", \"Age\", \"Treatment\"]) \\\n",
    "    .groupBy(\"Treatment\") \\\n",
    "    .sum() \\\n",
    "    .withColumnRenamed(\"sum(Duration)\", \"sum_Duration\") \\\n",
    "    .withColumnRenamed(\"sum(Age)\", \"sum_Age\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc8e3f-a8e5-4fa3-852b-916ca58af67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView(\"df\")\n",
    "spark.sql(\"SELECT sex, age FROM df LIMIT 4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69e4c9-89ac-4263-94a1-dafc3a7ceea0",
   "metadata": {},
   "source": [
    "### MapReduce Example Done via Spark SQL \n",
    "\n",
    "Let's redo it with Spark SQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7c9b2-cc3a-4a4f-a8e6-7e7761cc348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in each chapter to a list element\n",
    "my_chap = []\n",
    "for i in range(1, 54):\n",
    "    with open('dickens/chap' + str(i) + '.txt', 'r') as f:\n",
    "        my_chap.append(f.read())\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "sql_text = spark.createDataFrame(my_chap, StringType())\n",
    "sql_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86979731-2c70-48cf-87da-1466256186c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_text.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc8165-a024-4258-b30c-f72a8247a5c4",
   "metadata": {},
   "source": [
    "Ok, first we need to split the words out within each *row*. When we read in all the SQL functions there was a `split()` function that will work for us!\n",
    "\n",
    "Note the way we use the function without `.withColumn()` by using `.select()`. This is a common way to use these functions without adding to the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c2d1f-5fc7-4c14-af11-c9e5683520a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_text.select(split(sql_text.value, \" \").alias(\"words\")).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc741ad-5d82-4907-9a36-4f262e5bd9c4",
   "metadata": {},
   "source": [
    "Ok, now we have a data frame with one column where each entry is a list of the words! This is closer. \n",
    "\n",
    "What we need to do is now **explode** out these lists. We read in a function called **explode** that will split these values up and create new rows for each entry!\n",
    "\n",
    "Notice how we call the function inside select again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046251b-54e3-497d-8dd3-3879c4a68608",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_text.select(explode(split(sql_text.value, \" \")).alias(\"word\")).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58d4b2-d967-48a9-b972-412a1c714cc2",
   "metadata": {},
   "source": [
    "Woo, almost there. Now we can filter out the blank spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a433e04-52e8-40f1-9df6-12d40f8050c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words = sql_text.select(explode(split(sql_text.value, \" \")).alias(\"word\"))\n",
    "my_words \\\n",
    "    .filter(my_words.word != \"\") \\\n",
    "    .show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe01e5-2193-4b6b-8f69-843642a62169",
   "metadata": {},
   "source": [
    "Finally we group and count!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dbcc2-118b-4490-92e6-0a40f32d7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words \\\n",
    "    .filter(my_words.word != \"\") \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41b7cd-a2c2-4965-b47b-de1f3953b181",
   "metadata": {},
   "source": [
    "Arrange it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d55a43-08da-44ba-ab5a-8e6d079fcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = my_words \\\n",
    "                .filter(my_words.word != \"\") \\\n",
    "                .groupBy(\"word\") \\\n",
    "                .count()\n",
    "counts.sort(counts[\"count\"], ascending = False).show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
