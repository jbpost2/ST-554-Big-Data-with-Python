{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203c08a2-aadd-4215-8cb7-96f3645f85bb",
   "metadata": {},
   "source": [
    "# Notebook Corresponding to `pyspark` Notes\n",
    "\n",
    "This notebook has the code and examples from the three sets of notes:\n",
    "- `pyspark`: RDDs\n",
    "- `pyspark`: pandas-on-Spark\n",
    "- `pyspark`: Spark SQL\n",
    "\n",
    "Each section should be able to be run without running the cells from the other sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e5e3e-1761-4c8a-8076-13c4dc23ff83",
   "metadata": {},
   "source": [
    "## RDDs\n",
    "\n",
    "First, the code from the notes is given below. Make sure that the kernel chosen to run these notes is the `pyspark` kernel. See the top right of the notebook!\n",
    "\n",
    "Note: We can still run python code and load in libraries as normal.\n",
    "\n",
    "Now, we start by creating our spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4cb43-2095-4aed-bf0d-031c45386f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640b518-82cb-4bb2-9028-58012d9d1c7e",
   "metadata": {},
   "source": [
    "Let's populate a list with tuples and explicitly create an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9050c48-fa70-4d5c-89a4-a4e8a4d49f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate a list with tuples\n",
    "quick_cat = lambda x: \"a\" if x < 20 else \"b\"\n",
    "my_data = [(quick_cat(x), x) for x in range(1,51)]\n",
    "my_data[:3]\n",
    "#create the RDD\n",
    "my_rdd = spark.sparkContext.parallelize(my_data)\n",
    "my_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149b23a-dec4-4d81-bb70-0f2f67a1b6da",
   "metadata": {},
   "source": [
    "This is an object stored (likely) over multiple partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d58fc-7d7a-41ae-bcae-cf3ad9ee4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f810eb13-04d1-458b-b53b-54a8a6b14ba8",
   "metadata": {},
   "source": [
    "We see that `my_rdd` doesn't actually print out the data when we look at the object. This is because there may be a ton of data and it doesn't want to show it to you by default. Instead we can perform an action like the `.take()` to actually have some data returned to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d264e-1bf0-4b4d-98f6-d19ccefd3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c218ba-3a06-4a5e-8ad6-59265dfb22de",
   "metadata": {},
   "source": [
    "When we have tuple type object passed in as the data, the first value represents the `key` and the second the associated `values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2dd10-cc29-4f5b-bfec-d33bf45f6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.keys().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f261db4-7596-485b-a66e-5f01de8d2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.values().take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54943f-d411-4e24-9050-9c2c60b584f9",
   "metadata": {},
   "source": [
    "This allows us to do operations by key if we'd like! Note that `.count()` and `.countByKey()` are actions and so they return the value locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa254c12-77c2-465e-a892-dbf98a975c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407508a-1ee0-4fa2-92eb-ed62a6811fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bb5e2-4ad7-4708-a68a-8855c6080b97",
   "metadata": {},
   "source": [
    "If instead we wanted to use the result of this counting operation as a new RDD, we could instead use something like the `mapValues()` method. This returns an RDD rather than a value and so we need to use `.collect()` to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce765d-8b2b-40da-a78c-ac6ed9b2c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(len) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692320f-3775-4727-a2c3-15322d8bdb44",
   "metadata": {},
   "source": [
    "With this, we could do some other transformation on the resulting object (say using `.map()`, which can apply a function to each element of our RDD). For instance, creating a log transformed value as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620c6c5-500f-402f-835b-718cd2a363ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "my_rdd \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(len) \\\n",
    "    .map(lambda x: (x[0], x[1], log(x[1]))) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f5d99-ae89-40c4-b719-c7851d22c40a",
   "metadata": {},
   "source": [
    "RDD functions are hard to use though! We might want to find the total sum of the values for each key. We can use `.groupByKey()` and `mapValues()` for this but the documentation says it is better to use `aggregateByKey()`. But this function requires some confusing arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d6567-eda5-47f8-97bd-c47391e7d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(sum) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a482d9-396b-4d4c-8691-f045d7bcfdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd \\\n",
    "  .aggregateByKey(0, #initial value for each partition\n",
    "                  lambda within_1, within_2: within_1 + within_2, #how to combine values on the same partition, next function is how to combine across partitions\n",
    "                  lambda across_1, across_2: across_1 + across_2) \\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06344b7-4e5a-4c60-b8bd-4179946cdbe0",
   "metadata": {},
   "source": [
    "We can use `.map()` instead if we wanted to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea137c95-8c1c-478b-be5d-524e1ccf66c6",
   "metadata": {},
   "source": [
    "### MapReduce Example Done Explicity Using RDDs\n",
    "\n",
    "Recall that in the Hadoop section, we did a MapReduce algorithm to count the number of words in Oliver Twist.  We can redo that example using Spark!  It will actually be parallelized and all that automatically across our machine too!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318405cf-b290-4a78-83ad-c611e641feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#create a spark session object (simplified, defaults to local)\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1b761-8f60-4dd3-98f1-2f69d074367a",
   "metadata": {},
   "source": [
    "Now let's read in our 53 chapters as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6acba-b51a-4866-abdb-fe89b8673588",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap = []\n",
    "for i in range(1, 6):\n",
    "    with open('dickens/chap' + str(i) + '.txt', 'r') as f:\n",
    "        my_chap.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e147c7-6f99-46fd-89b9-2d968b16d133",
   "metadata": {},
   "source": [
    "We want spark to handle this using RDDs explicitly. We can do that is via the `sparkContext.parallelize()` method.  This just tells spark to take our list and distribute it/prepare it for parallel computations.\n",
    "\n",
    "Let's create some RDDs! We don't care about the chapters themselves, we just want the final word counts. This means we can start with an RDD without keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9214a-d13e-417d-9a6e-7bcc5a186fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd = spark.sparkContext.parallelize(my_chap)\n",
    "type(my_chap_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143f6d4-cedf-4bcf-9435-8dbc51e7be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e09e31-e941-4ff4-9358-d7e9ea201a9e",
   "metadata": {},
   "source": [
    "Great, now we want to take these different values (each chapter is a value) and split those strings by spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13e9e3-27ef-452d-bf24-f2f7c4575e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2decf08-9890-4f84-9393-bc67ca3b5232",
   "metadata": {},
   "source": [
    "Now we have an RDD whose elements are each word (again no keys). First let's filter to remove any empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dec7d2-a7ac-48c0-b972-17a52614254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c320e16f-eb5a-4470-b37b-89cec026f7c8",
   "metadata": {},
   "source": [
    "Let's do a transform on this where we make each word a key and assign it a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382519fb-4ffd-439c-aec0-fea968f25532",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09146e-6b56-4607-b2b9-2d99b0dd64bf",
   "metadata": {},
   "source": [
    "Nice! Now we just need to reduce this by key and we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70be36-fdcf-4f64-b8ac-a1ab0da2d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5b577-8863-427f-903a-0612104282c5",
   "metadata": {},
   "source": [
    "Let's sort this by the keys. We can sort descending by simply sorting on the negative of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3cfe85-78c6-49ec-afb0-0dcbeb2d27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: -x[1]) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51aa436-cdbe-4d9c-912d-093449597b4a",
   "metadata": {},
   "source": [
    "Let's collect all of the data and turn it into a regular pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4a2ba-d62b-4f8a-a31e-ab4e049db4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = my_chap_rdd \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .filter(lambda x: x != \"\") \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: -x[1]) \\\n",
    "    .collect()\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results, columns = [\"word\", \"count\"])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6e482-98bc-4881-87cb-671cc21734ba",
   "metadata": {},
   "source": [
    "## pandas-on-Spark\n",
    "\n",
    "Below is the code from the notes on pandas-on-Spark.\n",
    "\n",
    "First let's import our modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36f306-0a70-4f04-a677-427e0edeefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51847416-7026-4751-932d-b178a0e85abc",
   "metadata": {},
   "source": [
    "Create a pandas-on-Spark series via `ps.Series()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbbe0a-ca73-4dae-9959-bf8b4f72aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.Series([1, 3, 5, np.nan, 6, 8]) #ignore the warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdca4f2-bd47-4110-ac3b-547f5e83baf0",
   "metadata": {},
   "source": [
    "Create a pandas-on-Spark DataFrame via `ps.DataFrame()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedeef8a-47da-434a-abfc-1d520b863925",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140622b6-5a2c-4e47-8bc4-17f0286a1e24",
   "metadata": {},
   "source": [
    "Convert from a pandas DataFrame to a pandas-on-spark easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc188b2-a151-49ce-8c6a-a588182fcde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/red-wine.csv\", delimiter = \";\")\n",
    "psdf = ps.from_pandas(pdf)\n",
    "psdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53941f79-d7e0-490d-9549-6c73c0d39ace",
   "metadata": {},
   "source": [
    "Can subset the data using things we know like the `.loc[]` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279000c-02cd-4efa-8193-9fc315d720bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf.loc[psdf.quality > 5, [\"alcohol\", \"quality\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a14665-2640-4260-a275-b55c75735486",
   "metadata": {},
   "source": [
    "Can also read data directly into a pandas-on-spark data frame using the `ps.read_csv()` function (can't read from a URL though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433835c0-2a31-44f5-adf4-56bc202017db",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ps = ps.read_csv(\"data/titanic.csv\") #data uploaded to jhub in data folder\n",
    "titanic_ps[\"survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a5fc6-2976-409a-a061-6295299abfee",
   "metadata": {},
   "source": [
    "Can now do our usual summarizations using the `.groupby()` method along with a summarization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20862a3d-7601-4ef7-b89c-9bbe58e5b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ps.groupby(\"survived\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b997d96-884d-46ff-b8f1-3160d8e09165",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ps.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce400c9-84ab-4751-82b1-9d48c24ea4b3",
   "metadata": {},
   "source": [
    "We can also use the `.transform()` and `.apply()` methods (also used in regular pandas) to perform other common operations.\n",
    "\n",
    "First we can transform the values in our columns (say center and scale them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e564ab-5fd4-4a87-b7ca-5ab7b4315840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(pser) -> ps.Series[np.float64]:\n",
    "     return (pser + pser.mean())/pser.std()  # should always return the same length as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99fde9c-8979-4a1a-84dc-8b4cb1a7388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_res = titanic_ps[[\"age\", \"fare\"]] \\\n",
    "    .rename(columns = {\"age\": \"o_age\", \"fare\": \"o_fare\"}) \\\n",
    "    .join(titanic_ps[[\"age\", \"fare\"]]\n",
    "              .transform(standardize))\n",
    "std_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46271c3-3ba4-48d9-a059-2d52236295c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db65dda-c6ab-4a3d-8579-b3ba72890f84",
   "metadata": {},
   "source": [
    "Can use `.apply()` to possible return something shorter than the original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a0bf6-6a15-46be-8f0b-703efbeaf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_positives(pser) -> ps.Series[np.float64]:\n",
    "     return (pser[pser>30] + pser[pser>30].mean())/pser[pser>30].std()\n",
    "# can return something short than input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e68a9b-84fa-4c28-a388-4e6573a21a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_pos = titanic_ps[[\"age\"]].apply(standardize_positives)\n",
    "std_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d1325-9321-4575-b3b4-1c46c3ec00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c7090-c1a8-4ae9-8ac2-31b6da2cdfcb",
   "metadata": {},
   "source": [
    "### MapReduce Example Done via pandas-on-Spark\n",
    "\n",
    "Let's repeat our map reduce example but, you know, do it more easily :)\n",
    "\n",
    "Recall the `my_chap` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f00be5-f432-420c-8814-55a05d1c3393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pySpark/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chapter i  treats of the place where oliver twist was born and of the circumstances attending his birth  among other public buildings in a certain town which for many reasons it will be prudent to refrain from mentioning and to which i will assign no fictitious name there is one anciently common to most towns great or small to wit a workhouse and in this workhouse was born on a day and date which i need not trouble myself to repeat inasmuch as it can be of no possible consequence to the reader in this stage of the business at all events the item of mortality whose name is prefixed to the head of this chapter  for a long time after it was ushered into this world of sorrow and trouble by the parish surgeon it remained a matter of considerable doubt whether the child would survive to bear any name at all in which case it is somewhat more than probable that these memoirs would never have appeared or if they had that being comprised within a couple of pages they would have possessed the inestimable merit of being the most concise and faithful specimen of biography extant in the literature of any age or country  although i am not disposed to maintain that the being born in a workhouse is in itself the most fortunate and enviable circumstance that can possibly befall a human being i do mean to say that in this particular instance it was the best thing for oliver twist that could by possibility have occurred  the fact is that there was considerable difficulty in inducing oliver to take upon himself the office of respirationa troublesome practice but one which custom has rendered necessary to our easy existence and for some time he lay gasping on a little flock mattress rather unequally poised between this world and the next the balance being decidedly in favour of the latter  now if during this brief period oliver had been surrounded by careful grandmothers anxious aunts experienced nurses and doctors of profound wisdom he would most inevitably and indubitably have been killed in no time  there being nobody by however but a pauper old woman who was rendered rather misty by an unwonted allowance of beer and a parish surgeon who did such matters by contract oliver and nature fought out the point between them  the result was that after a few struggles oliver breathed sneezed and proceeded to advertise to the inmates of the workhouse the fact of a new burden having been imposed  upon the parish by setting up as loud a cry as could reasonably have been expected from a male infant who had not been possessed of that very useful appendage a voice for a much longer space of time than three minutes and a quarter  as oliver gave this first proof of the free and proper action of his lungs the patchwork coverlet which was carelessly flung over the iron bedstead rustled the pale face of a young woman was raised feebly from the pillow and a faint voice imperfectly articulated the words let me see the child and die  the surgeon had been sitting with his face turned towards the fire giving the palms of his hands a warm and a rub alternately  as the young woman spoke he rose and advancing to the beds head said with more kindness than might have been expected of him  oh you must not talk about dying yet  lor bless her dear heart no interposed the nurse hastily depositing in her pocket a green glass bottle the contents of which she had been tasting in a corner with evident satisfaction  lor bless her dear heart when she has lived as long as i have sir and had thirteen children of her own and all on em dead except two and them in the wurkus with me shell know better than to take on in that way bless her dear heart  think what it is to be a mother theres a dear young lamb do  apparently this consolatory perspective of a mothers prospects failed in producing its due effect  the patient shook her head and stretched out her hand towards the child  the surgeon deposited it in her arms  she imprinted her cold white lips passionately on its forehead passed her hands over her face gazed wildly round shuddered fell backand died they chafed her breast hands and temples but the blood had stopped forever  they talked of hope and comfort they had been strangers too long  its all over mrs thingummy said the surgeon at last  ah poor dear so it is said the nurse picking up the cork of the green bottle which had fallen out on the pillow as she stooped to take up the child  poor dear  you neednt mind sending up to me if the child cries nurse said the surgeon putting on his gloves with great deliberation its very likely it will be troublesome give it a little gruel if it is  he put on his hat and pausing by the bedside on his way to the door added she was a goodlooking girl too where did she come from  she was brought here last night replied the old woman by the overseers order  she was found lying in the street  she had walked some distance for her shoes were worn to pieces but where she came from or where she was going to nobody knows  the surgeon leaned over the body and raised the left hand  the old story he said shaking his head no weddingring i see ah goodnight  the medical gentleman walked away to dinner and the nurse having once more applied herself to the green bottle sat down on a low chair before the fire and proceeded to dress the infant  what an excellent example of the power of dress young oliver twist was  wrapped in the blanket which had hitherto formed his only covering he might have been the child of a nobleman or a beggar it would have been hard for the haughtiest stranger to have assigned him his proper station in society  but now that he was enveloped in the old calico robes which had grown yellow in the same service he was badged and ticketed and fell into his place at oncea parish childthe orphan of a workhousethe humble halfstarved drudgeto be cuffed and buffeted through the worlddespised by all and pitied by none  oliver cried lustily if he could have known that he was an orphan left to the tender mercies of churchwardens and overseers perhaps he would have cried the louder    '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "#read in each chapter to a list element\n",
    "my_chap = []\n",
    "for i in range(1, 6):\n",
    "    with open('dickens/chap' + str(i) + '.txt', 'r') as f:\n",
    "        my_chap.append(f.read())\n",
    "        \n",
    "my_chap[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b633507-518d-47fb-822d-3dd2d961988f",
   "metadata": {},
   "source": [
    "Let's put this into a pandas-on-Spark series and manipulate from there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d479ebc3-9369-458e-b00a-469b7329437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/03 07:47:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    chapter i  treats of the place where oliver tw...\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine the list elements into one large string\n",
    "from functools import reduce\n",
    "big_string = reduce(lambda x, y: x + y, my_chap)\n",
    "\n",
    "#create a series with the big string\n",
    "chap_pss = ps.Series(big_string)\n",
    "chap_pss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c22055-fe0b-4c6d-97a0-d3e66bd54834",
   "metadata": {},
   "source": [
    "Now use the `.str.split()` method on a pandas-on-Spark series to create a series with a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7178c0-69f9-4eed-af0c-b9a692082270",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_pss.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c8964-8ee4-4a41-a6ff-cef974cb4441",
   "metadata": {},
   "source": [
    "What we want is to have each word be in a column with the count associated as another column. Let's convert the list stored in the series to a data frame. Then remove the empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7503e81-53b1-450c-b11c-7bfa6c610cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = ps.DataFrame(chap_pss.str.split(\" \")[0], columns = [\"word\"])\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc1ba8-522b-4174-97ad-f2c56bb1c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = word_df.loc[word_df.word != \"\"]\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f0d62-2366-4b4b-a6b0-ae5a9c8a6d77",
   "metadata": {},
   "source": [
    "At first, we can just assign each value to a 1 and then use our usual `.groupby()` to get our desired result. Add a count of 1 for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b3d07-581a-461f-95f6-f7789e2ba04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df[\"count\"] = 1\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a6803-10d3-47a7-ba64-f3829396d590",
   "metadata": {},
   "source": [
    "Awesome! Now just group by the word and sum it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81a84a-6fc9-40cc-aa98-3b1ec456ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df \\\n",
    "    .groupby(\"word\") \\\n",
    "    .sum() \\\n",
    "    .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7b57a-6dfc-4a36-8658-fbcc57de408b",
   "metadata": {},
   "source": [
    "Sort it so we can compare to our previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1161fc9-a82d-4f3b-9e8a-8da98b760c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df \\\n",
    "    .groupby(\"word\") \\\n",
    "    .sum() \\\n",
    "    .sort_values(by = \"count\", ascending = False) \\\n",
    "    .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40603ba7-0cb0-49ce-9cda-e20fcc09ea24",
   "metadata": {},
   "source": [
    "Woot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f8068-3584-4ce7-9c06-1b92aa659312",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "Below is the code from the Spark SQL notes.\n",
    "\n",
    "Start with creation a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276cd4cd-28b9-44d7-a84d-a05f8b608cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/03 08:19:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad3820-391a-4574-a3f1-55a738dbbced",
   "metadata": {},
   "source": [
    "Now let's look at a few ways to create a Spark SQL Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930dc49-2323-43a5-b9df-56d4863fa40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime, date\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520ae75-ac10-48fd-8f06-c362adb289f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da387b2-8937-4c4f-ab46-97bc8c507c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"data/neuralgia.csv\",\n",
    "                     format=\"csv\", \n",
    "                     sep=\",\", \n",
    "                     inferSchema=\"true\", \n",
    "                     header=\"true\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c45c96-2232-4b5e-b1b2-02c4d19b545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaca10d-4db9-4b53-8dd8-b1ac49515c07",
   "metadata": {},
   "source": [
    "Conveniently, we can go back and forth between Spark SQL style Data Frames and pandas-on-Spark style Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790b4d3-3910-471e-879a-741ca4d7848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.load(\"data/neuralgia.csv\",\n",
    "                     format=\"csv\", \n",
    "                     sep=\",\", \n",
    "                     inferSchema=\"true\", \n",
    "                     header=\"true\")\n",
    "type(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5c320-7ad7-47f4-9c0a-c2288602bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfps = sdf.pandas_api()\n",
    "type(dfps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80812be7-6791-4500-adf5-8bd9d80dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = dfps.to_spark()\n",
    "type(sdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a4614-e6a9-4113-bde6-8c1d7c05b760",
   "metadata": {},
   "source": [
    "Schema and column names are important to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26441a4f-bdf8-44a3-bc93-f2831380cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"data/neuralgia.csv\",\n",
    "                     format=\"csv\", \n",
    "                     sep=\",\", \n",
    "                     inferSchema=\"true\", \n",
    "                     header=\"true\")\n",
    "df.printSchema()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d909c-8b21-4dc4-ad06-c8fc8073f6a6",
   "metadata": {},
   "source": [
    "We can return the data using `.take()`, `.show()`. and `.collect()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e7847-204d-4b55-859f-8286c75a4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e11661-7142-4bf9-818a-549f66d2bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dac2b9-03e1-4e44-97d3-bfe82835069d",
   "metadata": {},
   "source": [
    "Next, we'll look at common transformations. Starting with column operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba3de5-6cf1-43d6-940e-9eae8f7f5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select columns you want\n",
    "df.select(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e45b88-fb9c-4d3f-bd14-6418ea176290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995804a-8656-49ec-ba69-d8d699219fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Age\", \"Pain\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82228857-1d7b-43cc-91a7-47a3cef9d3b8",
   "metadata": {},
   "source": [
    "`.withColumn()` can be used to create new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e135622-68ff-4e6d-837f-57f663e06798",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"Current_Age\", df.Age + 2).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21696f40-604f-40e9-9e44-417e22101653",
   "metadata": {},
   "source": [
    "Can also rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf1c73-83b7-456d-998c-8d3cf5cd3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df \\\n",
    "  .withColumnRenamed('Age', 'Former_Age') \\\n",
    "  .withColumn(\"Current_Age\", col(\"Former_Age\") + 2) \\\n",
    "  .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fed8e8-b6fa-4f20-b80b-4b9cec687173",
   "metadata": {},
   "source": [
    "We can using conditional logic with `when()` from `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ecaeb-b5f8-41e2-96bd-30ca7fc0ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.withColumn(\"Age_cat\", \n",
    "               when(df.Age>75, \"75+\")\n",
    "              .when(df.Age>=70, \"70-75\")\n",
    "              .otherwise(\"<70\")) \\\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452dab69-5694-4ed0-864c-2bc494baa22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"Age_cat\", \n",
    "               when(df.Age>75, \"75+\")\n",
    "              .when(df.Age>=70, \"70-75\")\n",
    "              .otherwise(\"<70\")) \\\n",
    "   .withColumn(\"ln_Duration\", log(df.Duration)) \\\n",
    "   .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6432a0-04e7-48c7-b690-bac844bf1d1f",
   "metadata": {},
   "source": [
    "We can also create our own functions with `udf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52874983-9e4c-42e5-9932-2536b78f3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_trt = udf(lambda x: \"P Trt\" if x == \"P\" else \"Other\")\n",
    "df.withColumn('my_trt', code_trt('Treatment')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7236f-75e8-4cb9-8881-63711330ea14",
   "metadata": {},
   "source": [
    "We can do the common operations on rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b304757-0343-4d3e-a527-97505a204593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.Duration).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794738b1-f27c-4127-9a89-11f063d42ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df.Duration, ascending = False).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a2d4d-a568-4c8d-bcf2-d0970e72547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.Age < 65).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac8769-fbf5-4be6-85ca-dd9727876501",
   "metadata": {},
   "source": [
    "We can do basic summaries including grouped summaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188de337-90be-4eca-88d4-cc73d0ed0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Age\", \"Pain\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1e268-df08-44f3-be8c-2a872f2205b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .select([\"Duration\", \"Age\", \"Treatment\"]) \\\n",
    "    .agg(sum(\"Duration\"), avg(\"Age\"), count(\"Treatment\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ed745-6c6c-440c-9473-1c94e5722070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\"Duration\", \"Age\", \"Treatment\"]) \\\n",
    "    .groupBy(\"Treatment\") \\\n",
    "    .sum() \\\n",
    "    .withColumnRenamed(\"sum(Duration)\", \"sum_Duration\") \\\n",
    "    .withColumnRenamed(\"sum(Age)\", \"sum_Age\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc8e3f-a8e5-4fa3-852b-916ca58af67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView(\"df\")\n",
    "spark.sql(\"SELECT sex, age FROM df LIMIT 4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69e4c9-89ac-4263-94a1-dafc3a7ceea0",
   "metadata": {},
   "source": [
    "### MapReduce Example Done via Spark SQL \n",
    "\n",
    "Let's redo it with Spark SQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf7c9b2-cc3a-4a4f-a8e6-7e7761cc348f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in each chapter to a list element\n",
    "my_chap = []\n",
    "for i in range(1, 6):\n",
    "    with open('dickens/chap' + str(i) + '.txt', 'r') as f:\n",
    "        my_chap.append(f.read())\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "sql_text = spark.createDataFrame(my_chap, StringType())\n",
    "sql_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86979731-2c70-48cf-87da-1466256186c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='chapter i  treats of the place where oliver twist was born and of the circumstances attending his birth  among other public buildings in a certain town which for many reasons it will be prudent to refrain from mentioning and to which i will assign no fictitious name there is one anciently common to most towns great or small to wit a workhouse and in this workhouse was born on a day and date which i need not trouble myself to repeat inasmuch as it can be of no possible consequence to the reader in this stage of the business at all events the item of mortality whose name is prefixed to the head of this chapter  for a long time after it was ushered into this world of sorrow and trouble by the parish surgeon it remained a matter of considerable doubt whether the child would survive to bear any name at all in which case it is somewhat more than probable that these memoirs would never have appeared or if they had that being comprised within a couple of pages they would have possessed the inestimable merit of being the most concise and faithful specimen of biography extant in the literature of any age or country  although i am not disposed to maintain that the being born in a workhouse is in itself the most fortunate and enviable circumstance that can possibly befall a human being i do mean to say that in this particular instance it was the best thing for oliver twist that could by possibility have occurred  the fact is that there was considerable difficulty in inducing oliver to take upon himself the office of respirationa troublesome practice but one which custom has rendered necessary to our easy existence and for some time he lay gasping on a little flock mattress rather unequally poised between this world and the next the balance being decidedly in favour of the latter  now if during this brief period oliver had been surrounded by careful grandmothers anxious aunts experienced nurses and doctors of profound wisdom he would most inevitably and indubitably have been killed in no time  there being nobody by however but a pauper old woman who was rendered rather misty by an unwonted allowance of beer and a parish surgeon who did such matters by contract oliver and nature fought out the point between them  the result was that after a few struggles oliver breathed sneezed and proceeded to advertise to the inmates of the workhouse the fact of a new burden having been imposed  upon the parish by setting up as loud a cry as could reasonably have been expected from a male infant who had not been possessed of that very useful appendage a voice for a much longer space of time than three minutes and a quarter  as oliver gave this first proof of the free and proper action of his lungs the patchwork coverlet which was carelessly flung over the iron bedstead rustled the pale face of a young woman was raised feebly from the pillow and a faint voice imperfectly articulated the words let me see the child and die  the surgeon had been sitting with his face turned towards the fire giving the palms of his hands a warm and a rub alternately  as the young woman spoke he rose and advancing to the beds head said with more kindness than might have been expected of him  oh you must not talk about dying yet  lor bless her dear heart no interposed the nurse hastily depositing in her pocket a green glass bottle the contents of which she had been tasting in a corner with evident satisfaction  lor bless her dear heart when she has lived as long as i have sir and had thirteen children of her own and all on em dead except two and them in the wurkus with me shell know better than to take on in that way bless her dear heart  think what it is to be a mother theres a dear young lamb do  apparently this consolatory perspective of a mothers prospects failed in producing its due effect  the patient shook her head and stretched out her hand towards the child  the surgeon deposited it in her arms  she imprinted her cold white lips passionately on its forehead passed her hands over her face gazed wildly round shuddered fell backand died they chafed her breast hands and temples but the blood had stopped forever  they talked of hope and comfort they had been strangers too long  its all over mrs thingummy said the surgeon at last  ah poor dear so it is said the nurse picking up the cork of the green bottle which had fallen out on the pillow as she stooped to take up the child  poor dear  you neednt mind sending up to me if the child cries nurse said the surgeon putting on his gloves with great deliberation its very likely it will be troublesome give it a little gruel if it is  he put on his hat and pausing by the bedside on his way to the door added she was a goodlooking girl too where did she come from  she was brought here last night replied the old woman by the overseers order  she was found lying in the street  she had walked some distance for her shoes were worn to pieces but where she came from or where she was going to nobody knows  the surgeon leaned over the body and raised the left hand  the old story he said shaking his head no weddingring i see ah goodnight  the medical gentleman walked away to dinner and the nurse having once more applied herself to the green bottle sat down on a low chair before the fire and proceeded to dress the infant  what an excellent example of the power of dress young oliver twist was  wrapped in the blanket which had hitherto formed his only covering he might have been the child of a nobleman or a beggar it would have been hard for the haughtiest stranger to have assigned him his proper station in society  but now that he was enveloped in the old calico robes which had grown yellow in the same service he was badged and ticketed and fell into his place at oncea parish childthe orphan of a workhousethe humble halfstarved drudgeto be cuffed and buffeted through the worlddespised by all and pitied by none  oliver cried lustily if he could have known that he was an orphan left to the tender mercies of churchwardens and overseers perhaps he would have cried the louder    ')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_text.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc8165-a024-4258-b30c-f72a8247a5c4",
   "metadata": {},
   "source": [
    "Ok, first we need to split the words out within each *row*. When we read in all the SQL functions there was a `split()` function that will work for us!\n",
    "\n",
    "Note the way we use the function without `.withColumn()` by using `.select()`. This is a common way to use these functions without adding to the original data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efe12316-86c8-439b-9f2c-1ed2076f9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae52ed21-d238-4a12-bc88-fb792a16d752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'split(value,  , -1) AS words'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(sql_text.value, \" \").alias(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213c2d1f-5fc7-4c14-af11-c9e5683520a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|[chapter, i, , tr...|\n",
      "|[chapter, ii, , t...|\n",
      "|[chapter, iii, , ...|\n",
      "|[chapter, iv, , o...|\n",
      "+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_text.select(split(sql_text.value, \" \").alias(\"words\")).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc741ad-5d82-4907-9a36-4f262e5bd9c4",
   "metadata": {},
   "source": [
    "Ok, now we have a data frame with one column where each entry is a list of the words! This is closer. \n",
    "\n",
    "What we need to do is now **explode** out these lists. We read in a function called **explode** that will split these values up and create new rows for each entry!\n",
    "\n",
    "Notice how we call the function inside select again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dc58e0d-3865-48db-b295-5cfc371ee3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'explode(split(value,  , -1)) AS word'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode(split(sql_text.value, \" \")).alias(\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4046251b-54e3-497d-8dd3-3879c4a68608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|chapter|\n",
      "|      i|\n",
      "|       |\n",
      "| treats|\n",
      "+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_text.select(explode(split(sql_text.value, \" \")).alias(\"word\")).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58d4b2-d967-48a9-b972-412a1c714cc2",
   "metadata": {},
   "source": [
    "Woo, almost there. Now we can filter out the blank spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08982d0d-ea65-4603-a740-e01b8a5d9a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_words = sql_text.select(explode(split(sql_text.value, \" \")).alias(\"word\"))\n",
    "my_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a433e04-52e8-40f1-9df6-12d40f8050c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|chapter|\n",
      "|      i|\n",
      "| treats|\n",
      "|     of|\n",
      "+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_words \\\n",
    "    .filter(my_words.word != \"\") \\\n",
    "    .show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe01e5-2193-4b6b-8f69-843642a62169",
   "metadata": {},
   "source": [
    "Finally we group and count!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf6dbcc2-118b-4490-92e6-0a40f32d7937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===========================>                          (64 + 64) / 128]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|      some|   31|\n",
      "|       few|    7|\n",
      "|      hope|    5|\n",
      "| overseers|    2|\n",
      "|surrounded|    2|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_words \\\n",
    "    .filter(my_words.word != \"\") \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41b7cd-a2c2-4965-b47b-de1f3953b181",
   "metadata": {},
   "source": [
    "Arrange it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0d55a43-08da-44ba-ab5a-8e6d079fcaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 1005|\n",
      "| and|  439|\n",
      "|   a|  416|\n",
      "|  of|  389|\n",
      "|  to|  357|\n",
      "|  in|  257|\n",
      "| was|  222|\n",
      "| his|  219|\n",
      "|  he|  203|\n",
      "|  mr|  157|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = my_words \\\n",
    "                .filter(my_words.word != \"\") \\\n",
    "                .groupBy(\"word\") \\\n",
    "                .count()\n",
    "counts.sort(counts[\"count\"], ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d29deb-fd5b-481c-8977-727071642fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
