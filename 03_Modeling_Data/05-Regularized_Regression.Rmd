---
title: "Regularized Regression"
author: "Justin Post"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "css/ncsu.css", "css/ncsu-fonts.css", "css/mycss.css"]
    nature:
      beforeInit: ["js/ncsu-scale.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "partials/header.html"
    self_contained: yes
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  fig.align = "center",
  #fig.width = 11,
  #fig.height = 5
  cache = FALSE
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(tidyverse)
library(knitr)
library(reticulate)
#use_python("C:\\ProgramData\\Anaconda3\\python.exe")
#use_python("C:\\Users\\jbpost2\\AppData\\Local\\Programs\\Python\\Python310\\python.exe")
use_python("C:\\python\\python.exe")
options(dplyr.print_min = 5)
options(reticulate.repl.quiet = TRUE)
```


layout: false
class: title-slide-section-red, middle

# Regularized Regression
Justin Post 

---
layout: true

<div class="my-footer"><img src="img/logo.png" style="height: 60px;"/></div> 



---

# Regularization Methods

- Recall the LASSO model (like least squares but a penalty term added)
    + $\alpha$ (>0) is called a tuning parameter
  
$$\min\limits_{\beta's}\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}))^2 + \alpha\sum_{j=1}^{p}|\beta_j|$$
- Sets coefficients to 0 as you 'shrink'!


---

# Tuning Parameter

- When choosing the tuning parameter, we are really considering a **family of models**!

- Let's recall an example we did

```{python, echo = TRUE, eval = TRUE}
import pandas as pd
import numpy as np
from sklearn import linear_model
from math import sqrt
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression, LassoCV, Lasso
fat_data = pd.read_csv("https://www4.stat.ncsu.edu/~online/datasets/fat.csv")
fat_data.columns
```


---

# Cleaning and Splitting the Data

- Drop some variables we don't want
- Remove any rows with missing values

```{python}
mod_fat_data = fat_data.drop(["Unnamed: 0", "siri", "density"], axis = 1).dropna()

X_train, X_test, y_train, y_test = train_test_split(
  mod_fat_data.drop("brozek", axis = 1),
  mod_fat_data["brozek"], 
  test_size=0.20, 
  random_state=41)
```

---

# Scale Data with Regularization

- Usually want to scale the data if using regularization methods
    + Subtract mean, divide by sd
    + Use the training means and sds for test set too!
    
```{python}
means = X_train.apply(np.mean, axis = 0)
stds = X_train.apply(np.std, axis = 0)
X_train = X_train.apply(lambda x: (x-np.mean(x))/np.std(x), axis = 0)
X_train.head()
```

---

# Scale Data with Regularization

- Usually want to scale the data if using regularization methods
    + Subtract mean, divide by sd
    + Use the training means and sds for test set too!

```{python}
#quick function to standardize based off of a supplied mean and std
def my_std_fun(x, means, stds):
    return(x-means)/stds
#loop through the columns and use the function on each
for x in X_test.columns:
    X_test[x] = my_std_fun(X_test[x], means[x], stds[x])
X_test.head()
```


---

# Fit a LASSO Model Using CV

```{python}
lasso_mod = LassoCV(cv=5, random_state=0) \
                  .fit(X_train, y_train)
print(lasso_mod.alpha_)
print(np.array(list(zip(X_train.columns, lasso_mod.coef_))))
```


---

# LASSO Fits Visual

```{python, echo = FALSE, eval = TRUE, include = FALSE, message = FALSE}
#code from https://www.kirenz.com/post/2019-08-12-python-lasso-regression-auto/
import matplotlib.pyplot as plt

alphas = np.linspace(0.01,2.2,500)
lasso = linear_model.Lasso(max_iter=10000)
coefs = []

for a in alphas:
    lasso.set_params(alpha=a)
    lasso.fit(X_train, y_train)
    coefs.append(lasso.coef_)
```
```{python, echo = FALSE, fig.align = 'center', out.width = "450px", message = FALSE}
ax = plt.gca()
ax.plot(alphas, coefs)
#ax.set_xscale('log')
plt.axis('tight')
plt.xlabel('alpha')
plt.ylabel('Standardized Coefficients')
plt.title('Lasso coefficients as a function of alpha');
plt.axvline(x = lasso_mod.alpha_, color = "Black")
plt.show()
```


---

# Fit 'Best' Model by CV on All Training Data

```{python}
lasso_best = Lasso(lasso_mod.alpha_).fit(X_train,y_train)
```

- Predict on the test set (using the standardized test predictors!)

```{python}
lasso_pred = lasso_best.predict(X_test)
#could compare this to other 'best' models
np.sqrt(mean_squared_error(y_test, lasso_pred))
```

---

# Penalized Regression or Regularized Regression

In linear regression, adding a penalty term to the loss function is called penalized regression or regularized regression.

- $L_1$ penalty shrinks and does variable selection
$$\min\limits_{\beta's}\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}))^2 + \alpha\sum_{j=1}^{p}|\beta_j|$$


---

# Penalized Regression or Regularized Regression

In linear regression, adding a penalty term to the loss function is called penalized regression or regularized regression.

- $L_1$ penalty shrinks and does variable selection
$$\min\limits_{\beta's}\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}))^2 + \alpha\sum_{j=1}^{p}|\beta_j|$$

- $L_2$ penalty shrinks coefficients (works well for multicollinearity)
$$\min\limits_{\beta's}\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}))^2 + \lambda\sum_{j=1}^{p}\beta_j^2$$

---

layout: false

# Penalized Regression or Regularized Regression

In linear regression, adding a penalty term to the loss function is called penalized regression or regularized regression.

- $L_1$ penalty shrinks and does variable selection
$$\min\limits_{\beta's}\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}))^2 + \alpha\sum_{j=1}^{p}|\beta_j|$$

- $L_2$ penalty shrinks coefficients (works well for multicollinearity)
$$\min\limits_{\beta's}\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}))^2 + \lambda\sum_{j=1}^{p}\beta_j^2$$

- $L_1$ and $L_2$ penalties combine the approaches
$$\min\limits_{\beta's}\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}))^2 + \alpha\sum_{j=1}^{p}|\beta_j|+ \lambda\sum_{j=1}^{p}\beta_j^2$$


---

# Penalized Regression or Regularized Regression

- For MLR, these can be done via 

    + `sklearn.linear_model.Lasso` 
    + `sklearn.linear_model.Ridge` 
    + `sklearn.linear_model.ElasticNet`


---

# Penalized Regression or Regularized Regression

- For MLR, these can be done via 

    + `sklearn.linear_model.Lasso` 
    + `sklearn.linear_model.Ridge` 
    + `sklearn.linear_model.ElasticNet`
    
- `sklearn.linear_model.*CV` to easily use CV!
    
- Tuning parameters for Elastic Net:
$$\min\limits_{\beta's}\frac{1}{2n}\sum_{i=1}^{n}(y_i-(\beta_0+\beta_1x_{1i}+...+\beta_px_{pi}))^2$$
$$+\alpha*L1\_ratio\sum_{j=1}^{p}|\beta_j|+ 0.5*\alpha(1-L1\_ratio)\sum_{j=1}^{p}\beta_j^2$$


---

# Elastic Net


```{python, eval = FALSE}
from sklearn.linear_model import ElasticNetCV
regr = ElasticNetCV(cv=5, 
                    random_state=0, 
                    l1_ratio = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.98, 0.99, 1],
                    n_alphas = 50)
regr.fit(X_train, y_train)
```

```{python, include = FALSE}
from sklearn.linear_model import ElasticNetCV
regr = ElasticNetCV(cv=5, 
                    random_state=0, 
                    l1_ratio = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.98, 0.99, 1],
                    n_alphas = 100)
regr.fit(X_train, y_train)
```

```{python}
print(regr.alpha_)
print(regr.l1_ratio_)
```
  

---

# Elastic Net

- Refit on full training data with best tuning parameters

```{python, eval = FALSE}
from sklearn.linear_model import ElasticNet
en = ElasticNet(alpha = regr.alpha_, l1_ratio = regr.l1_ratio_)
en.fit(X_train, y_train)
```
```{python, include = FALSE}
from sklearn.linear_model import ElasticNet
en = ElasticNet(alpha = regr.alpha_, l1_ratio = regr.l1_ratio_)
en.fit(X_train, y_train)
```
```{python}
print(np.array(list(zip(X_train.columns, en.coef_))))
```

---

# Compare on Test Set


```{python}
lasso_pred = lasso_best.predict(X_test)
en_pred = en.predict(X_test)
print([np.sqrt(mean_squared_error(y_test, lasso_pred)),
       np.sqrt(mean_squared_error(y_test, en_pred))])
```


---

# Regularized Logistic Regression

- Same ideas here!

- `sklearn.linear_model.LogisticRegression` can do all three penalized methods mentioned

    + `penalty` = 'l1', 'l2', 'elasticnet', or `none`
    + `default='l2'`! (`C` is regularization parameter = 1 by default)
    + For elastic net, `solver = 'saga'` and specify `l1_ratio`


---

# Regularized Logistic Regression

- Same ideas here!

- `sklearn.linear_model.LogisticRegression` can do all three penalized methods mentioned

    + `penalty` = 'l1', 'l2', 'elasticnet', or `none`
    + `default='l2'`! (`C` is regularization parameter = 1 by default)
    + For elastic net, `solver = 'saga'` and specify `l1_ratio`

- `sklearn.linear_model.LogisticRegressionCV` for CV!


---

# Quick Example

- Make a binary version of response

```{python}
y_train2 = y_train < 25
y_test2 = y_test < 25
```

---

# Quick Example

- Make a binary version of response

```{python}
y_train2 = y_train < 25
y_test2 = y_test < 25
```

- Fit `L2` regularized logistic regression

```{python}
from sklearn.linear_model import LogisticRegressionCV
log_reg_cv = LogisticRegressionCV(cv = 5,
                               solver = "newton-cg",
                               penalty = "l2",
                               Cs = 250,
                               scoring = "neg_log_loss",
                               random_state = 10)
```

```{python eval = FALSE}
log_reg_cv.fit(X_train, y_train2)
```

```{python include = FALSE}
log_reg_cv.fit(X_train, y_train2)
```


---

# Results

- Optimal regularization value (smaller means more regularized)
```{python}
log_reg_cv.C_
```

- Fit optimal model
```{python}
from sklearn.linear_model import LogisticRegression
log_reg_best_cv = LogisticRegression(solver = "newton-cg",
                             penalty = "l2",
                             C = log_reg_cv.C_[0],
                             random_state = 5)
```
```{python eval = FALSE}
log_reg_best_cv.fit(X_train, y_train2)
```
```{python include = FALSE}
log_reg_best_cv.fit(X_train, y_train2)
```

---

# Compare Coefficients

- Compare non-regularize model with regularized:

```{python, eval = FALSE}
log_reg_full = LogisticRegression(solver = "newton-cg", penalty = "none", random_state = 0)
log_reg_full.fit(X_train, y_train2)
```

```{python include = FALSE}
log_reg_full = LogisticRegression(solver = "newton-cg", penalty = "none", random_state = 0)
log_reg_full.fit(X_train, y_train2)
```


---

# Compare Coefficients

- Compare non-regularize model with regularized:

```{python, eval = FALSE}
log_reg_full = LogisticRegression(solver = "newton-cg", penalty = "none", random_state = 0)
log_reg_full.fit(X_train, y_train2)
```

```{python}
for i in range(log_reg_full.coef_.shape[1]):
  print(X_train.columns[i], log_reg_full.coef_[:,i], log_reg_best_cv.coef_[:,i])
```


---

# Compare on Test Data

- Which model generalizes better?

```{python}
cv_proba_preds = log_reg_best_cv.predict_proba(X_test)
full_proba_preds = log_reg_full.predict_proba(X_test)

from sklearn.metrics import log_loss, accuracy_score
log_loss(y_test2, cv_proba_preds)
log_loss(y_test2, full_proba_preds)
log_loss(y_test2, np.array([[0,1] for _ in range(len(y_test2.values))]))
```


---

# Compare on Test Data

- Which model generalizes better?

```{python}
cv_preds = log_reg_best_cv.predict(X_test)
full_preds = log_reg_full.predict(X_test)

from sklearn.metrics import log_loss, accuracy_score
accuracy_score(y_test2, cv_preds)
accuracy_score(y_test2, full_preds)
accuracy_score(y_test2, np.array([1 for _ in range(len(y_test2.values))]))
```


---

# Complicated Process

- Process often pretty involved

    + Split data
    + Create dummy variables, interaction terms, standardize data, etc.
    + Fit a model, often with CV
    + Choose best model 
    + Predict on test set (using appropriate transformations from training set!)

---

# Complicated Process

- Process often pretty involved

    + Split data
    + Create dummy variables, interaction terms, standardize data, etc.
    + Fit a model, often with CV
    + Choose best model 
    + Predict on test set (using appropriate transformations from training set!)

- If we were to fit a LASSO model, a Ridge Regression model, and an Elastic Net model, only the 'fit' part really has to change!

- Future: Put process into a **pipeline** for ease!

---

# Recap

- Regularization can improve prediction and do variable selection at the same time

- Implemented for both MLR type models and logistic regression type models

