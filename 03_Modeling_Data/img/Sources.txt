SparkStreamingExample.png
https://www.youtube.com/watch?v=Mxr408U_gqo&t=2s

#submitting a batch job
https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm



https://www.upsolver.com/blog/streaming-data-architecture-key-components
The top part of this is a pretty good intro and a similar article by them (https://www.upsolver.com/blog/6-most-common-streaming-data-use-cases) gives some examples in more detail.

Streaming data refers to data that is continuously generated, usually in high volumes and at high velocity. A streaming data source would typically consist of a stream of logs that record events as they happen – such as a user clicking on a link in a web page, or a sensor reporting the current temperature.

In all of these cases we have end devices that are continuously generating thousands or millions of records, forming a data stream – unstructured or semi-structured form, most commonly JSON or XML key-value pairs. Here’s an example of how a single streaming event would look – in this case the data we are looking at is a website session:

A single streaming source will generate massive amounts of these events every minute. In its raw form, this data is very difficult to work with as the lack of schema and structure makes it difficult to query with SQL-based analytic tools; instead, data needs to be processed, parsed, and structured before any serious analysis can be done.

Traditional method for handling streaming data
1. The Message Broker / Stream Processor
2. Batch and Real-time ETL Tools
Data streams from one or more message brokers must be aggregated, transformed, and structured before data can be analyzed with SQL-based analytics tools.  This would be done by an ETL tool or platform rthat eceives queries from users, fetches events from message queues, then applies the query to generate a result – in the process often performing additional joins, transformations, or aggregations on the data. The result may be an API call, an action, a visualization, an alert, or in some cases a new data stream.
3. Data Analytics / Serverless Query Engine (a list of common tools is given)




https://aws.amazon.com/streaming-data/ (some examples given here as well)

Streaming data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Streaming data includes a wide variety of data such as log files generated by customers using your mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.

This data needs to be processed sequentially and incrementally on a record-by-record basis or over sliding time windows, and used for a wide variety of analytics including correlations, aggregations, filtering, and sampling. Information derived from such analysis gives companies visibility into many aspects of their business and customer activity such as –service usage (for metering/billing), server activity, website clicks, and geo-location of devices, people, and physical goods –and enables them to respond promptly to emerging situations. For example, businesses can track changes in public sentiment on their brands and products by continuously analyzing social media streams, and respond in a timely fashion as the necessity arises.

Streaming data processing is beneficial in most scenarios where new, dynamic data is generated on a continual basis. It applies to most of the industry segments and big data use cases. Companies generally begin with simple applications such as collecting system logs and rudimentary processing like rolling min-max computations. Then, these applications evolve to more sophisticated near-real-time processing. Initially, applications may process data streams to produce simple reports, and perform simple actions in response, such as emitting alarms when key measures exceed certain thresholds. Eventually, those applications perform more sophisticated forms of data analysis, like applying machine learning algorithms, and extract deeper insights from the data. Over time, complex, stream and event processing algorithms, like decaying time windows to find the most recent popular movies, are applied, further enriching the insights.

Batch vs Stream
Batch processing can be used to compute arbitrary queries over different sets of data. It usually computes results that are derived from all the data it encompasses, and enables deep analysis of big data sets. MapReduce-based systems, like Amazon EMR, are examples of platforms that support batch jobs. In contrast, stream processing requires ingesting a sequence of data, and incrementally updating metrics, reports, and summary statistics in response to each arriving data record. It is better suited for real-time monitoring and response functions.

Challenges in working with streaming data
Streaming data processing requires two layers: a storage layer and a processing layer. The storage layer needs to support record ordering and strong consistency to enable fast, inexpensive, and replayable reads and writes of large streams of data. The processing layer is responsible for consuming data from the storage layer, running computations on that data, and then notifying the storage layer to delete data that is no longer needed. You also have to plan for scalability, data durability, and fault tolerance in both the storage and processing layers. As a result, many platforms have emerged that provide the infrastructure needed to build streaming data applications including Amazon Kinesis Data Streams, Amazon Kinesis Data Firehose, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Apache Flume, Apache Spark Streaming, and Apache Storm.



https://streamsets.com/learn/stream-processing-streaming-data/
Everyone wants their slice of that data to do what they do better (links given on site):
Sales and marketing can offer real-time suggestions for a next best action
Operations and customer service cut repair and build time with more efficiency
Security and compliance can detect fraud and take action before damage is done

From Apache Kafka to Object Stores
Apache Kafka is an open source distributed event streaming platform, known as a “pub/sub” messaging system. A streaming data source starts publishing or streaming data and a destination system subscribes to receive the data. The publisher doesn’t wait for subscribers and subscribers jump into the stream when they need it. Kafka is fast, scalable, durable and was a pillar of on-premises big data deployment.

Cloud platforms introduced a new way of storing unstructured data called an object store. Producers became decoupled from consumers, and the cost of storage became negligible. You could keep all the data you wanted as objects to be accessed when needed. For example, Amazon Kinesis integrates directly with Amazon Redshift (an analytics database) and Amazon S3 for streaming data. 

At DNB, Norway’s largest financial services group, data engineers use streaming instead of batch wherever possible as a data engineering best practice. 

“We encourage our data engineers to use streaming mode wherever possible. The downstream pipeline can be run as per the requirement, but it always gives us the option of running it more frequently than once a day to a near real-time by using this approach.”




https://www.striim.com/blog/6-best-practices-for-real-time-data-movement-and-stream-processing/
Batch processing: data is typically extracted from databases at the end of the day, saved to disk for transformation, and then loaded in batch to a data warehouse.

Batch data integration is useful for data that isn’t extremely time-sensitive. Electric bills are a relevant example. Your electric consumption is collected during a month and then processed and billed at the end of that period.

Stream processing: data is continuously collected and processed and dispersed to downstream systems. Stream processing is (near) real-time processing.

Real-time data processing has many use cases. For example, online brokerage platforms need to present fresh data at all times; even a slight delay could prove disastrous in a fast-moving trade. 




https://www.infoworld.com/article/3646589/what-is-streaming-data-event-stream-processing-explained.html
There are three ways to deal with streaming data: batch process it at intervals ranging from hours to days, process the stream in real time, or do both in a hybrid process.

Batch processing has the advantage of being able to perform deep analysis, including machine learning, and the disadvantage of having high latency. Stream processing has the advantage of low latency, and the disadvantage of only being able to perform simple analysis, such as calculating average values over a time window and flagging deviations from the expected values.

Hybrid processing combines both methods and reaps the benefits of both. In general, the data is processed as a stream and simultaneously branched off to storage for later batch processing. To give an example, consider an acoustic monitor attached to an industrial machine. The stream processor can detect an abnormal squeak and issue an alert; the batch processor can invoke a model to predict the time to failure based on the squeak as it progresses, and schedule maintenance for the machine long before it fails.




https://iwringer.wordpress.com/2015/08/03/patterns-for-streaming-realtime-analytics/
An actual list of streaming analysis that might be done

https://kafka.apache.org/intro
Good intro video about the ideas with streaming data
logs of events in a sequence are produced (often many, from differing sources)
Actions done on them:
grouping, aggregating, enriching (joining data, includes data from databaes and not just other logs), filtering

Kafka stream can handle these


