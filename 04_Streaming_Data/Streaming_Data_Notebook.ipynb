{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example to show word count using spark SQL\n",
    "\n",
    "Import necessary things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/08 16:31:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import explode, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data from a local source into a Spark SQL style data frame. We'll use the Oliver Twist chapter we created a while back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|chapter i  treats...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('value', StringType(), True)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chap1 = spark.read.text(\"data/chap1.txt\")\n",
    "chap1.show()\n",
    "chap1.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use `split(str, regex, limit)`: Splits str around occurrences that match regex and returns an array with a length of at most limit.\n",
    "Remember we have lazy eval though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'split(value,  , -1)'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(chap1.value, \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the result of that, we'll use `explode()`: Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'explode(split(value,  , -1))'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode(split(chap1.value, \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the column name isn't great, let's create an alias for the column name so it is easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'explode(split(value,  , -1)) AS word'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode(split(chap1.value, \" \")).alias(\"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have a column object that we can select from our original `chap1` data frame.  We need to use the select method.\n",
    "([syntax and more info](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = explode(split(chap1.value, \" \")).alias(\"word\")\n",
    "chap1.select(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.show()` method to actually get the data back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         word|\n",
      "+-------------+\n",
      "|      chapter|\n",
      "|            i|\n",
      "|             |\n",
      "|       treats|\n",
      "|           of|\n",
      "|          the|\n",
      "|        place|\n",
      "|        where|\n",
      "|       oliver|\n",
      "|        twist|\n",
      "|          was|\n",
      "|         born|\n",
      "|          and|\n",
      "|           of|\n",
      "|          the|\n",
      "|circumstances|\n",
      "|    attending|\n",
      "|          his|\n",
      "|        birth|\n",
      "|             |\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = chap1.select(col)\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that was a good check to make sure we had what we wanted.  Now we want to count the number of times each word occurs.  We'll use `groupBy()` and `count()` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `.show()` to execute all the steps above and get something back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|         some|    2|\n",
      "|          few|    1|\n",
      "|         hope|    1|\n",
      "|    overseers|    2|\n",
      "|   surrounded|    1|\n",
      "|    biography|    1|\n",
      "|  perspective|    1|\n",
      "|circumstances|    1|\n",
      "|  articulated|    1|\n",
      "|        among|    1|\n",
      "|          day|    1|\n",
      "|         lips|    1|\n",
      "|    appendage|    1|\n",
      "|       raised|    2|\n",
      "|      whether|    1|\n",
      "|          did|    2|\n",
      "|        space|    1|\n",
      "|    existence|    1|\n",
      "|          two|    1|\n",
      "|     instance|    1|\n",
      "|    buildings|    1|\n",
      "|    strangers|    1|\n",
      "|     occurred|    1|\n",
      "|      inmates|    1|\n",
      "|      backand|    1|\n",
      "|       within|    1|\n",
      "|       favour|    1|\n",
      "|        could|    3|\n",
      "|          him|    2|\n",
      "|       badged|    1|\n",
      "+-------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = words.groupBy(\"word\").count()\n",
    "counts.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's sort it and show some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   75|\n",
      "|     |   40|\n",
      "|  and|   35|\n",
      "|   of|   35|\n",
      "|    a|   33|\n",
      "|   to|   27|\n",
      "|   in|   22|\n",
      "|  was|   17|\n",
      "|   it|   13|\n",
      "|  her|   13|\n",
      "|  had|   12|\n",
      "| have|   12|\n",
      "| that|   12|\n",
      "|   by|   11|\n",
      "|  she|   11|\n",
      "| been|   11|\n",
      "|  his|   11|\n",
      "|   he|   11|\n",
      "|   on|   10|\n",
      "|which|   10|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts.sort('count', ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Using Spark Structured Streaming\n",
    "\n",
    "First read in functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import explode, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in the stream\n",
    "\n",
    "In this case, set up that we are *listening* to a folder call `csv_files` for the addition of new `.csv` files. We'll set up the schema for the resulting SQL style data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType().add(\"value\", \"string\")\n",
    "chaps = spark.readStream.schema(myschema).csv(\"csv_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set up transformations/aggregations to do\n",
    "\n",
    "We'll basically pull the exact code from above to act on our `chaps` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = chaps.select(explode(split(chaps.value, \" \")).alias(\"word\"))\n",
    "#now count the words and sort it.\n",
    "wordCounts = words \\\n",
    "                .groupBy(\"word\") \\\n",
    "                .count() \\\n",
    "                .sort('count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 & 4: Set up Writing of the Query to an Output Source and `.start()` the Query\n",
    "\n",
    "Now we'll use the `.writeStream` method to output the result of this to the console (we'll talk more about what this all means in a bit). The `.start()` method instructs the query to being looking for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:31:51 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9841acd7-73e9-4ff9-b550-127028185a7e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/08 16:31:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   75|\n",
      "|     |   40|\n",
      "|  and|   35|\n",
      "|   of|   35|\n",
      "|    a|   33|\n",
      "|   to|   27|\n",
      "|   in|   22|\n",
      "|  was|   17|\n",
      "|   it|   13|\n",
      "|  her|   13|\n",
      "|  had|   12|\n",
      "| have|   12|\n",
      "| that|   12|\n",
      "|   by|   11|\n",
      "|  she|   11|\n",
      "| been|   11|\n",
      "|  his|   11|\n",
      "|   he|   11|\n",
      "|   on|   10|\n",
      "|which|   10|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|  word|count|\n",
      "+------+-----+\n",
      "|   the|  332|\n",
      "|      |  214|\n",
      "|     a|  165|\n",
      "|   and|  153|\n",
      "|    of|  148|\n",
      "|    to|  120|\n",
      "|   was|   95|\n",
      "|    in|   93|\n",
      "|    he|   62|\n",
      "|   had|   60|\n",
      "|    it|   55|\n",
      "|   his|   51|\n",
      "|  that|   50|\n",
      "|oliver|   47|\n",
      "|   for|   46|\n",
      "|  with|   44|\n",
      "|   him|   42|\n",
      "|     i|   39|\n",
      "|  said|   36|\n",
      "|    at|   35|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that it is *listening* we can start adding files to the `csv_files` folder and see Spark update with new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Creating Data Using the `rate` Format\n",
    "\n",
    "Let's just see how to generate data using the `rate` format. This is just data that can be used to play around with methods and get things working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "rateDF = spark.readStream.format(\"rate\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[timestamp: timestamp, value: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rateDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:33:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-51ab7639-d0ec-4dbd-b782-aa3dd79a8992. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/08 16:33:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|           timestamp|value|\n",
      "+--------------------+-----+\n",
      "|2024-04-08 16:33:...|    0|\n",
      "+--------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|           timestamp|value|\n",
      "+--------------------+-----+\n",
      "|2024-04-08 16:33:...|    1|\n",
      "+--------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|           timestamp|value|\n",
      "+--------------------+-----+\n",
      "|2024-04-08 16:33:...|    2|\n",
      "+--------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|           timestamp|value|\n",
      "+--------------------+-----+\n",
      "|2024-04-08 16:33:...|    3|\n",
      "+--------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|           timestamp|value|\n",
      "+--------------------+-----+\n",
      "|2024-04-08 16:33:...|    4|\n",
      "+--------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|           timestamp|value|\n",
      "+--------------------+-----+\n",
      "|2024-04-08 16:33:...|    5|\n",
      "+--------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|           timestamp|value|\n",
      "+--------------------+-----+\n",
      "|2024-04-08 16:33:...|    6|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writeDF = rateDF.writeStream.outputMode(\"append\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now *usually* we can stop the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDF.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can easily add some transformations in that would allow us to check functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:33:23 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a48c538d-fb71-46c0-a22c-ba52cbfc11d7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/08 16:33:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+-------------+\n",
      "|timestamp|value|squared_value|\n",
      "+---------+-----+-------------+\n",
      "+---------+-----+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+-----+-------------+\n",
      "|           timestamp|value|squared_value|\n",
      "+--------------------+-----+-------------+\n",
      "|2024-04-08 16:33:...|    0|          0.0|\n",
      "+--------------------+-----+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+-----+-------------+\n",
      "|           timestamp|value|squared_value|\n",
      "+--------------------+-----+-------------+\n",
      "|2024-04-08 16:33:...|    1|          1.0|\n",
      "+--------------------+-----+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+-----+-------------+\n",
      "|           timestamp|value|squared_value|\n",
      "+--------------------+-----+-------------+\n",
      "|2024-04-08 16:33:...|    2|          4.0|\n",
      "+--------------------+-----+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+-----+-------------+\n",
      "|           timestamp|value|squared_value|\n",
      "+--------------------+-----+-------------+\n",
      "|2024-04-08 16:33:...|    3|          9.0|\n",
      "+--------------------+-----+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------+-----+-------------+\n",
      "|           timestamp|value|squared_value|\n",
      "+--------------------+-----+-------------+\n",
      "|2024-04-08 16:33:...|    4|         16.0|\n",
      "+--------------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, pow\n",
    "manipDF = rateDF.withColumn(\"squared_value\", pow(col(\"value\"), 2))\n",
    "writeDF = manipDF.writeStream.outputMode(\"append\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDF.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Stream Example `memory` Format\n",
    "\n",
    "We can write a table to memory and then access it via SQL style commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:33:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2c5ba013-e1a4-4d20-8014-f5372ee2628d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/08 16:33:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "writeTable = manipDF.writeStream \\\n",
    "                    .format(\"memory\") \\\n",
    "                    .queryName(\"my_table\") \\\n",
    "                    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a table that exists for us called `my_table`. We can use `spark.sql()` to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+\n",
      "|           timestamp|value|squared_value|\n",
      "+--------------------+-----+-------------+\n",
      "|2024-04-08 16:33:...|    0|          0.0|\n",
      "|2024-04-08 16:33:...|    1|          1.0|\n",
      "|2024-04-08 16:33:...|    2|          4.0|\n",
      "|2024-04-08 16:34:...|    3|          9.0|\n",
      "|2024-04-08 16:34:...|    4|         16.0|\n",
      "|2024-04-08 16:34:...|    5|         25.0|\n",
      "+--------------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from my_table\").show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeTable.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table still exists after we stop the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+\n",
      "|           timestamp|value|squared_value|\n",
      "+--------------------+-----+-------------+\n",
      "|2024-04-08 16:33:...|    0|          0.0|\n",
      "|2024-04-08 16:33:...|    1|          1.0|\n",
      "|2024-04-08 16:33:...|    2|          4.0|\n",
      "|2024-04-08 16:34:...|    3|          9.0|\n",
      "|2024-04-08 16:34:...|    4|         16.0|\n",
      "|2024-04-08 16:34:...|    5|         25.0|\n",
      "+--------------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from my_table\").show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating Over Windows\n",
    "\n",
    "Let's do a more complicated example where we were are reading in csv files from a folder and doing aggregations over time windows. This implies that our data will have some timestamps associated with it!\n",
    "\n",
    "First we'll need some data to stream. We'll read in the `neuralgia.csv` file we've seen previously in the course. Then we'll add a time stamp and output parts of the data to `.csv` files that will be read in via a stream.\n",
    "\n",
    "**We don't want to submit this code right now! We'll submit this in a python console in another window to simulate the idea of getting new data in.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in some data to sample from\n",
    "import pandas as pd\n",
    "neuralgia = pd.read_csv(\"data/neuralgia.csv\")\n",
    "\n",
    "#Now a for loop to sample a few rows and output them to a data set\n",
    "#Put a pause in as well\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "for i in range(0,10):\n",
    "    #randomly sample a few rows\n",
    "    temp = neuralgia.loc[np.random.randint(neuralgia.shape[0], size = 5)]\n",
    "    temp[\"timestamp\"] = [time.strftime(\"%H:%M:%S\", time.localtime())]*5\n",
    "    temp.to_csv(\"csv_neuralgia/neuralgia\" + str(i) + \".csv\", index = False, header = False)\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, when we get files in the `csv_neuralgia` directory we want to read those in. Let's set up the schema for that and read in the files as as stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "#set up schema\n",
    "myschema = StructType().add(\"Treatment\", \"string\").add(\"Sex\", \"string\").add(\"Age\", \"integer\").add(\"Duration\", \"integer\").add(\"Pain\", \"string\").add(\"timestamp\", \"timestamp\")\n",
    "#set up the stream\n",
    "df = spark.readStream.schema(myschema).csv(\"csv_neuralgia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to do some aggregations on the data. Specifically, we'll group the data by the \"Sex\" column and find the average \"Duration\" of pain. To start with, let's just output this to the console to see if it is working **(no windowing yet)**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:34:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-af09457d-adcb-4797-9051-7d0c609dfe33. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/08 16:34:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-------------+\n",
      "|Sex|avg(Duration)|\n",
      "+---+-------------+\n",
      "|  F|         27.0|\n",
      "|  M|         10.0|\n",
      "+---+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+-------------+\n",
      "|Sex|avg(Duration)|\n",
      "+---+-------------+\n",
      "|  F|         19.5|\n",
      "|  M|       10.375|\n",
      "+---+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---+-------------+\n",
      "|Sex|avg(Duration)|\n",
      "+---+-------------+\n",
      "|  F|         14.0|\n",
      "|  M|       10.375|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#do our aggregations\n",
    "agg = df.groupBy(\"Sex\").avg(\"Duration\")\n",
    "#write out the results to the console to start with\n",
    "myquery = agg.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is running we can submit the first chunk of code to a console and see it update in the console!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "myquery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do the same thing as above but find aggregations over windows of time rather than overall. We'll use the `window()` function with the `timestamp` column to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:37:01 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-baf75c09-c3c2-417a-b982-1e171b694723. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/08 16:37:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+---+------------------+\n",
      "|              window|Sex|     avg(Duration)|\n",
      "+--------------------+---+------------------+\n",
      "|{2024-04-08 16:37...|  M|              20.0|\n",
      "|{2024-04-08 16:37...|  F|27.666666666666668|\n",
      "+--------------------+---+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+---+------------------+\n",
      "|              window|Sex|     avg(Duration)|\n",
      "+--------------------+---+------------------+\n",
      "|{2024-04-08 16:37...|  F|               9.0|\n",
      "|{2024-04-08 16:37...|  M|              20.0|\n",
      "|{2024-04-08 16:37...|  F|27.666666666666668|\n",
      "+--------------------+---+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+---+------------------+\n",
      "|              window|Sex|     avg(Duration)|\n",
      "+--------------------+---+------------------+\n",
      "|{2024-04-08 16:37...|  F|               9.0|\n",
      "|{2024-04-08 16:38...|  M|              12.0|\n",
      "|{2024-04-08 16:37...|  M|              20.0|\n",
      "|{2024-04-08 16:38...|  F| 4.666666666666667|\n",
      "|{2024-04-08 16:37...|  F|27.666666666666668|\n",
      "+--------------------+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now including windowing\n",
    "from pyspark.sql.functions import window\n",
    "#'listen' on the same folder\n",
    "df = spark.readStream.schema(myschema).csv(\"csv_neuralgia\")\n",
    "#do our aggregations with a window of 30 seconds, no overlap.\n",
    "agg = df.groupBy(\n",
    "            window(\"timestamp\", \"30 seconds\", \"30 seconds\"),\n",
    "            \"Sex\") \\\n",
    "        .avg(\"Duration\")\n",
    "#write it!\n",
    "windowQuery = agg.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is running we can submit the first chunk of code to a console and see it update in the console!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to include a watermark as well. We do so with the `.withWatermark()` method! This will allow us to output to a file (like .csv) when doing aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:38:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#'listen' on the same folder\n",
    "df = spark.readStream.schema(myschema).csv(\"csv_neuralgia\")\n",
    "#do our aggregations with a window of 30 seconds, no overlap. Provide a 15 second watermark\n",
    "agg = df \\\n",
    "        .withWatermark(\"timestamp\", \"2 seconds\") \\\n",
    "        .groupBy(\n",
    "            window(\"timestamp\", \"30 seconds\", \"30 seconds\"),\n",
    "            \"Sex\") \\\n",
    "        .avg(\"Duration\")\n",
    "#now the window column isn't a type that can be sent to a csv...\n",
    "#instead cast it to a string and we'll be all set\n",
    "agg2 = agg \\\n",
    "        .select(col(\"Sex\"), \n",
    "                col(\"avg(Duration)\").alias(\"Avg_Duration\"), \n",
    "                col(\"Window\").cast(\"string\")\n",
    "               )\n",
    "#write it!\n",
    "windowWaterQuery = agg2 \\\n",
    "                    .writeStream \\\n",
    "                    .outputMode(\"append\") \\\n",
    "                    .option(\"checkpointlocation\", \"check\") \\\n",
    "                    .format(\"csv\") \\\n",
    "                    .option(\"path\", \"output_csv/\") \\\n",
    "                    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to submit the code that creates the csv files now. After that runs for a minute, let's stop the query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowWaterQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output csv is actually stored in pieces over my local cluster. We can `coalesce` that into a single file with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsafiles = spark.read.option('header', 'false').csv(\"output_csv/part-*.csv\")\n",
    "allsafiles.coalesce(1).write.format(\"csv\").option(\"header\",\"false\").save(\"output_csv/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Joins Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do our set up and read in a static spark SQL data frame representing ad *impressions* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|adId|      impressionTime|\n",
      "+----+--------------------+\n",
      "|   0|2022-01-01 00:02:...|\n",
      "|   1|2022-01-01 00:05:...|\n",
      "|   2|2022-01-01 00:12:...|\n",
      "|   3|2022-01-01 00:13:...|\n",
      "|   4|2022-01-01 00:14:...|\n",
      "|   5|2022-01-01 00:19:...|\n",
      "|   6|2022-01-01 00:21:...|\n",
      "|   7|2022-01-01 00:21:...|\n",
      "|   8|2022-01-01 00:27:...|\n",
      "|   9|2022-01-01 00:28:...|\n",
      "+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "adImpressionSchema = StructType().add('adId', \"integer\").add('impressionTime', 'timestamp')\n",
    "#First read in a static dataframe\n",
    "staticDF = spark.read.csv('data/impressions.csv', schema = adImpressionSchema, header = True)\n",
    "staticDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we will now place some *click* data into a folder that is being monitored. This will be joined with the ad data on the `adId` column. (We'll just drop a couple files in the folder being monitored and see that it is working.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:51:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a547b519-af9a-44e3-a451-83db8dfdb0a9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/08 16:51:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+--------------------+--------------------+\n",
      "|adId|      impressionTime|           clickTime|\n",
      "+----+--------------------+--------------------+\n",
      "|  35|2022-01-01 01:24:...|2022-01-01 01:30:...|\n",
      "|  38|2022-01-01 01:33:...|2022-01-01 01:37:...|\n",
      "|  58|2022-01-01 02:30:...|2022-01-01 02:42:...|\n",
      "|  90|2022-01-01 04:02:...|2022-01-01 04:10:...|\n",
      "|  94|2022-01-01 04:11:...|2022-01-01 04:13:...|\n",
      "| 117|2022-01-01 05:26:...|2022-01-01 05:29:...|\n",
      "| 123|2022-01-01 05:51:...|2022-01-01 06:03:...|\n",
      "| 127|2022-01-01 06:04:...|2022-01-01 06:16:...|\n",
      "| 140|2022-01-01 06:41:...|2022-01-01 06:45:...|\n",
      "| 170|2022-01-01 07:56:...|2022-01-01 08:07:...|\n",
      "| 190|2022-01-01 08:57:...|2022-01-01 09:05:...|\n",
      "| 197|2022-01-01 09:25:...|2022-01-01 09:34:...|\n",
      "| 246|2022-01-01 12:13:...|2022-01-01 12:20:...|\n",
      "| 251|2022-01-01 12:17:...|2022-01-01 12:26:...|\n",
      "| 252|2022-01-01 12:18:...|2022-01-01 12:18:...|\n",
      "| 260|2022-01-01 12:38:...|2022-01-01 12:38:...|\n",
      "| 313|2022-01-01 14:42:...|2022-01-01 14:52:...|\n",
      "| 319|2022-01-01 14:46:...|2022-01-01 14:49:...|\n",
      "| 322|2022-01-01 15:01:...|2022-01-01 15:11:...|\n",
      "| 335|2022-01-01 15:38:...|2022-01-01 15:42:...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now set up the stream to take in data from the .csv\n",
    "adClickSchema = StructType().add('adId', \"integer\").add('clickTime', 'timestamp')\n",
    "df = spark.readStream.schema(adClickSchema).csv(\"click_csv_files\")\n",
    "\n",
    "#Set up the join and start the query!\n",
    "joinquery = staticDF \\\n",
    "                .join(df, \"adId\", \"inner\") \\\n",
    "                .writeStream.outputMode(\"append\") \\\n",
    "                .format(\"console\") \\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinquery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
